Khai bÃºt Ä‘áº§u xuÃ¢n báº±ng cÃ¡ch Ä‘iá»ƒm láº¡i vÃ i kiáº¿n thá»©c cÆ¡ báº£n :vvvv 



Háº³n nhá»¯ng ai lÃ m quen vá»›i Machine Learning vÃ  AI, vÃ  ká»ƒ cáº£ nhá»¯ng ngÆ°á»i chÆ°a tá»«ng tiáº¿p xÃºc, vÃ  cáº£ nhá»¯ng ngÆ°á»i ngoÃ i ngÃ nh IT nÃ y ná»¯a, Ä‘á»u Ã­t nhiá»u tá»«ng biáº¿t Ä‘áº¿n cÃ¡c AI tá»± váº½ tranh, tá»± chÆ¡i nháº¡c, tá»± viáº¿t text, hay nÃ³i Ä‘Ãºng hÆ¡n lÃ  tá»± sinh (generative). trong nhá»¯ng nÄƒm trá»Ÿ láº¡i Ä‘Ã¢y, cÃ¡c mÃ´ hÃ¬nh sinh Ä‘Ã£ Ä‘áº¡t Ä‘Æ°á»£c nhá»¯ng bÆ°á»›c tiáº¿n vÃ´ cÃ¹ng Ä‘Ã¡ng ká»ƒ vÃ  táº¡o ra nhiá»u giÃ¡ trá»‹ chÃ¢n thá»±c. Nháº¯c Ä‘áº¿n há» mÃ´ hÃ¬nh sinh generative models, táº¥t nhiÃªn pháº£i ká»ƒ Ä‘áº¿n hai cÃ¡i tÃªn phá»• biáº¿n nháº¥t vÃ  Ä‘Æ°á»£c quan tÃ¢m nháº¥t lÃ  cÃ¡c mÃ´ hÃ¬nh **GAN (Generative Adversarial Networks)** vÃ  **Variational Autoencoders (VAEs)**.

CÃ¡c má»¥c trong bÃ i viáº¿t bao gá»“m:
* #1 MÃ´ hÃ¬nh Autoencoder
* #2 Má»‘i liÃªn há»‡ giá»¯a Autoencoder vÃ  PCA trong váº¥n Ä‘á» giáº£m chiá»u dá»¯ liá»‡u
* #3 **Variational Autoencoder** cáº£i tiáº¿n nhá»¯ng gÃ¬ so vá»›i Autoencoder cÆ¡ báº£n


ÄÃ¢y lÃ  má»™t bÃ i viáº¿t dÃ i dÃ i (táº£n máº¡n mÃ  :D) Náº¿u muá»‘n nhanh chÃ³ng, báº¡n cÃ³ thá»ƒ lÆ°á»›t tháº³ng Ä‘áº¿n tá»«ng pháº§n mong muá»‘n.

TrÆ°á»›c khi Ä‘i vÃ o VAE, mÃ¬nh muá»‘n nháº¯c láº¡i má»™t chÃºt vá» Autoencoder vÃ  PCA.
# 1. Nháº¯c láº¡i Autoencoder

Náº¿u báº¡n chÆ°a tá»«ng nghe vá» AutoEncoder, Ä‘á»«ng ngáº¡i lÃ½ thuyáº¿t dÃ i dÃ²ng, click vÃ o Ä‘Ã¢y vÃ  báº¡n cÃ³ thá»ƒ hiá»ƒu ngay Ã½ tÆ°á»Ÿng cÆ¡ báº£n cá»§a Auto Encoder lÃ  lÃ m gÃ¬ trong vÃ i phÃºt. NhÃ¬n chung, Ã½ tÆ°á»Ÿng xuáº¥t phÃ¡t cá»§a Autoencoder khÃ¡ Ä‘Æ¡n giáº£n.

* [Autoencoder In PyTorch - Theory & Implementation](https://www.python-engineer.com/posts/pytorch-autoencoder/).

P/s: MÃ¬nh thÃ­ch cÃ¡i video nÃ y vÃ¬ chá»§ video lÃ  dÃ¢n engineer nÃªn cÃ¡ch tiáº¿p cáº­n khÃ¡ chi lÃ  thoáº£i mÃ¡i :vv VÃ  giá»ng tiáº¿ng anh dá»… nghe ná»¯a.


### Autoencoder gá»“m 3 pháº§n
![image.png](https://images.viblo.asia/8cf254ba-6dd4-4c7d-b496-16008a40c07b.png)
* **Encoder**: chÃºng ta cÃ³ má»™t cÃ¡i áº£nh. VÃ­ dá»¥ áº£nh 128x128x 3 channel mÃ u RGB, vÃ  ta tháº¥y lÆ°u hÆ¡i máº¥t nhiá»u khÃ´ng gian. NÃªn chÃºng ta cáº§n má»™t khÃ´ng gian nhá» hÆ¡n Ä‘á»ƒ chá»©a áº£nh. Váº­y chÃºng ta sáº½ láº¥y ra nhá»¯ng Ä‘áº·c trÆ°ng cá»§a cÃ¡i áº£nh Ä‘Ã³, bá» Ä‘i nhá»¯ng thá»© ko cáº§n thiáº¿t, sau Ä‘Ã³ nÃ©n nÃ³ vÃ o má»™t cÃ¡i vecto Ä‘áº·c trÆ°ng vá»›i Ã­t chiá»u hÆ¡n ban Ä‘áº§u. ÄÃ³ lÃ  Ä‘iá»u mÃ  pháº§n Encoder Ä‘ang cá»‘ gáº¯ng lÃ m.
* **Botteneck**: vecto Ä‘áº·c trÆ°ng Ä‘Ã£ nÃ³i á»Ÿ trÃªn, nÆ¡i chá»©a cÃ¡c thÃ´ng tin quan trá»ng Ä‘Ã£ Ä‘Æ°á»£c cÃ´ Ä‘á»ng cá»§a Ä‘áº§u vÃ o. Bá»Ÿi tháº¿ nÃªn cÃ³ thá»ƒ rÃºt ra má»™t vÃ i Ä‘iá»u: 
* **(1)** chiá»u cá»§a bottelneck cháº¯c cháº¯n pháº£i nhá» hÆ¡n chiá»u Ä‘áº§u vÃ o vÃ  
* **(2)** Bottleneck cÃ ng nhá», cÃ ng giáº£m overfitting vÃ¬ mÃ´ hÃ¬nh sáº½ pháº£i chá»n lá»c cÃ¡c thÃ´ng tin quan trá»ng hÆ¡n Ä‘á»ƒ Ä‘em theo, khÃ´ng cÃ³ kháº£ nÄƒng Ã´m Ä‘á»“m quÃ¡ nhiá»u vÃ  
* **(3)** Bottelneck nhá» quÃ¡, lÆ°u Ä‘Æ°á»£c quÃ¡ Ã­t thÃ´ng tin, khiáº¿n Decoder giáº£i mÃ£ khÃ´ng ra. Rá»‘t cuá»™c thÃ¬, cÃ¡i gÃ¬ Balance cÅ©ng má»›i tá»‘t =))))
 
     Tá»« pháº§n sau, mÃ¬nh sáº½ gá»i nÃ³ lÃ  latent space (khÃ´ng gian tiá»m áº©n).
* **Decoder**: Giáº£i mÃ£ ngÆ°á»£c tá»« bottleneck Ä‘á»ƒ cá»‘ gáº¯ng táº¡o ra má»™t cÃ¡i áº£nh má»›i cÃ³ liÃªn quan cháº·t cháº½ vá»›i áº£nh cÅ© (Ä‘Æ°Æ¡ng nhiÃªn rá»“i nÃ³ chá»©a cÃ¡c thÃ´ng tin quan trá»ng cá»§a áº£nh cÅ© mÃ  =))). LiÃªn quan cháº·t cháº½ Ä‘áº¿n má»©c nÃ o thÃ¬ báº¡n cÃ³ thá»ƒ nhÃ¬n vÃ o hÃ m loss, hÃ m so sÃ¡nh giá»¯a áº£nh má»›i vÃ  áº£nh cÅ©. Trong AE hÃ m loss Ä‘Æ°á»£c gá»i lÃ  **reconstruction loss**. NhÆ° cÃ³ thá»ƒ tháº¥y trong video trÃªn, reconstruction loss Ä‘Æ°á»£c sá»­ dá»¥ng lÃ  MSELoss. CÃ³ thá»ƒ thay báº±ng L1 Loss, cÃ²n náº¿u input lÃ  categorical thÃ¬ dÃ¹ng Cross Entropy Loss.

Khoan Ä‘Ã£ dá»«ng khoáº£ng chá»«ng lÃ  5s. Tháº¿ cÃ³ khÃ¡c gÃ¬ cá»‘ gáº¯ng táº¡o ra áº£nh má»›i nhÆ° áº£nh cÅ©, LÃ  copy paste hay gÃ¬?

ÄÃºng váº­y, náº¿u báº¡n Ä‘Ã£ xem video trÃªn thÃ¬ sáº½ tháº¥y áº£nh má»›i cÃ³ ná»™i dung y nhÆ° áº£nh cÅ©. ÄÃºng váº­y, Ä‘iá»u mÃ  Autoencoder Ä‘ang cá»‘ gáº¯ng lÃ m lÃ  **tÃ¡i hiá»‡n láº¡i ná»™i dung cÅ©, nhÆ°ng vá»›i nhá»¯ng giÃ¡ trá»‹ pixel má»›i** =))))) Trong thá»±c táº¿, má»™t autoencoder lÃ  má»™t táº­p há»£p cÃ¡c háº¡n cháº¿ buá»™c máº¡ng pháº£i tÃ¬m hiá»ƒu nhá»¯ng cÃ¡ch thá»©c má»›i Ä‘á»ƒ Ä‘áº¡i diá»‡n cho dá»¯ liá»‡u mÃ  khÃ´ng lÃ m thay Ä‘á»•i ná»™i dung cá»§a nÃ³.
![1.png](https://images.viblo.asia/c2473efd-7032-4726-8f2d-d82ffad6f499.png)

### Váº­y Autoencoder Ä‘Æ°á»£c dÃ¹ng lÃ m gÃ¬? 

NhÃ¬n sÆ¡ qua cÃ³ thá»ƒ tháº¥y ngay vÃ i á»©ng dá»¥ng:
* Giáº£m chiá»u dá»¯ liá»‡u
* Táº¡o data má»›i (nÃªn má»›i Ä‘Æ°á»£c gá»i lÃ  mÃ´ hÃ¬nh sinh =)))
* Khá»­ nhiá»…u: bá»Ÿi vÃ¬ Ä‘iá»u mÃ  mÃ´ hÃ¬nh Ä‘ang cá»‘ lÃ m chÃ­nh lÃ  giá»¯ láº¡i nhá»¯ng gÃ¬ tá»•ng quÃ¡t vÃ  quan trá»ng, nÃªn viá»‡c khÃ©o lÃ©o bá» Ä‘i nhá»¯ng chi tiáº¿t khÃ´ng liÃªn quan chÃ­nh lÃ  má»™t pháº§n cá»§a viá»‡c khá»­ nhiá»…u. Tháº­m chá»‰ khÃ´ng chá»‰ dÃ¹ng Ä‘á»ƒ khá»­ nhiá»ƒu áº£nh, mÃ  cÃ²n cÃ³ thá»ƒ khá»­ tiáº¿ng á»“n noisy trong Ã¢m tranh cháº³ng háº¡n. Äá»c thÃªm vá» **Denoising Autoencoder (DAE)**. Hoáº·c báº¡n cÃ³ thá»ƒ Ä‘áº§y Ä‘á»§ 5 loáº¡i AutoEncoder phá»• biáº¿n táº¡i pháº§n tÃ i liá»‡u tham kháº£o cuá»‘i bÃ i.


![image.png](https://images.viblo.asia/c616699b-6a03-41e4-b6b6-23253d1e0978.png)





NhÆ° hÃ¬nh áº£nh trÃªn, vá»›i Ä‘áº§u vÃ o $x$, sau khi Ä‘i qua bá»™ mÃ£ hÃ³a $E$ vÃ  bá»™ giáº£i mÃ£ $D$, ta thu Ä‘Æ°á»£c $d(e(x))$. Má»¥c tiÃªu lÃ  tÃ¬m $(e^*, d^*)\in(E \times D)$ sao cho

$$(e^*, d^*)=\underset{(e, d) \in E \times D}{argmin} (error(x, d(e(x))))

Tá»« Ä‘Ã¢y mÃ¬nh gá»i $N$ lÃ  sá»‘ data, $n_d$ lÃ  sá»‘ chiá»u dá»¯ liá»‡u ban Ä‘áº§u vÃ  $n_e$ lÃ  sá»‘ chiá»u dá»¯ liá»‡u sau khi encode. (latent dimension).

### Triá»ƒn khai code
Code cÃ³ thá»ƒ Ä‘Æ°á»£c triá»ƒn khai ráº¥t Ä‘Æ¡n giáº£n trÃªn PyTorch nhÆ° sau:

```python
import torch

import torch.nn.functional as F

class Encoder(torch.nn.Module):
 def __init__(self, latent_dim=128):
   super(Encoder, self).__init__()
   self.enc1 = torch.nn.Linear(128x128, 512) # input image shape: (3,128,128)
   self.mean_x = torch.nn.Linear(512,latent_dim)
   self.logvar_x = torch.nn.Linear(512, latent_dim)
   
 def forward(self,inputs):
   x = self.enc1(inputs)
   x= F.relu(x)
   z_mean = self.mean_x(x)
   z_log_var = self.logvar_x(x)
   return z_mean, z_log_var
```
VÃ  Decode: 
```python
class Decoder(torch.nn.Module):
 def __init__(self, latent=128):
   super(Decoder, self).__init__()
   self.dec1 = torch.nn.Linear(latent, 512)
   self.out = torch.nn.Linear(512, 128x128)


 def forward(self,z):
   z = self.dec1(z)
   z = F.relu(z)
   return self.out(z)
```


## NÄƒm loáº¡i Autoencoder phá»‘ biáº¿n
* Undercomplete autoencoders
* Sparse autoencoders
* Contractive autoencoders
* Denoising autoencoders
* Variational Autoencoders (for generative modelling)

TÃ¬m Ä‘á»c Ä‘áº§y Ä‘á»§ táº¡i tÃ i liá»‡u á»Ÿ pháº§n 6.

Tiáº¿p theo, hÃ£y nghÃ­a qua PCA má»™t chÃºt.
# 2. Autoencoder vÃ  má»‘i liÃªn há»‡ vá»›i PCA

á» phÃ­a trÃªn, mÃ¬nh cÃ³ nháº¯c Ä‘áº¿n dÃ¹ng Autoencoder Ä‘á»ƒ giáº£m chiá»u dá»¯ liá»‡u (Dimensionality Reduction). 

Nháº¯c tá»›i cÃ¡c thuáº­t toÃ¡n Dimensionality Reduction, khÃ´ng thá»ƒ khÃ´ng nháº¯c tá»›i má»™t trong nhá»¯ng thuáº­t toÃ¡n cÆ¡ báº£n nháº¥t lÃ  **Principal Component Analysis (PCA)** (PhÃ¢n tÃ­ch thÃ nh pháº§n chÃ­nh). PhÆ°Æ¡ng phÃ¡p nÃ y dá»±a trÃªn quan sÃ¡t ráº±ng dá»¯ liá»‡u thÆ°á»ng khÃ´ng phÃ¢n bá»‘ ngáº«u nhiÃªn trong khÃ´ng gian mÃ  thÆ°á»ng phÃ¢n bá»‘ gáº§n cÃ¡c Ä‘Æ°á»ng/máº·t Ä‘áº·c biá»‡t nÃ o Ä‘Ã³. 

Há»­, hÃ¬nh nhÆ° hÆ¡i trá»«u tÆ°á»£ng. Váº­y ta cÃ³ ngay má»™t vÃ­ dá»¥.

![image.png](https://images.viblo.asia/f1bfc1c5-6d90-45f7-90ab-6322755e2499.png)

Trong hÃ¬nh trÃªn, ta Ä‘áº·t cÃ¡c Ä‘iá»ƒm Ä‘ang cá»‘ gáº¯ng encode cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u A, B, C á»Ÿ dáº¡ng 2 chiá»u nÃ©n vá» cÃ²n má»™t chiá»u. Äiá»u nÃ y lÃ  cÃ³ thá»ƒ bá»Ÿi vÃ¬ cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u cÃ³ váº» nhÆ° Ä‘ang phÃ¢n bá»‘ gáº§n má»™t Ä‘Æ°á»ng nÃ o Ä‘Ã³. Sau Ä‘Ã³ Ä‘Æ°a vá» má»™t Ä‘Æ°á»ng tháº³ng (tuyáº¿n tÃ­nh).

á» trÃªn, mÃ¬nh cÅ©ng cÃ³ nháº¯c Ä‘áº¿n viá»‡c Encoder sáº½ cá»‘ giá»¯ láº¡i nhá»¯ng thÃ´ng tin quan trá»ng. Váº­y náº¿u, táº¥t cáº£ má»i thÃ´ng tin Ä‘á»u cÃ³ váº» quan trá»ng nhÆ° nhau, thÃ¬ giá»¯ láº¡i cÃ¡i nÃ o?

Háº³n báº¡n sáº½ nÃ³i, lÃ m gÃ¬ cÃ³ chuyá»‡n má»i thÃ´ng tin Ä‘á»u quan trá»ng nhÆ° nhau. Váº­y mÃ¬nh xin phÃ©p sá»­a láº¡i cÃ¢u há»i trÃªn nhÆ° sau: náº¿u Ä‘áº¿n má»™t thá»i Ä‘iá»ƒm nÃ o Ä‘Ã³, má»i giÃ¡ trá»‹ Ä‘á»u mang thÃ´ng tin nhÆ° nhau. Báº¡n muá»‘n nÃ©n bottleneck chá»‰ lÃ  má»™t vecto 128 chiá»u, nhÆ°ng váº«n cÃ²n khoáº£ng 200 giÃ¡ trá»‹ mÃ  bá» cÃ¡i nÃ o Ä‘i cÅ©ng lÃ m máº¥t thÃ´ng tin quan trá»ng.

Váº­y pháº£i lÃ m sao?

BÃ i viáº¿t vá» PCA trÃªn forum [Machine learning cÆ¡ báº£n](https://machinelearningcoban.com/2017/06/15/pca/) Ä‘Ã£ nÃªu ra má»™t vÃ­ dá»¥ khÃ¡ dá»… hiá»ƒu. Giáº£ sá»­ ta cÃ³ 4 camera láº¯p á»Ÿ bá»‘n gÃ³c phÃ²ng. Vai trÃ² nhÆ° nhau nÃªn camera nÃ o cÅ©ng quan trá»ng. NhÆ°ng náº¿u Ä‘áº·t trong hoÃ n cáº£nh, má»™t ngÆ°á»i Ä‘ang Ä‘i vá» hÆ°á»›ng 1 camera, dÃ¹ng há»‡ quy chiáº¿u lÃ  má»™t Ä‘Æ°á»ng tháº³ng Ä‘i qua 2 camera vÃ  ngÆ°á»i, thÃ¬ lÃºc nÃ y cháº¯c cháº¯n hÃ¬nh áº£nh camera Ä‘Ã³ thu Ä‘Æ°á»£c (khuÃ´n máº·t) sáº½ há»¯u Ã­ch hÆ¡n nhiá»u so vá»›i cÃ¡c camera cÃ²n láº¡i (chá»‰ tháº¥y gÃ¡y vÃ  tÃ³c). Váº­y Ä‘áº·t trong má»™t trÆ°á»ng há»£p cá»¥ thá»ƒ, má»©c Ä‘á»™ quan trá»ng 4 camera Ä‘Ã£ khÃ¡c nhau.

Váº­y máº¥u chá»‘t lÃ , thá»­ Ä‘áº·t trong má»™t há»‡ quy chiáº¿u khÃ¡c Ä‘i, má»©c Ä‘á»™ quan trá»ng cá»§a cÃ¡c thÃ´ng tin sáº½ thay Ä‘á»•i.

NÃ³i theo kiá»ƒu ToÃ¡n hÆ¡n má»™t chÃºt (From Machine Learning cÆ¡ báº£n)
> PCA chÃ­nh lÃ  phÆ°Æ¡ng phÃ¡p Ä‘i tÃ¬m má»™t há»‡ cÆ¡ sá»Ÿ má»›i sao cho thÃ´ng tin cá»§a dá»¯ liá»‡u chá»§ yáº¿u táº­p trung á»Ÿ má»™t vÃ i toáº¡ Ä‘á»™, pháº§n cÃ²n láº¡i chá»‰ mang má»™t lÆ°á»£ng nhá» thÃ´ng tin. VÃ  Ä‘á»ƒ cho Ä‘Æ¡n giáº£n trong tÃ­nh toÃ¡n, PCA sáº½ tÃ¬m má»™t há»‡ trá»±c chuáº©n Ä‘á»ƒ lÃ m cÆ¡ sá»Ÿ má»›i.

Má»i chi tiáº¿t toÃ¡n há»c vá» PCA cÃ³ thá»ƒ tÃ¬m tháº¥y táº¡i [ML cÆ¡ báº£n - BÃ i 27: Principal Component Analysis (pháº§n 1/2)](https://machinelearningcoban.com/2017/06/15/pca/)
HÃ¬nh nhÆ° hÆ¡i lan man rá»“i, quay vá» chá»§ Ä‘á» chÃ­nh thÃ´i.

Váº­y PCA liÃªn quan gÃ¬ á»Ÿ Ä‘Ã¢y?

NhÆ° Ä‘Ã£ tháº¥y á»Ÿ trÃªn, PCA cá»‘ gáº¯ng chuyá»ƒn dá»¯ liá»‡u tá»« high-dimensional vá» má»™t hyperplane tháº¥p hÆ¡n. ChÃ­nh nÃ³, chÃ­nh lÃ  Ä‘iá»u mÃ  pháº§n Encoder cÅ©ng Ä‘ang cá»‘ lÃ m.

NhÆ°ng dá»… tháº¥y, PCA chá»‰ xÃ¢y dá»±ng cÃ¡c má»‘i quan há»‡ tuyáº¿n tÃ­nh.

Trong khi Ä‘Ã³, vá»›i viá»‡c thÃªm cÃ¡c Activation function vÃ o máº¡ng neural lÃ m Autoencoder cÃ³ kháº£ nÄƒng há»c cÃ¡c má»‘i quan há»‡ phi tuyáº¿n. Náº¿u bá» cÃ¡c hÃ m nÃ y Ä‘i, AE giá» cÃ³ thá»ƒ chá»‰ coi ngang nhÆ° PCA.

![image.png](https://images.viblo.asia/97faf918-26d6-41f7-9a08-ce5508cbdd18.png)

Má»‘i liÃªn há»‡ giá»¯a PCA vÃ  Autoencoder Ä‘Æ°á»£c thá»ƒ hiá»‡n á»Ÿ hÃ¬nh dÆ°á»›i.

![image.png](https://images.viblo.asia/b25ab2c7-7be6-4ff3-a6f1-a8e535ad7b51.png)


Táº£n máº¡n tháº¿ Ä‘á»§ rá»“i. HÃ£y vÃ o pháº§n chÃ­nh cá»§a bÃ i viáº¿t hÃ´m nay :vvv

# 3. Variational Autoencoders (VAEs)

á» pháº§n Ä‘áº§u, mÃ¬nh Ä‘Ã£ nháº¯c Ä‘áº¿n viá»‡c áº£nh má»›i Ä‘Æ°á»£c táº¡o tá»« Autoencoder cÃ³ liÃªn quan cháº·t cháº½ Ä‘áº¿n áº£nh cÅ©. Tháº¿ nÃ o lÃ  **liÃªn quan cháº·t cháº½**? Thá»±c ra cÃ³ má»™t cÃ¢u há»i lá»›n hÆ¡n, **Autoencoder cÃ³ gÃ¬ khÃ´ng tá»‘t**?

Paper gá»‘c:
* [An Introduction to Variational Autoencoders](https://arxiv.org/pdf/1906.02691.pdf)

### Má»™t sá»‘ thuáº­t ngá»¯
* latent space: khÃ´ng gian tiá»m áº©n (chá»‰ vÃ¹ng bottleneck) (cháº³ng biáº¿t dá»‹ch tháº¿ nÃ o cho chuáº©n =))))
* latent dimension: chiá»u dá»¯ liá»‡u sau khi encode
* TÃ­nh liÃªn tá»¥c **continuity**: hai Ä‘iá»ƒm gáº§n nhau trong khÃ´ng gian tiá»m tÃ ng khÃ´ng Ä‘Æ°á»£c cung cáº¥p hai ná»™i dung hoÃ n toÃ n khÃ¡c nhau sau khi Ä‘Æ°á»£c giáº£i mÃ£
* TÃ­nh Ä‘áº§y Ä‘á»§ **completeness**: Ä‘á»‘i vá»›i má»™t phÃ¢n phá»‘i Ä‘Ã£ chá»n , má»™t Ä‘iá»ƒm Ä‘Æ°á»£c láº¥y máº«u tá»« khÃ´ng gian tiá»m áº©n sáº½ cung cáº¥p ná»™i dung "cÃ³ Ã½ nghÄ©a" sau khi Ä‘Æ°á»£c giáº£i mÃ£
* Regular space: Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a lÃ , náº¿u cÃ³ táº­p bao Ä‘Ã³ng F, xÃ©t báº¥t kÃ½ Ä‘iá»ƒm x nÃ o khÃ´ng thuá»™c F, luá»“n tá»“n táº¡i lÃ¢n cáº­n U cá»§a x vÃ  V cá»§a F khÃ´ng trÃ¹ng nhau (tÃ¡ch rá»i nhau). Táº¡i Ä‘Ã¢y, báº¥t ká»³ hai Ä‘iá»ƒm nÃ o cÃ³ thá»ƒ phÃ¢n biá»‡t Ä‘Æ°á»£c vá» máº·t cáº¥u trÃºc liÃªn káº¿t Ä‘á»u cÃ³ thá»ƒ Ä‘Æ°á»£c phÃ¢n tÃ¡ch báº±ng cÃ¡c vÃ¹ng lÃ¢n cáº­n.

![image.png](https://images.viblo.asia/bf621f91-e194-476c-9c40-3d1ea82db7c7.png)

### Háº¡n chá»ƒ cá»§a AE
Váº¥n Ä‘á» Ä‘Æ°á»£c chá»‰ ra á»Ÿ Autoencoder lÃ  ká»ƒ cáº£ khi latent dimension tháº¥p, máº¡ng váº«n cÃ³ thá»ƒ táº­n dá»¥ng má»i kháº£ nÄƒng má»™t cÃ¡ch quÃ¡ má»©c (overfitting). Má»¥c tiÃªu duy nháº¥t cá»§a nÃ³ lÃ  giáº£m loss xuá»‘ng má»©c tháº¥p nháº¥t, bá» qua má»i Ä‘iá»u cÃ²n láº¡i nhÆ° giá»¯a nguyÃªn phÃ¢n phá»‘i, abc vv... Khi Ä‘Ã³, khÃ´ng gian tiá»m áº©n cá»§a má»™t bá»™ mÃ£ tá»± Ä‘á»™ng cÃ³ thá»ƒ cá»±c ká»³ báº¥t thÆ°á»ng, dáº«n Ä‘áº¿n:
* Máº¥t tÃ­nh liÃªn tá»¥c: cÃ¡c Ä‘iá»ƒm gáº§n trong khÃ´ng gian tiá»m áº©n cÃ³ thá»ƒ cung cáº¥p dá»¯ liá»‡u Ä‘Æ°á»£c giáº£i mÃ£ ráº¥t khÃ¡c nhau
* Máº¥t tÃ­nh Ä‘áº§y Ä‘á»§: Má»™t sá»‘ Ä‘iá»ƒm cá»§a khÃ´ng gian tiá»m áº©n cÃ³ thá»ƒ cung cáº¥p ná»™i dung vÃ´ nghÄ©a sau khi Ä‘Æ°á»£c giáº£i mÃ£,â€¦

Váº­y,  náº¿u chÃºng ta khÃ´ng cáº©n tháº­n vá» Ä‘á»‹nh nghÄ©a cá»§a kiáº¿n trÃºc, hiá»ƒn nhiÃªn máº¡ng sáº½ táº­n dá»¥ng má»i kháº£ nÄƒng quÃ¡ má»©c Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c nhiá»‡m vá»¥ cá»§a nÃ³. ÄÃ³ lÃ  lÃ½ do ta nÃªn quy Ä‘á»‹nh nÃ³ má»™t cÃ¡ch rÃµ rÃ ng tá»« trÆ°á»›c.



* ÄÃ³ lÃ  lÃ½ do cáº§n Ä‘áº¿n **Variational Autoencoders (VAEs)**.

### Ã tÆ°á»Ÿng chÃ­nh
> Thay vÃ¬ mÃ£ hÃ³a (encode) Ä‘áº§u vÃ o nhÆ° má»™t Ä‘iá»ƒm duy nháº¥t, ta mÃ£ hÃ³a nÃ³ dÆ°á»›i dáº¡ng phÃ¢n phá»‘i trÃªn khÃ´ng gian tiá»m áº©n.

HÃ¬nh áº£nh dÆ°á»›i Ä‘Ã¢y mÃ´ táº£ sá»£ khÃ¡c nhau giá»¯a AE vÃ  VAE.

![1_ejNnusxYrn1NRDZf4Kg2lw@2x.png](https://images.viblo.asia/b4a675f8-dfec-49f3-81be-f185ea9a6d5b.png)

Cá»¥ thá»ƒ hÆ¡n má»™t chÃºt ná»¯a vá» Ä‘iá»u thá»±c sá»± diá»…n ra á»Ÿ latent space.

![60bbe752a4dbfa4ee8b4912e_funny1.png](https://images.viblo.asia/b80fe4cc-7fd0-4f58-95ef-c194d6f88da5.png)
<div align="center">Autoencoder</div> 

![60bbe7e0e2a8da8a3a877a05_pasted 1.png](https://images.viblo.asia/a2f2c48f-bd30-45bd-82d6-4b2c4ece0b4b.png)

<div align="center">Variational Autoencoder</div>





* Tuy nhiÃªn, viá»‡c mÃ£ hÃ³a nÃ³ dÆ°á»›i dáº¡ng má»™t phÃ¢n phá»‘i váº«n chÆ°a Ä‘á»§ Ä‘á»ƒ Ä‘Ã¡m báº£o 2 yáº¿u tá»‘ Ä‘Ã£ náº¿u á»Ÿ Ä‘áº§u. NÃ³ cÃ³ thá»ƒ sáº½ Ä‘Æ°a vá» má»™t phÃ¢n phá»‘i vá»›i tiny variances, hoáº·c má»™t phÃ¢n phá»‘i cÃ³ sá»± sai khÃ¡c mean lá»›n (cÃ¡c cá»¥m cÃ¡ch xa nhau trong khÃ´ng gian tiá»m áº©n). ÄÃ³ lÃ  lÃ½ do, khÃ´ng nhá»¯ng Ä‘Æ°a vá» phÃ¢n phá»‘i, ta cÃ²n pháº£i **chuáº©n hÃ³a phÃ¢n phá»‘i**, bao gá»“m chuáº©n hÃ³a covariance matrix vÃ  mean cá»§a phÃ¢n phá»‘i. 

* Háº§u háº¿t, viá»‡c chuáº©n hÃ³a sáº½ cá»‘ gáº¯ng Ä‘Æ°a vá» phÃ¢n phá»‘i chuáº©n. Má»¥c tiÃªu cuá»‘i cÃ¹ng lÃ  ngÄƒn cÃ¡c phÃ¢n phá»‘i Ä‘Ã£ mÃ£ hÃ³a cÃ¡ch xa nhau trong khÃ´ng gian tiá»m áº©n vÃ  khuyáº¿n khÃ­ch cÃ ng nhiá»u cÃ ng tá»‘t cÃ¡c phÃ¢n phá»‘i tráº£ vá» "chá»“ng chÃ©o nhau", nhá» Ä‘Ã³, Ä‘áº¡t Ä‘Æ°á»£c cÃ¡c Ä‘iá»u kiá»‡n vá» tÃ­nh liÃªn tá»¥c vÃ  Ä‘áº§y Ä‘á»§.

![image.png](https://images.viblo.asia/3e4f95dd-39ea-476b-9991-8023ac2c3c36.png)

* NhÆ° váº­y, ta thu Ä‘Æ°á»£c vecto $z$ Ä‘Æ°á»£c sample tá»« phÃ¢n phá»‘i chuáº©n cÃ³ tuÃ¢n theo trung bÃ¬nh $\mu$ vÃ  Ä‘á»™ lá»‡ch chuáº©n $sigma$. Tá»« Ä‘Ã³, z Ä‘Æ°á»£c decode.

Triá»ƒn khai code:
```python
class VAE(torch.nn.Module):
   def __init__(self, latent_dim=20):
       super(VAE, self).__init__()
       self.encoder = Encoder(latent_dim)
       self.decoder = Decoder(latent_dim)

   def forward(self,inputs):
     z_mean, z_log_var = self.encoder(inputs)
     z = self.reparameterize(z_mean, z_log_var)
     reconstructed = self.decoder(z)
     return reconstructed, z_mean, z_log_var

   def reparameterize(self, mu, log_var):
     std = torch.exp(0.5 * log_var)
     eps = torch.randn_like(std)
     return mu + (eps * std)
```
### HÃ m máº¥t mÃ¡t

á» pháº§n AE Ä‘Ã£ sá»­ dá»¥ng má»™t hÃ m máº¥t mÃ¡t lÃ  **reconstruction loss** Ä‘á»ƒ Ä‘o lÆ°á»£ng thÃ´ng tin bá»‹ máº¥t mÃ¡t sau khi tÃ¡i táº¡o. Äá»‘i vá»›i VAE cáº§n pháº£i cÃ³ má»™t hÃ m ná»¯a Ä‘á»ƒ quáº£n lÃ½ tÃ­nh regular (Ä‘á»u Ä‘áº·n) cá»§a khÃ´ng gian tiá»m áº©n, Ä‘Æ°á»£c gá»i lÃ  **regularisation loss**. HÃ m loss nÃ y Ä‘Æ°á»£c biá»ƒu thá»‹ dÆ°á»›i dáº¡ng [ Kulback-Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) (KL phÃ¢n ká»³). 

ÄÃ¢y lÃ  má»™t phÃ©p Ä‘o sá»± khÃ¡c nhau giá»¯a má»™t phÃ¢n phá»‘i xÃ¡c suáº¥t vÃ  Ä‘áº¡t giÃ¡ trá»‹ báº±ng 0 khi hai phÃ¢n phá»‘i Ä‘Æ°á»£c coi lÃ  giá»‘ng nhau. HÃ m loss nÃ y sáº½ cá»‘ gáº¯ng giáº£m thiá»ƒu sá»± phÃ¢n ká»³ KL giá»¯a phÃ¢n phá»‘i ban Ä‘áº§u vÃ  phÃ¢n phá»‘i Ä‘Æ°á»£c tham sá»‘ hÃ³a cá»§a chÃºng tÃ´i.

$$ \text { loss }=\|x-x \hat{x}\|^{2}+\mathrm{KL}\left[N\left(\mu_{x}, \sigma_{x}\right), N(0, I)\right]

$$ \text {loss} =\|x-d(z)\|^{2}+K L\left[N\left(\mu_{x}, \sigma_{x}\right), N(0, I)\right]

KL divergence loss ngÄƒn cáº£n máº¡ng há»c cÃ¡c phÃ¢n phá»‘i háº¹p vÃ  cá»‘ gáº¯ng Ä‘Æ°a phÃ¢n phá»‘i gáº§n hÆ¡n vá»›i phÃ¢n phá»‘i chuáº©n Ä‘Æ¡n vá»‹.

$$ \mathbf{K L}\left(N \left (\mu_x,\sigma_x\right), N\left(0, I_{d}\right)\right)=\frac{1}{2} \sum_{i=1}^{d}\left(\sigma\left(x\right)-\log \sigma \left(x\right)-1+\mu \left(x\right)^{2}\right)

```python
def final_loss(reconstruction, train_x, mu, logvar):
   BCE = torch.nn.BCEWithLogitsLoss(reduction='sum')(reconstruction, train_x)
   KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
   return BCE + KLD
```

### Reparameter trick 
á» pháº§n triá»ƒn khai code phÃ­a trÃªn, cÃ³ má»™t pháº§n Ä‘Æ°á»£c gá»i lÃ  reparamater.
ThÃ´ng thÆ°á»ng, Ä‘á»ƒ huáº¥n luyá»‡n máº¡ng neural network ta dÃ¹ng thuáº­t toÃ¡n gradient descent, mÃ  muá»‘n thá»±c hiá»‡n gradient descent thÃ¬ pháº£i cÃ³ Ä‘áº¡o hÃ m. Trong Variational Autoencoder, thÃ¬ ta cÃ³ má»™t bÆ°á»›c sample tá»« phÃ¢n phá»‘i chuáº©n Ä‘á»ƒ thu Ä‘Æ°á»£c vector áº©n $z$ Ä‘á»ƒ Ä‘Æ°a qua decoder. Æ , váº­y chá»— sampling nÃ y thÃ¬ lan truyá»n ngÆ°á»£c tháº¿ nÃ o.

Thay vÃ¬ trá»±c tiáº¿p sample tá»« phÃ¢n phá»‘i xÃ¡c suáº¥t, ngÆ°á»i ta sá»­ dá»¥ng 1 máº¹o nhá» gá»i lÃ  reparameterization. Vector $z$ sáº½ Ä‘Æ°á»£c tÃ­nh báº±ng cÃ´ng thá»©c.
$$ z = \mu_x + \epsilon \odot \sqrt{\sigma_x}

trong Ä‘Ã³ $\epsilon \sim N(0, I_d)$. 

Táº¡i sao láº¡i cÃ³ cÃ´ng thá»©c nÃ y?

Äiá»u nÃ y Ä‘áº¿n tá»« thá»±c táº¿ ráº±ng náº¿u $z$ lÃ  má»™t biáº¿n ngáº«u nhiÃªn tuáº§n theo phÃ¢n phá»‘i Guassian vá»›i giÃ¡ trá»‹ trung bÃ¬nh $\mu_x$ vÃ  hiá»‡p phÆ°Æ¡ng sai $\sigma^2$ thÃ¬ nÃ³ cÃ³ thá»ƒ biá»ƒu diá»…n á»Ÿ dáº¡ng trÃªn. VÃ  Ä‘á»“ng thá»i Ä‘Ã³ cÅ©ng lÃ  lÃ½ do ta cÃ³ cÃ´ng thá»©c hÃ m loss phÃ­a trÃªn.

$$ \mathbf{K L}\left(N \left (\mu_x,\sigma_x\right), N\left(0, I_{d}\right)\right)=\frac{1}{2} \sum_{i=1}^{d}\left(\sigma\left(x\right)-\log \sigma \left(x\right)-1+\mu \left(x\right)^{2}\right)
```Python
def reparameterize(self, mu, log_var):
     std = torch.exp(0.5 * log_var)
     eps = torch.randn_like(std)
     return mu + (eps * std)
```


![60e424b06f61a263edba1fe6_diagrammetic.png](https://images.viblo.asia/b81f9aef-4bf0-4c56-9483-e6f700cf30da.png)
# 4. Triá»ƒn khai code

ÄÃ£ Ä‘áº¿n Ä‘Ã¢y rá»“i thÃ¬ cÃ¹ng lÆ°á»›t qua má»™t tutorial ráº¥t Ä‘áº§y Ä‘á»§ mÃ¬nh lÆ°á»£m Ä‘Æ°á»£c trÃªn AI Summer =)))) MÃ¬nh cÅ©ng khÃ¡ thÃ­ch trang nÃ y, tuy khÃ´ng cÃ³ quÃ¡ nhiá»u bÃ i viáº¿t nhá»¯ng má»—i bÃ i Ä‘á»u khÃ¡ chá»‰n chu.

* [AI Summer - JAX vs Tensorflow vs Pytorch: Building a Variational Autoencoder (VAE)](https://theaisummer.com/jax-tensorflow-pytorch/)

# 5. Autoencoding Variational Autoencoders (AVAE)

* [The Autoencoding Variational Autoencoder](https://arxiv.org/pdf/2012.03715.pdf)

# 6. Káº¿t - TÃ i liá»‡u

Máº·c dÃ¹ thoáº¡t nhÃ¬n, autoencoders cÃ³ váº» dá»… dÃ ng (vÃ¬ chÃºng cÃ³ ná»n táº£ng lÃ½ thuyáº¿t ráº¥t Ä‘Æ¡n giáº£n), nhÆ°ng viá»‡c khiáº¿n chÃºng há»c cÃ¡ch biá»ƒu diá»…n Ä‘áº§u vÃ o cÃ³ Ã½ nghÄ©a lÃ  Ä‘iá»u khÃ¡ khÃ³ khÄƒn.

Cáº£m Æ¡n tÃ¡c giáº£ bÃ i viáº¿t gá»‘c tiáº¿ng anh, má»™t bÃ i viáº¿t ráº¥t chi tiÃªt. BÃ i viáº¿t trÃªn cá»§a mÃ¬nh váº«n cÃ²n nhiá»u chi tiáº¿t chÆ°a truyá»n táº£i Ä‘Æ°á»£c (cháº¯c chá»‰ Ä‘Æ°á»£c táº§m 60%), báº¡n cÃ³ thá»ƒ xem bÃ i viáº¿t gá»‘c táº¡i Ä‘Ã¢y, nháº¥t lÃ  pháº§n **Mathematical details of VAEs** vÃ  lÃ½ do cá»§a cÃ¡i tÃªn **Variational**  :

* [Understanding Variational Autoencoders (VAEs)](https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73)

* hoáº·c paper gá»‘c:  [An Introduction to Variational Autoencoders](https://arxiv.org/pdf/1906.02691.pdf)

ThÃªm má»™t bÃ i viáº¿t ná»¯a tá»•ng quan chung vá» Autoencoder:
* [An Introduction to Autoencoders: Everything You Need to Know ](https://www.v7labs.com/blog/autoencoders-guide) (má»™t senpai trong team cÃ³ dá»‹ch má»™t pháº§n bÃ i viáº¿t nÃ y, cÃ¡c báº¡n cÃ³ thá»ƒ Ä‘á»c táº¡i [Ä‘Ã¢y](), pháº§n PhÃ¢n loáº¡i cÃ¡c dáº¡ng Autoencoder Ä‘áº·c biá»‡t lÃ  VAE vÃ  DAE chÆ°a Ä‘Æ°Æ¡c dá»‹ch, khÃ¡ hay vÃ  cÃ³ thá»ƒ Ä‘á»c thÃªm náº¿u muá»‘n)

* https://machinelearningcoban.com/2017/03/04/overfitting

* Má»™t senpai trong team cÅ©ng Ä‘Ã£ viáº¿t má»™t bÃ i giá»›i thiá»‡u tá»•ng quan vá» VAE dÃ nh cho nhá»¯ng ai náº¯m báº¯t nhanh vÃ  cÃ³ kÃ¨m triá»ƒn khai code, báº¡n cÃ³ thá»ƒ Ä‘á»c táº¡i Ä‘Ã¢y: [Giá»›i thiá»‡u váº§ Variational Autoencoder](https://viblo.asia/p/gioi-thieu-ve-variational-autoencoder-Eb85ozN0l2G)


BÃ i viáº¿t Ä‘Ã£ dÃ i. Mong má»i ngÆ°á»i thÃ´ng cáº£m vÃ¬ nÃ³ cÃ³ pháº§n hÆ¡i lan man vÃ  táº£n máº¡n. Náº¿u tháº¥y bÃ i há»¯u Ã­ch cÃ³ thá»ƒ táº·ng mÃ¬nh má»™t **Upvote** cho mk cÃ³ thÃªm Ä‘á»™ng lá»±c khai bÃºt Ä‘áº§u xuÃ¢n :vv. 

Äá»“ng thá»i bÃ i viáº¿t nÃ y lÃ  Ä‘á»ƒ chuáº©n bá»‹ cho má»™t sá»‘ bÃ i viáº¿t khÃ¡c sáº¯p tá»›i, hi vá»ng sáº½ cÃ³ Ã­ch.


ChÃºc má»i ngÆ°á»i nÄƒm má»›i vui váº» thÃ nh cÃ´ng. ğŸ¥°