TrÆ°á»›c tiÃªn mÃ¬nh xin cáº£m Æ¡n táº¥t cáº£ má»i ngÆ°á»i Ä‘Ã£, Ä‘ang vÃ  sáº½ Ä‘á»c bÃ i viáº¿t nÃ y cá»§a mÃ¬nh. ÄÃ¢y lÃ  bÃ i viáº¿t Ä‘áº§u tay cá»§a mÃ¬nh vá»›i má»¥c Ä‘á»‹ch chia sáº», trao Ä‘á»•i kiáº¿n thá»©c nÃªn sáº½ khÃ´ng thá»ƒ trÃ¡nh khá»i nhá»¯ng sai sÃ³t, ráº¥t mong nháº­n Ä‘Æ°á»£c Ã½ kiáº¿n cá»§a cÃ¡c báº¡n Ä‘á»ƒ cÃ¡c bÃ i viáº¿t sau nÃ y cá»§a mÃ¬nh Ä‘Æ°á»£c tá»‘t hÆ¡n.

Trong bÃ i viáº¿t nÃ y mÃ¬nh sáº½ giá»›i thiá»‡u vá» kiáº¿n trÃºc FastSpeech dá»±a trÃªn bÃ i bÃ¡o [FastSpeech: Fast, Robust and Controllable
Text to Speech](https://arxiv.org/pdf/1905.09263.pdf), do Ä‘Ã³ yÃªu cáº§u má»™t sá»‘ kiáº¿n thá»©c cÆ¡ báº£n, cÃ¡c báº¡n cÃ³ thá»ƒ tham kháº£o táº¡i bÃ i viáº¿t [Má»™t sá»‘ kiáº¿n thá»©c cÆ¡ báº£n vá» Text2Speech](https://viblo.asia/p/mot-so-kien-thuc-co-ban-ve-text2speech-63vKjWLNZ2R).

# I. Tá»•ng quan
Text2Speech hay Text to Speech (TTS) lÃ  má»™t cÃ´ng nghá»‡ giÃºp chuyá»ƒn chá»¯ viáº¿t thÃ nh giá»ng nÃ³i, Ä‘Ã£ Ä‘Æ°á»£c nghiÃªn cá»©u tá»« vÃ i chá»¥c nÄƒm trÆ°á»›c vÃ  Ä‘áº·c biá»‡t phÃ¡t triá»ƒn máº¡nh trong nhá»¯ng nÄƒm gáº§n Ä‘Ã¢y nhá» sá»± phÃ¡t triá»ƒn cá»§a trÃ­ tuá»‡ nhÃ¢n táº¡o Ä‘á»ƒ Ä‘Ã¡p á»©ng yÃªu cáº§u vá» cháº¥t lÆ°á»£ng giá»ng nÃ³i. ÄÃ£ cÃ³ ráº¥t nhiá»u mÃ´ hÃ¬nh há»c sÃ¢u (Deep Learning - DL) vá» lÄ©nh vá»±c nÃ y, nhÆ° Tacotron, Tacotron2 (trÃªn viblo cÅ©ng Ä‘Ã£ cÃ³ series vá» chá»§ Ä‘á» nÃ y, cÃ¡c báº¡n cÃ³ thá»ƒ tham kháº£o táº¡i [Ä‘Ã¢y](https://viblo.asia/s/doi-dieu-ve-xu-ly-giong-noi-J3ZgB88BKmB)), Deep Voice 3. Nhá»¯ng mÃ´ hÃ¬nh nÃ y cÃ³ Ä‘iá»ƒm chung lÃ  sáº½ táº¡o ra cÃ¡c **mel-spectrogram** tá»« chá»¯ viáº¿t sau Ä‘Ã³ tá»•ng há»£p thÃ nh giá»ng nÃ³i báº±ng cÃ¡ch sá»­ dá»¥ng cÃ¡c cÃ´ng cá»¥ gá»i lÃ  **vocoder** nhÆ° **Griffin-Lim**, **WaveNet**, **WaveGlow**. Tuy nhiÃªn theo tÃ¡c giáº£, cÃ¡c mÃ´ hÃ¬nh trÃªn cÃ³ má»™t sá»‘ nhÆ°á»£c Ä‘iá»ƒm:

* Tá»‘c Ä‘á»™ suy luáº­n (inference) cháº­m: Do cÃ¡c mÃ´ hÃ¬nh trÃªn Ä‘á»u dá»±a trÃªn RNN, hÃ ng trÄƒm hoáº·c hÃ ng nghÃ¬n mel-spectrogram vá»›i cÃ¡i sau Ä‘Æ°á»£c dá»± Ä‘oÃ¡n dá»±a vÃ o cÃ¡i trÆ°á»›c sáº½ lÃ m giáº£m tá»‘c Ä‘á»™ suy luáº­n.
* Giá»ng nÃ³i tá»•ng há»£p Ä‘Æ°á»£c thÆ°á»ng gáº·p váº¥n Ä‘á» vá»›i nhá»¯ng tá»« bá»‹ bá» qua hoáº·c láº·p láº¡i.
* Thiáº¿u kháº£ nÄƒng kiá»ƒm soÃ¡t tá»‘c Ä‘á»™ vÃ  giá»ng nÃ³i.

Do Ä‘Ã³ tÃ¡c giáº£ Ä‘á» xuáº¥t mÃ´ hÃ¬nh FastSpeech nháº­n vÃ o cÃ¡c Ã¢m vá»‹ (phoneme), sá»­ dá»¥ng máº¡ng chuyá»ƒn tiáº¿p dá»±a trÃªn **"self-attention"** vÃ  **convolution 1D** Ä‘á»ƒ dá»± Ä‘oÃ¡n chuá»—i **mel-spectrogram** (chuá»—i nhá»¯ng cá»­a sá»• phá»•). MÃ´ hÃ¬nh nÃ y Ä‘Ã£ giáº£i quyáº¿t cÃ¡c váº¥n Ä‘á»:

* TÄƒng tá»‘c quÃ¡ trÃ¬nh tá»•ng há»£p báº±ng cÃ¡ch táº¡o ra mel-spectrogram má»™t cÃ¡ch song song.
* Bá»™ dá»± Ä‘oÃ¡n thá»i lÆ°á»£ng Ã¢m vá»‹ (Phoneme duration predictor) giÃºp háº¡n cháº¿ Ä‘Æ°á»£c cÃ¡c váº¥n Ä‘á» máº¥t tá»« vÃ  láº·p tá»«.
* Bá»™ Ä‘iá»u chá»‰nh Ä‘á»™ dÃ i (length regulator) xÃ¡c Ä‘á»‹nh Ä‘á»™ dÃ i cá»§a cÃ¡c mel-spectrograms tá»« Ä‘Ã³ dá»… dÃ ng Ä‘iá»u chá»‰nh tá»‘c Ä‘á»™ giá»ng nÃ³i báº±ng cÃ¡ch kÃ©o dÃ i hoáº·c rÃºt ngáº¯n thá»i lÆ°á»£ng cá»§a Ã¢m vá»‹ cÅ©ng nhÆ° Ä‘iá»u chá»‰nh Ã¢m Ä‘iá»‡u báº±ng cÃ¡ch thÃªm vÃ o cÃ¡c khoáº£ng ngáº¯t giá»¯a cÃ¡c Ã¢m vá»‹ liá»n ká».

BÃ¢y giá» chÃºng ta cÃ¹ng Ä‘i tÃ¬m hiá»ƒu xem mÃ´ hÃ¬nh hoáº¡t Ä‘á»™ng nhÆ° tháº¿ nÃ o.
# II. Kiáº¿n trÃºc mÃ´ hÃ¬nh
Thay vÃ¬ dá»±a trÃªn cáº¥u trÃºc encoder-attention-decoder, fastspeech dá»±a trÃªn cáº¥u trÃºc chuyá»ƒn tiáº¿p (feed-forward), cáº¥u trÃºc tá»•ng thá»ƒ cá»§a mÃ´ hÃ¬nh Ä‘Æ°á»£c biá»ƒu diá»…n nhÆ° hÃ¬nh dÆ°á»›i Ä‘Ã¢y:

![](https://images.viblo.asia/aa6b5136-59d3-4602-9500-b14a78f97949.png)

NgoÃ i cÃ¡c thÃ nh pháº§n cá»Ÿ báº£n cá»§a má»™t mÃ´ hÃ¬nh xá»­ lÃ½ ngÃ´n ngá»¯ lÃ  **Embedding** vÃ  **Positional Encoding**, thá»© lÃ m cho fastspeech khÃ¡c vá»›i cÃ¡c mÃ´ hÃ¬nh khÃ¡c Ä‘Ã³ chÃ­nh lÃ  **FFT block** Ä‘Æ°á»£c chia thÃ nh 2 pháº§n, N khá»‘i á»Ÿ vá» phÃ­a Ã¢m vá»‹ vÃ  N khá»‘i á»Ÿ phÃ­a phá»• tÃ­n hiá»‡u Ä‘Æ°á»£c káº¿t ná»‘i vá»›i nhau bá»Ÿi **Length Regulator** (N cÃ³ thá»ƒ Ä‘Æ°á»£c coi lÃ  má»™t hyperparameter cá»§a mÃ´ hÃ¬nh). Váº­y nhá»¯ng thÃ nh pháº§n nÃ y lÃ  gÃ¬, mÃ¬nh sáº½ trÃ¬nh bÃ y bÃªn dÆ°á»›i, hÃ£y cÃ¹ng theo dÃµi nhÃ©!
## 1. Feed-Forward Transformer

ThÃ nh pháº§n xÆ°Æ¡ng sá»‘ng cá»§a fastspeech lÃ  nhá»¯ng khá»‘i thÃ nh pháº§n dá»±a trÃªn **self-attention** cá»§a mÃ´ hÃ¬nh **Transformer** vÃ  máº¡ng tÃ­ch cháº­p 1 chiá»u **1D convolution** (thay vÃ¬ 2 lá»›p Dense nhÆ° cÃ¡c mÃ´ hÃ¬nh transformer khÃ¡c) Ä‘Æ°á»£c tÃ¡c giáº£ gá»i lÃ  **"Feed-Forward Transformer" (FFT)** (**LÆ°u Ã½:** phÃ©p biáº¿n Ä‘á»•i fourier nhanh trong xá»­ lÃ½ tÃ­n hiá»‡u cÅ©ng thÆ°á»ng Ä‘Æ°á»£c kÃ½ hiá»‡u lÃ  FFT nhÆ°ng trong bÃ i viáº¿t nÃ y khÃ´ng sá»­ dá»¥ng Ä‘áº¿n nÃªn tá»« giá» Ä‘áº¿n háº¿t bÃ i viáº¿t nÃ y, **"FFT"** sáº½ chá»‰ sá»­ dá»¥ng Ä‘á»ƒ nÃ³i vá» **"Feed-Forward Transformer"** ). NÃ³i váº­y cÃ³ thá»ƒ hÆ¡i khÃ³ hÃ¬nh dung, hÃ£y nhÃ¬n vÃ o hÃ¬nh biá»ƒu diá»…n cáº¥u trÃºc cá»§a khá»‘i FFT sáº½ dá»… hÆ¡n:

![](https://images.viblo.asia/12fb0647-b4f6-42ff-92f7-81582ee56559.png)

Tá»« hÃ¬nh váº½ chÃºng ta cÃ³ thá»ƒ dá»… dÃ ng náº¯m Ä‘Æ°á»£c cáº¥u trÃºc cá»§a má»™t **FFT Block**, bao gá»“m má»™t **self-attention** cá»¥ thá»ƒ lÃ  **multi-head attention** (mÃ¬nh sáº½ khÃ´ng Ä‘i vÃ o chi tiáº¿t cÆ¡ cháº¿ cá»§a attention, Ä‘Ã£ cÃ³ nhiá»u bÃ i viáº¿t vá» váº¥n Ä‘á» nÃ y, cÃ¡c báº¡n cÃ³ thá»ƒ tham kháº£o trÃªn blog vá» AI cá»§a [KhanhBlog](https://phamdinhkhanh.github.io/2019/06/18/AttentionLayer.html) hoáº·c trÃªn [Viblo](https://viblo.asia/p/tan-man-ve-self-attention-07LKXoq85V4)). NhÆ° Ä‘Ã£ Ä‘á» cáº­p á»Ÿ trÃªn, khÃ¡c vá»›i cÃ¡c cáº¥u trÃºc **Transformer** khÃ¡c, 2 lá»›p **Conv 1D** vá»›i hÃ m kÃ­ch hoáº¡t lÃ  **ReLU** Ä‘Æ°á»£c sá»­ dá»¥ng thay cho 2 lá»›p **Dense**. LÃ½ giáº£i cho sá»± khÃ¡c biá»‡t nÃ y, tÃ¡c giáº£ cho ráº±ng nhá»¯ng tráº¡ng thÃ¡i áº©n liá»n ká» nhau thÃ¬ liÃªn quan cháº·t cháº½ hÆ¡n trong Ã¢m vá»‹ vÃ  trong chuá»—i mel-spectrogram. Äiá»u nÃ y Ä‘Ã£ Ä‘Æ°á»£c tÃ¡c giáº£ thá»ƒ hiá»‡n báº±ng thá»±c nghiá»‡m. Sau má»—i khá»‘i **multi-head attention** vÃ  **Conv1D** lÃ  má»™t káº¿t ná»‘i táº¯t (**residual connection**),  **layer normalization** vÃ  **dropout**.

## 2.Length Regulator
Má»™t Ã¢m cÃ³ thá»ƒ ngáº¯n hoáº·c dÃ i, do Ä‘Ã³ cÃ³ thá»ƒ biá»ƒu diá»…n báº±ng sá»‘ lÆ°á»£ng cÃ¡c cá»­a sá»• mel-spectrogram khÃ¡c nhau. VÃ¬ váº­y náº¿u chá»‰ sá»­ dá»¥ng cÃ¡c khá»‘i FFT sáº½ lÃ m sai lá»‡ch giá»¯a Ã¢m vÃ  chuá»—i cÃ¡c phá»• cá»§a chÃºng. **Length Regulator (*LR*)** sinh ra Ä‘á»ƒ giáº£i quyáº¿t váº¥n Ä‘á» Ä‘Ã³, ngoÃ i ra *LR* cÃ²n giÃºp Ä‘iá»u khiá»ƒn tá»‘c Ä‘á»™ vÃ  ngá»¯ Ä‘iá»‡u cá»§a giá»ng nÃ³i. Nghe tháº­t thÃº vá»‹ pháº£i khÃ´ng? Cáº¥u trÃºc cá»§a nÃ³ nhÆ° sau:

![](https://images.viblo.asia/b15d752d-467e-4d72-8c86-e6ef646e7255.png)

ThÃ´ng thÆ°á»ng, má»—i mÃ´ hÃ¬nh sáº½ cÃ³ cÃ¡c tham sá»‘ Ä‘á»ƒ tÃ­nh mel-spectrogram cá»‘ Ä‘á»‹nh, má»—i cá»­a sá»• sáº½ tÆ°Æ¡ng á»©ng vá»›i má»™t thá»i gian nhá», má»—i má»™t Ã¢m sáº½ cÃ³ Ä‘á»™ dÃ i khÃ¡c nhau do Ä‘Ã³ sáº½ tÆ°Æ¡ng á»©ng vá»›i sá»‘ lÆ°á»£ng cÃ¡c cá»­a sá»• phá»• khÃ¡c nhau, kÃ½ hiá»‡u sá»‘ nÃ y lÃ  $d$, gá»i lÃ  thá»i lÆ°á»£ng Ã¢m. Thá»i lÆ°á»£ng Ã¢m sáº½ Ä‘Æ°á»£c dá»± Ä‘oÃ¡n qua bá»™ dá»± Ä‘oÃ¡n **"Duration Predictor"**. Sau Ä‘Ã³ dá»±a vÃ o $d$, *LR* sáº½ Ä‘iá»u chá»‰nh kÃ­ch thÆ°á»›c cá»§a vector tráº¡ng thÃ¡i áº©n báº±ng cÃ¡ch láº·p láº¡i cÃ¡c tráº¡ng thÃ¡i má»™t sá»‘ láº§n báº±ng vá»›i thá»i lÆ°á»£ng Ã¢m tÆ°Æ¡ng á»©ng trong $d$ Nghe trÃ¬u tÆ°á»£ng quÃ¡ nhá»‰, hÃ£y cÃ¹ng xem xÃ©t vÃ­ dá»¥ sau:

Giáº£ sá»­ $\mathcal{H}_{pho} = [h_1, h_2, ..., h_n]$ ($n$ lÃ  Ä‘á»™ dÃ i cá»§a chuá»—i Ã¢m), vá»›i thá»i lÆ°á»£ng Ã¢m $\mathcal{D} = [d_1, d_2, ..., d_n]$ thá»a mÃ£n $\Sigma^{n}_{i=1} d_i = m$ vá»›i $m$ lÃ  Ä‘á»™ dÃ i chuá»—i mel-spectrogram. Bá»™ Ä‘iá»u chá»‰nh thá»i lÆ°á»£ng Ã¢m $\mathcal{LR}$ lÃ  cÃ³ tÃ¡c dá»¥ng nhÆ° má»™t hÃ m:
$$
\mathcal{H}_{mel} = \mathcal{LR}(\mathcal{H}_{pho},\mathcal{D}, \alpha)
$$
vá»›i $\alpha$ lÃ  má»™t hyperparameter Ä‘á»ƒ xÃ¡c Ä‘á»‹nh kÃ­ch thÆ°á»›c chuá»—i mel-spectrogram $\mathcal{H}_{mel}$ vÃ  tá»« Ä‘Ã³ kiá»ƒm soÃ¡t tá»‘c Ä‘á»™ cá»§a giá»ng Ä‘á»c. Cá»¥ thá»ƒ hÆ¡n, xÃ©t
$$\mathcal{H}_{pho} = [h_1, h_2, h_3, h_4] vÃ  \mathcal{D} = [2, 2, 3, 1]$$
thÃ¬ vá»›i $\alpha = 1$:
$$\mathcal{H}_{mel} = [h_1, h_1, h_2, h_2, h_3, h_3, h_3, h_4]$$
Khi $\alpha = 1.3$ hoáº·c $\alpha = 0.5$ thá»i lÆ°á»£ng Ã¢m lÃºc nÃ y sáº½ Ä‘Æ°á»£c Ä‘iá»u chá»‰nh:
$$\mathcal{D}_{\alpha=1.3} = [2.6, 2.6, 3.9, 1.3] \approx [3, 3, 4, 1]$$
$$\mathcal{D}_{\alpha=0.5} = [1, 1, 1.5, 0.5] \approx [1, 1, 2, 1]$$
lÃºc nÃ y:
$$\mathcal{H}_{mel} =[h_1, h_1, h_1, h_2, h_2, h_2, h_3, h_3, h_3, h_3, h_4] (\alpha=1.3)$$
$$\mathcal{H}_{mel} =[h_1, h_2, h_3, h_3, h_4] (\alpha=0.5)$$
NgoÃ i Ä‘iá»u chá»‰nh tá»‘c Ä‘á»™ Ä‘á»c, $\mathcal{LR}$ cÃ²n cÃ³ thá»ƒ Ä‘iá»u chá»‰nh thá»i lÆ°á»£ng cá»§a cÃ¡c khoáº£ng ngáº¯t Ä‘á»ƒ cho giá»ng Ä‘á»c Ä‘a dáº¡ng hÆ¡n.

## 3. Duration Predictor

Má»™t thÃ nh pháº§n khÃ´ng thá»ƒ thiáº¿u cá»§a **Length Regulator** chÃ­nh lÃ  **Duration Predictor** - thÃ nh pháº§n dá»± Ä‘oÃ¡n má»—i Ã¢m vá»‹ tÆ°Æ¡ng á»©ng vá»›i bao nhiÃªu cá»­a sá»• mel-spectrogram. Cáº¥u trÃºc cá»§a **Duration Predictor** nhÆ° sau:

![](https://images.viblo.asia/0a0e5a22-5f5b-4d4b-ace0-5051009b373f.png)

Má»—i khá»‘i **Duration Predictor** bao gá»“m 2 lá»›p **Conv1D** vá»›i hÃ m kÃ­ch hoáº¡t **ReLU** theo sau lÃ   **layer normalization** vÃ  lá»›p **dropout**, cuá»‘i cÃ¹ng lÃ  má»™t lá»›p **Linear**.

Tuy nhiÃªn quÃ¡ trÃ¬nh huáº¥n luyá»‡n cÃ¡c tham sá»‘ cá»§a **Duration Predictor** cÃ³ hÆ¡i Ä‘áº·c biá»‡t: tÃ¡c giáº£ sá»­ dá»¥ng má»™t mÃ´ hÃ¬nh gá»i lÃ  ***"teacher model"*** dá»±a trÃªn máº¡ng há»“i quy Transformer Ä‘á»ƒ trÃ­ch xuáº¥t thá»i lÆ°á»£ng cá»§a má»—i Ã¢m vá»‹, cá»¥ thá»ƒ vá»›i má»—i cáº·p dá»¯ liá»‡u, trÃ­ch xuáº¥t cÃ¡c **attention alignments** cá»§a **decoder-to-encoder** trong **teacher model**. Bá»Ÿi vÃ¬ mÃ´ hÃ¬nh sá»­ dá»¥ng **multi-head attention** nÃªn sáº½ cÃ³ nhiá»u head cá»§a attention. Tuy nhiÃªn khÃ´ng pháº£i táº¥t cáº£ cÃ¡c head nÃ y Ä‘á»u Ä‘Æ°á»£c sá»­ dá»¥ng mÃ  tÃ¡c giáº£ Ä‘Ã£ Ä‘Æ°a ra má»™t tá»‰ lá»‡ (**focus rate**) 
$$F = \frac{1}{S}\sum_{s=1}^{S}{\max_{1 \leq t \leq T} a_{s,t}}$$
trong Ä‘Ã³ $S$ vÃ  $T$ láº§n lÆ°á»£t lÃ  Ä‘á»™ dÃ i cá»§a chuá»—i mel-spectrogram vÃ  chuá»—i Ã¢m vá»‹,  $a_{s,t}$ lÃ  pháº§n tá»­ á»Ÿ hÃ ng $s$-th cá»™t $t$-th cá»§a ma tráº­n attention. TÃ­nh giÃ¡ trá»‹ $F$ cho tá»«ng head vÃ  láº¥y head á»©ng cÃ³ giÃ¡ trá»‹ cá»§a $F$ lÃ  lá»›n nháº¥t. Sau cÃ¹ng, tÃ­nh thá»i lÆ°á»£ng cá»§a Ã¢m $\mathcal{D} = [d_1, d_2, ..., d_n]$ theo cÃ´ng thá»©c 
$$d_i = \sum_{s=1}^S [\mathop{\arg\max}_{t} a_{s,t} = i]$$
VÃ  cuá»‘i cÃ¹ng chÃºng ta Ä‘Ã£ cÃ³ Ä‘Æ°á»£c *ground-truth* cá»§a thá»i lÆ°á»£ng Ã¢m $\mathcal{D}$. GiÃ¡ trá»‹ nÃ y sau Ä‘Ã³ Ä‘Æ°á»£c tÃ­nh lá»—i báº±ng MSE vÃ  Ä‘Æ°á»£c cáº­p nháº­t cÃ¡c tham sá»‘ cá»§a **Duration Predictor**.

# III. Lá»i káº¿t
 Cáº£m Æ¡n cÃ¡c báº¡n Ä‘Ã£ theo dÃµi bÃ i viáº¿t cá»§a mÃ¬nh Ä‘áº¿n Ä‘Ã¢y. NhÆ° váº­y lÃ  mÃ¬nh vá»«a trÃ¬nh bÃ y vá» kiáº¿n trÃºc cá»§a mÃ´ hÃ­nh FastSpeech, má»™t mÃ´ hÃ¬nh cÃ³ tá»‘c Ä‘á»™ cao, máº¡nh máº½ vÃ  cÃ³ kháº£ nÄƒng kiá»ƒm soÃ¡t cÃ¡c há»‡ thá»‘ng Text To Speech. CÅ©ng giá»‘ng nhÆ° cÃ¡c mÃ´ hÃ¬nh TTS khÃ¡c, Ä‘á»ƒ cÃ³ thá»ƒ táº¡o ra Ä‘Æ°á»£c tiáº¿ng nÃ³i cáº§n sá»­ dá»¥ng thÃªm mÃ´ hÃ¬nh tá»•ng há»£p tiáº¿ng nÃ³i. Chi tiáº¿t cÃ¡c báº¡n cÃ³ thá»ƒ tham kháº£o trong bÃ i bÃ¡o gá»‘c cá»§a tÃ¡c giáº£. NgoÃ i ra trong bÃ i bÃ¡o tÃ¡c giáº£ cÃ²n nÃ³i Ä‘áº¿n quÃ¡ trÃ¬nh chuáº©n bá»‹ dá»¯ liá»‡u vÃ  huáº¥n luyá»‡n mÃ´ hÃ¬nh, link cá»§a bÃ i mÃ¬nh sáº½ Ä‘á»ƒ dÆ°á»›i pháº§n tham kháº£o. Má»™t láº§n ná»¯a cáº£m Æ¡n cÃ¡c báº¡n Ä‘Ã£ theo dÃµi bÃ i viáº¿t cá»§a mÃ¬nh, náº¿u cÃ³ thá»ƒ thÃ¬ vui lÃ²ng cho mÃ¬nh xin 1 vote ğŸ˜.
 
 # Tham kháº£o
 1. [FastSpeech: Fast, Robust and Controllable Text to Speech](https://arxiv.org/pdf/1905.09263.pdf)