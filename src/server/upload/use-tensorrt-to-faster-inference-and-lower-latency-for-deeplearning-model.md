# TensorRT l√† g√¨ ?
TensorRT l√† m·ªôt th∆∞ vi·ªán ƒë∆∞·ª£c ph√°t tri·ªÉn b·ªüi NVIDIA nh·∫±m c·∫£i thi·ªán t·ªëc ƒë·ªô inference, gi·∫£m ƒë·ªô tr√¨ tru·ªá tr√™n c√°c thi·∫øt b·ªã ƒë·ªì h·ªça NVIDIA(GPU). N√≥ c√≥ th·ªÉ c·∫£i thi·ªán t·ªëc ƒë·ªô suy lu·∫≠n l√™n ƒë·∫øn 2-4 l·∫ßn so v·ªõi c√°c d·ªãch v·ª• real-time v√† nhanh h∆°n g·∫•p 30 l·∫ßn so v·ªõi hi·ªáu su·∫•t c·ªßa CPU.<br>
![](https://images.viblo.asia/bf05b3b6-13c5-4379-ba24-10b24ef5a177.jpg)
N·ªôi dung b√†i n√†y ch√∫ng ta t·∫≠p trung m·ªôt s·ªë v·∫•n ƒë·ªÅ sau ƒë√¢y:
* üëçT·∫°i sao TensorRT c·∫£i thi·ªán t·ªëc ƒë·ªô inference
* üëçVi·ªác t·ªëc ƒë·ªô ƒë∆∞·ª£c c·∫£i thi·ªán c√≥ c·∫ßn ƒë√°nh ƒë·ªïi c√°i g√¨ kh√¥ng?
* üëçL√†m sao ƒë·ªÉ s·ª≠ d·ª•ng TensorRT tr√™n deep learning?
# TensorRT c·∫£i thi·ªán vi·ªác t·ªëi ∆∞u nh∆∞ th·∫ø n√†o ?
TensorRT s·∫Ω th·ª±c hi·ªán 5 lo·∫°i t·ªëi ∆∞u ƒë·ªÉ tƒÉng hi·ªáu su·∫•t inference. Ch√∫ng ta s·∫Ω th·ªèa lu·∫≠n 5 lo·∫°i n√†y ·ªü b√™n d∆∞·ªõi. <br>
![](https://images.viblo.asia/0d0e6872-16be-4b66-9990-35cca7282bb8.png)<br>
## 1. Precision Calibration
Trong su·ªët qu√° tr√¨nh training, c√°c tham s·ªë v√† h√†m k√≠ch ho·∫°t activations trong ƒë·ªô ch√≠nh x√°c FP32(Float Point 32) s·∫Ω ƒë∆∞·ª£c convert v·ªÅ ƒë·ªô ch√≠nh x√°c FP16 ho·∫∑c INT8. Vi·ªác t·ªëi ∆∞u n√≥ s·∫Ω gi·∫£m ƒë·ªô tr√¨ tru·ªá v√† tƒÉng t·ªëc ƒë·ªô suy lu·∫≠n nh∆∞ng ph·∫£i tr·∫£ m·ªôt c√°i gi√° l√† ph·∫£i gi·∫£m ƒë·ªô ch√≠nh x√°c c·ªßa model m·∫∑c d√π kh√¥ng ƒë√°ng k·ªÉ. Trong nh·∫≠n di·ªán real-time th√¨ ƒë√¥i khi v·ªã·ªát ƒë√°nh ƒë·ªïi ƒë·ªô ch√≠nh x√°c so v·ªõi t·ªëc ƒë·ªô suy lu·∫≠n l√† c·∫ßn thi·∫øt.<br>
![](https://images.viblo.asia/fa31e668-c908-4964-bf88-bebc8a9f0ef9.jpg)

## 2. Layer & Tensor Fusion
TensorRT n√≥ s·∫Ω g·ªôp layer and tensor ƒë·ªÉ t·ªëi ∆∞u h√≥a b·ªô nh·ªõ GPU v√† bƒÉng th√¥ng b·ªüi vi·ªác g·ªôp c√°c nodel theo chi·ªÅu d·ªçc, chi·ªÅu ngang ho·∫∑c c·∫£ hai.<br>
* Improve GPU utilization - less kernel launch overhead, better memory usage and bandwidth
* Vertical fusion = Combine sequential kernel calls
* Horizontal fusion = Combine same kernels that have common input but different weights
![](https://images.viblo.asia/a3f0c02a-3000-4f94-b963-35c53889789a.png)

## 3. Kernel auto-tuning
Trong qu√° tr√¨nh t·ªëi ∆∞u model, m·ªôt v√†i kernel ri√™ng d√†nh cho vi·ªác t·ªëi ∆∞u s·∫Ω th·ª±c thi trong su·ªët ti·∫øn tr√¨nh.<br>
* There are multiple low-level algorithms/implementations for common operations
* TensorRT selects the optimal kernels based on your parameters e.g: batch_size, filter-size, input data size.
* TensorRT selects the optimal kernel based on your target platform.
## 4. Dynamic Tensor Memory
* Allocates just the memory required for each tensor and only for the duration of its usage
* Reduces memory footprint and improves memory re-use
## 5. Multiple Stream Execution
* Allows processing multiple input streams in parallel
# Workflow
ƒê·ªÉ √°p d·ª•ng TensorRT tr√™n deep learning th√¨ ch√∫ng ta c·∫ßn ph·∫£i convert model t·ªõi model-TRT theo nh∆∞ lu·ªìng bi·ªÉu ƒë·ªì sao ƒë√¢y. <br>
![](https://images.viblo.asia/885ab7e6-350a-4664-abb0-8d0751de308a.png)

# Code
## 1. C√†i ƒë·∫∑t m√¥i tr∆∞·ªùng TensorRT
ƒê·ªÉ c√†i ƒë·∫∑t TensorRT tr√™n h·ªá th·ªëng c·ªßa b·∫°n th√¨ c·∫ßn m·ªôt s·ªë y√™u c·∫ßu sau ƒë√¢y:
* NVIDIA-GPU
* Tensorflow-GPU >=2.0
```
pip install tensorflow-gpu==2.0.0

wget https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb

dpkg -i nvidia-machine-learning-repo-*.deb
apt-get update

sudo apt-get install libnvinfer5
pip install 'h5py==2.10.0' --force-reinstall
```
## 2. Convert model ResNet-50 to TF-TRT
C√°c b·∫°n c√≥ th·ªÉ convert c√°c model kh√°c t·ªõi TensorRT, ·ªü ƒë√¢y m√¨nh l·∫•y v√≠ d·ª• l√† ResNet-50.<br>
```python
from tensorflow.python.compiler.tensorrt import trt_convert as trt
from tensorflow.keras.applications.resnet50 import ResNet50
import os
import numpy as np
# Load model =>Predict => save model
model = ResNet50(weights='imagenet')
model.save('/content/resnet50_saved_model')
# Convert to TF_TRT => SavedModel

print('Converting to TF-TRT FP32 or FP16 or INT8...')
# N·∫øu convert TF-TRT FP32 : trt.TrtPrecisionMode.FP32
# N·∫øu convert TF-TRT FP16 : trt.TrtPrecisionMode.FP16
# N·∫øu convert TF-TRT INT8 : trt.TrtPrecisionMode.INT8
conversion_params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(precision_mode=trt.TrtPrecisionMode.FP16,
                                                               max_workspace_size_bytes=8000000000)
converter = trt.TrtGraphConverterV2(input_saved_model_dir='resnet50_saved_model',
                                    conversion_params=conversion_params)
# Converter method used to partition and optimize TensorRT compatible segments
converter.convert()

# Save the model to the disk 
converter.save(output_saved_model_dir='resnet50_saved_model_TFTRT_FP32')
print('Done Converting to TF-TRT FP32')
```
## 3. Load l·∫°i model ƒë√£ convert
```python
# Load converted model and infer
import tensorflow as tf
root = tf.saved_model.load('/content/resnet50_saved_model_TFTRT_FP32')
infer = root.signatures['serving_default']
# output = infer(input_tensor)
print(infer)
```
# K·∫øt qu·∫£
ƒê·ªÉ so s√°nh k·∫øt qu·∫£ c·ªßa vi·ªác s·ª≠ d·ª•ng TensorRT v·ªõi vi·ªác inference native th√¨ m√¨nh Inference ResNet-50 tr√™n TF-TRT FP32, FP16, INT8 v√† native. <br>
```python
wget https://raw.githubusercontent.com/tensorflow/tensorrt/master/tftrt/blog_posts/Leveraging%20TensorFlow-TensorRT%20integration%20for%20Low%20latency%20Inference/tf2_inference.py
python tf2_inference.py --use_tftrt_model --precision fp16
python tf2_inference.py --use_tftrt_model --precision fp32
python tf2_inference.py --use_tftrt_model --precision int8
python tf2_inference.py --use_native_tensorflow

```


| FP16|  FP32 | Native|
| -------- | -------- | -------- |
|  Average step time: 2.1 msec       | Average step time: 2.5 msec     | Average step time: 4.1 msec     |
| Average throughput: 244248 samples/sec     | Average throughput: 240145 samples/sec| Average throughput: 126328 samples/sec|

<br>
Qua ƒë√≥ ta th·∫•y ƒë∆∞·ª£c vi·ªác khi convert model sang TensorRT s·∫Ω tƒÉng t·ªëc ƒë·ªô inference v√† gi·∫£m ƒë·ªô tr√¨ tru·ªá kh√° ƒë√°ng k·ªÉ so v·ªõi inference truy·ªÅn th·ªëng.

# T√†i li·ªáu tham kh·∫£o
To√†n b·ªô code : https://colab.research.google.com/drive/15m95GzznIoCRn1XnMQXd9L-onpJiCWM3?usp=sharing<br>
Link t√†i li·ªáu tham kh·∫£o :
* https://blog.tensorflow.org/2021/01/leveraging-tensorflow-tensorrt-integration.html
* https://www.youtube.com/watch?v=Gd-9cUb7x5Q
* https://www.youtube.com/watch?v=67ev-6Xn30U
* [blog](https://medium.com/@abhaychaturvedi_72055/understanding-nvidias-tensorrt-for-deep-learning-model-optimization-dad3eb6b26d9#:~:text=TensorRT%20is%20a%20library%20developed,CUDA%2C%20NVIDIA's%20parallel%20programming%20model.&text=While%20as%20per%20the%20documentation,compared%20to%20CPU%20only%20performance.)
* https://developer.nvidia.com/tensorrt