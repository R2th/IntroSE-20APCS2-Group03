# Abstract 
ChÃ o cÃ¡c báº¡n, hÃ´m nay chá»§ Ä‘á» mÃ¬nh Ä‘Æ°a ra khÃ¡ lÃ  chung chung. Tuy nhiÃªn, theo gÃ³c nhÃ¬n cá»§a má»™t Ä‘á»©a cÅ©ng cháº­m chá»¯ng bÆ°á»›c chÃ¢n vÃ o lÄ©nh vá»±c nÃ y thÃ¬ mÃ¬nh tháº¥y ná»™i dung nÃ y khÃ¡ cáº§n thiáº¿t Ä‘á»ƒ giÃºp cÃ¡c báº¡n tiáº¿p cáº­n vÃ  hiá»ƒu hÆ¡n vá» Machine Learning cÅ©ng nhÆ° workflow cá»§a Machine Learning 

BÃ i viáº¿t nÃ y mÃ¬nh cÃ³ cÃ´ng nhÆ° lÃ  phiÃªn dá»‹ch cá»§a Machine Learning Roadmap (2020), thÃªm vÃ o Ä‘Ã³ lÃ  cÃ¡c ná»™i dung bá»• sung, kinh nghiá»‡m cá»§a mÃ¬nh trong quÃ¡ trÃ¬nh há»c táº­p vÃ  lÃ m viá»‡c. Ã€, bÃ i viáº¿t cá»§a mÃ¬nh chá»§ yáº¿u Ä‘Æ°a ra cÃ¡c tá»« khÃ³a chá»© khÃ´ng phÃ¢n tÃ­ch sÃ¢u Ä‘á»ƒ cÃ¡c báº¡n náº¿u muá»‘n Ä‘i sÃ¢u hÆ¡n thÃ¬ cÃ³ thá»ƒ tÃ¬m kiáº¿m cÃ¡c tá»« khÃ³a Ä‘Ã³ nhÃ©.

BÃ i viáº¿t Ä‘Æ°á»£c láº¥y cáº£m há»©ng tá»« bÃ i chia sáº» bá»• Ã­ch cá»§a má»™t báº¡n trong nhÃ³m Machine Learning cÆ¡ báº£n https://www.facebook.com/vefacademy/photos/a.1010757529668990/1013281886083221/

ChÃºc má»i ngÆ°á»i cÃ³ nhá»¯ng phÃºt giÃ¢y Ä‘á»c tháº­t thÆ° giÃ£n!

**Machine Leanring Process:** Gá»“m cÃ¡c pháº§n lá»›n sau
1. Data Collection
2. Data Preparation
3. Train model
4.  Analysis â€“ Evaluation 

Giá»›i thiá»‡u qua váº­y thÃ´i, cÃ¹ng Ä‘i sÃ¢u vÃ o tá»«ng pháº§n Ä‘á»ƒ hiá»ƒu rÃµ hÆ¡n nhÃ©

## I. Data Collection
### 1. YÃªu cáº§u
* Loáº¡i váº¥n Ä‘á» cáº§n giáº£i quyáº¿t lÃ  gÃ¬?
* Nguá»“n dá»¯ liá»‡u (data) cÃ³ tá»“n táº¡i hay khÃ´ng?
* Vá» quyá»n riÃªng tÆ° cá»§a dá»¯ liá»‡u? Data public? CÃ¡c váº¥n Ä‘á» phÃ¡t sinh?
* NÃªn lÆ°u trá»¯ dá»¯ liá»‡u á»Ÿ Ä‘Ã¢u?

### 2. Kiá»ƒu dá»¯ liá»‡u
**Dá»¯ liá»‡u cÃ³ cáº¥u trÃºc (Structured Data)**: Xuáº¥t hiá»‡n dáº¡ng báº£ng, cÃ³ nhiá»u kiá»ƒu dá»¯ liá»‡u khÃ¡c nhau
* Nomical/Categorical
* Numerical
* Ordinal 
* Time series 

**Dá»¯ liá»‡u khÃ´ng cÃ³ cáº¥u trÃºc (Unstructured Data)**: vÃ­ dá»¥ nhÆ° áº£nh, video, vÄƒn báº£n ngÃ´n ngá»¯ tá»± nhiÃªn, giá»ng nÃ³i, â€¦

## II. Data Preparation 
### 1. PhÃ¢n tÃ­ch dá»¯ liá»‡u, hiá»ƒu vá» dá»¯ liá»‡u mÃ¬nh Ä‘ang lÃ m viá»‡c cÃ¹ng
* Äáº·c Ä‘iá»ƒm dá»¯ liá»‡u Ä‘áº§u vÃ o? Má»¥c tiÃªu Ä‘áº§u ra lÃ  gÃ¬?
* Loáº¡i dá»¯ liá»‡u lÃ m viá»‡c cÃ¹ng? (Structured, Unstructured, â€¦)
* CÃ³ tá»“n táº¡i cÃ¡c giÃ¡ trá»‹ bá»‹ máº¥t? (CÃ¡i nÃ y mÃ¬nh hay gáº·p khÃ¡ nhiá»u, dá»¯ liá»‡u á»Ÿ má»™t sá»‘ vá»‹ trÃ­ bá»‹ thiáº¿u) HÆ°á»›ng giáº£i quyáº¿t? (Loáº¡i bá», sá»­a, thÃªm, â€¦)
* CÃ³ xuáº¥t hiá»‡n cÃ¡c ngoáº¡i lai (outliers) khÃ´ng? ChÃºng thÆ°á»ng xuáº¥t hiá»‡n á»Ÿ Ä‘Ã¢u? CÃ³ nhiá»u khÃ´ng? Tá»· lá»‡ tháº¿ nÃ o vá»›i toÃ n bá»™ táº­p dá»¯ liá»‡u?
### 2. Tiá»n xá»­ lÃ½ Ä‘á»ƒ Ä‘Æ°a dá»¯ liá»‡u vÃ o mÃ´ hÃ¬nh
TÃ¹y bÃ i toÃ¡n mÃ  viá»‡c tiá»n xá»­ lÃ½ dá»¯ liá»‡u sáº½ khÃ¡c nhau, chÃºng ta khÃ´ng thá»ƒ Ã¡p dá»¥ng tiá»n xá»­ lÃ½ áº£nh cho tiá»n xá»­ lÃ½ tá»± nhiÃªn Ä‘Æ°á»£c :)

DÆ°á»›i Ä‘Ã¢y lÃ  má»™t vÃ i ná»™i dung khÃ¡i quÃ¡t cá»§a viá»‡c tiá»‡n xá»­ lÃ½ dá»¯ liá»‡u:

* Xá»­ lÃ½ dá»¯ liá»‡u bá»‹ thiáº¿u:
    * TÃ­nh trung bÃ¬nh cá»™t Ä‘á»ƒ dÃ¹ng cho Ã´ bá»‹ thiáº¿u, mÃ´ hÃ¬nh hÃ³a cÃ¡c giÃ¡ trá»‹ cÃ²n thiáº¿u, ...
    * Sá»­ dá»¥ng K-Nearest Neighbors (K-NN), Random Imputation, Sliding Window, â€¦
* Feature Encoding (NÃªn nhá»› mÃ´ hÃ¬nh cá»§a chÃºng ta chá»‰ hiá»ƒu cÃ¡c dá»¯ liá»‡u dáº¡ng sá»‘, dÃ¹ Ä‘áº§u vÃ o cÃ³ lÃ  text, hÃ¬nh áº£nh hay Ã¢m thanh thÃ¬ chÃºng ta Ä‘á»u cáº§n pháº£i chuyá»ƒn Ä‘á»•i vá» dáº¡ng sá»‘)
    * One-hot encoding
    * Label Encoder
    * Embedding Encoder
* Feature Normalization: Thá»­ hÃ¬nh dung dá»¯ liá»‡u Ä‘áº§u vÃ o cá»§a báº¡n gá»“m Ä‘á»§ thá»© nhÆ° giÃ¡ nhÃ , diá»‡n tÃ­ch, sá»‘ phÃ²ng ngá»§, sá»‘ táº§ng, â€¦ mÃ  má»—i trÆ°á»ng Ä‘Ã³ láº¡i cÃ³ vÃ¹ng giÃ¡ trá»‹ khÃ¡c nhau, trÆ°á»ng thÃ¬ chá»‰ lÃ  3, 4, 5 (táº§ng), cÃ³ trÆ°á»ng láº¡i lÃªn tá»›i 100, 1000 (diá»‡n tÃ­ch) váº­y nÃªn viá»‡c chuáº©n hÃ³a lÃ  vÃ´ cÃ¹ng quan trá»ng. Trong pháº§n nÃ y sáº½ cÃ³ 2 tá»« khÃ³a chÃ­nh lÃ  Scaling vÃ  Standardization 
    * Feature Scaling: Ä‘Æ¡n giáº£n lÃ  ta sáº½ Ä‘Æ°a cÃ¡c giÃ¡ trá»‹ vá» thuá»™c khoáº£ng [0, 1] báº±ng cÃ¡ch tÃ¬m ra min, max Ä‘á»‘i vá»›i tá»«ng trÆ°á»ng dá»¯ liá»‡u, rá»“i chuáº©n hÃ³a. Táº¥t nhiÃªn cÃ¡c báº¡n pháº£i hiá»ƒu 0.3 cá»§a trÆ°á»ng dá»¯ liá»‡u nÃ y khÃ¡c vÃ  khÃ´ng liÃªn quan tá»›i 0.3 cá»§a trÆ°á»ng dá»¯ liá»‡u khÃ¡c (vÃ­ dá»¥ sá»‘ phÃ²ng vá»›i giÃ¡ nhÃ  cháº³ng háº¡n)
    * Feature Standardization: Chuáº©n hÃ³a Ä‘á»ƒ cÃ¡c giÃ¡ trá»‹ cÃ³ giÃ¡ trá»‹ trung bÃ¬nh báº±ng 0 vÃ  phÆ°Æ¡ng sai Ä‘Æ¡n vá»‹. NÃ³ tÃ­nh báº±ng cÃ¡ch trá»« giÃ¡ trá»‹ trung bÃ¬nh vÃ  chia cho Ä‘á»™ lá»‡ch chuáº©n cá»§a má»™t Ä‘áº·c trÆ°ng cá»¥ thá»ƒ. Vá»›i phÆ°Æ¡ng phÃ¡p nÃ y thÃ¬ giÃ¡ trá»‹ cÃ³ thá»ƒ khÃ´ng thuá»™c [0, 1]. Äiá»u nÃ y giÃºp phÆ°Æ¡ng phÃ¡p máº¡nh máº½ Ä‘á»‘i vá»›i váº¥n Ä‘á» ngoáº¡i lai
* Feature Engineering: Biáº¿n Ä‘á»•i dá»¯ liá»‡u thÃ nh cÃ³ Ã½ nghÄ©a Ä‘áº¡i diá»‡n hÆ¡n. Dá»¯ liá»‡u ban Ä‘áº§u báº¡n cÃ³ chá»‰ lÃ  dá»¯ liá»‡u thÃ´, chÆ°a xá»­ lÃ½. Má»™t sá»‘ kÄ© thuáº­t cÃ³ thá»ƒ káº¿ tá»›i nhÆ°:
    * Decompose: vÃ­ dá»¥ vá»›i thÃ´ng tin 03-02-2021 ta cÃ³ thá»ƒ phÃ¢n rÃ£ ra cÃ¡c thÃ´ng tin nhÆ° ngÃ y nÃ o, ngÃ y bao nhiÃªu cá»§a nÄƒm, ngÃ y bao nhiÃªu cá»§a tuáº§n, thá»© máº¥y, â€¦
    * Discretization: CÃ¡c báº¡n cÃ³ thá»ƒ hiá»ƒu lÃ  gom nhÃ³m, tÃ¹y vÃ o má»¥c Ä‘Ã­ch bÃ i toÃ¡n. VÃ­ dá»¥ vá»›i giÃ¡ trá»‹ tuá»•i tÃ¡c, ta cÃ³ thá»ƒ chuyá»ƒn tá»« cÃ¡c con sá»‘ cá»¥ thá»ƒ hay nhÃ³m nhá» thÃ nh nhÃ³m lá»›n hÆ¡n, tá»« 20-40, hay dÆ°á»›i 50, â€¦
    * Crossing and Interaction features: ta sáº½ combine cÃ¡c features láº¡iÄ‘á»ƒ táº¡o ra má»™t feature má»›i 
    * Indicator features: Sá»­ dá»¥ng cÃ¡c thÃ nh pháº§n cá»§a dá»¯ liá»‡u Ä‘á»ƒ chá»‰ ra má»™t sá»‘ thá»© cÃ³ thá»ƒ quan trá»ng
    * Feature selection: Cháº¡y nhá»¯ng features cÃ³ giÃ¡ trá»‹ nháº¥t trong dataset Ä‘á»ƒ sá»­ dá»¥ng cho model (Äiá»u nÃ y cÃ³ thá»ƒ giÃºp model giáº£m overfitting, giáº£m thá»i gian training, tÄƒng accuracy, giáº£m sá»‘ chiá»u dá»¯ liá»‡u, â€¦)
    * Dealing with imbalance: Trong bÃ i toÃ¡n ML thÃ¬ sáº½ khÃ´ng Ã­t láº§n chÃºng ta gáº·p váº¥n Ä‘á» máº¥t cÃ¢n báº±ng dá»¯ liá»‡u (vÃ­ dá»¥ class A cÃ³ 100 áº£nh, class non-A cÃ³ 100.000 áº£nh). Má»™t sá»‘ giáº£i phÃ¡p Ä‘Æ°a ra:
        * Thu tháº­p thÃªm dá»¯ liá»‡u
        * Sá»­ dá»¥ng scikit-learn-contrib imbalance-learn package, má»™t thÆ° viá»‡n cá»§a python giÃºp há»— trá»£ giáº£i quyáº¿t váº¥n Ä‘á» imbalance data 
        * Tham kháº£o má»™t sá»‘ bÃ i bÃ¡o vá» giáº£i quyáº¿t váº¥n Ä‘á» imbalance Ä‘á»ƒ hiá»ƒu kÄ© thuáº­t xá»­ lÃ½ cá»§a há». VÃ­ dá»¥ nhÆ° sá»­ dá»¥ng má»™t hÃ m loss cÃ³ tÃªn lÃ  Focal Loss
### 3. Data Splitting 
ThÃ´ng thÆ°á»ng sáº½ chia táº­p dataset lÃ m 3 pháº§n chÃ­nh:
* Training set (70-80%): ÄÃ¢y lÃ  pháº§n dá»¯ liá»‡u mÃ´ hÃ¬nh sáº½ sá»­ dá»¥ng Ä‘á»ƒ há»c, Ä‘iá»u chá»‰nh cÃ¡c parameters 
* Validation set (10-15%): Pháº§n dá»¯ liá»‡u sáº½ giÃºp chÃºng ta Ä‘iá»u chá»‰nh cÃ¡c hyper-parameters Ä‘á»ƒ mÃ´ hÃ¬nh tá»‘t hÆ¡n, trÃ¡nh cÃ¡c váº¥n Ä‘á» overfitting. Pháº§n dá»¯ liá»‡u nÃ y giá»‘ng nhÆ° má»™t bÃ i kiá»ƒm tra 15â€™, kiá»ƒm tra trong tá»«ng bÆ°á»›c há»c cá»§a báº¡n xem báº¡n cÃ³ há»c tá»‘t tháº­t khÃ´ng hay há»c váº¹t 
* Test set (10-15%): Pháº§n dá»¯ liá»‡u nÃ y sáº½ dÃ¹ng Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh, khÃ´ng sá»­ dá»¥ng Ä‘á»ƒ tuning model. Pháº§n dá»¯ liá»‡u nÃ y cÃ³ thá»ƒ coi lÃ  thi cuá»‘i kÃ¬, Ä‘Ã¡nh giÃ¡ tá»•ng thá»ƒ quÃ¡ trÃ¬nh há»c.
## III. Training model on data 
### 1. Chá»n thuáº­t toÃ¡n phÃ¹ há»£p
* Supervised Algorithms:
    * Linear regression
    * Logistic regression
    * K-Nearest Neighbors
    * Support Vector Machines
    * Decision Trees â€“ Random Forests
    * AdaBoost/Gradient Boosting Machines
    * Neural Network 
        * Convolutional neural networks
        * Recurrent neural networks
        * Transformer networks
* Unsupervised Algorithms:
    * Clustering
    * Visualization and dimensionality reduction
        * Principal Component Analysis (PCA)
        * Autoencoders
        * T-Distributed Stochastic Neighbor Embedding (t-SNE)
    * An anomaly detection
        * Autoencoder
        * One-class classification
### 2. Lá»±a chá»n cÃ¡ch há»c cho model 
* Batch learning: Táº¥t cáº£ data tá»“n táº¡i trong má»™t â€œbig static warehouseâ€, cáº§n train model vá»›i nÃ³ (Data warehouse Ä‘Æ°á»£c hiá»ƒu lÃ  má»™t kÄ© thuáº­t thu tháº­p vÃ  quáº£n lÃ½ dá»¯ liá»‡u tá»« nhiá»u nguá»“n khÃ¡c nhau Ä‘á»ƒ cung cáº¥p nhá»¯ng hiá»ƒu biáº¿t nghiá»‡p vá»¥ cÃ³ Ã½ nghÄ©a, nÃ³ lÃ  bá»™ lÆ°u trá»¯ Ä‘iá»‡n tá»­ lÆ°u trá»¯ má»™t sá»‘ lÆ°á»£ng lá»›n thÃ´ng tin, Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ truy váº¥n vÃ  phÃ¢n tÃ­ch, thay vÃ¬ xá»­ lÃ½ giao dá»‹ch. Data warehouse â€“ cÆ¡ sá»Ÿ dá»¯ liá»‡u há»— trá»£ ra quyáº¿t Ä‘á»‹nh â€“ nÃ³ lÃ  má»™t mÃ´i trÆ°á»ng, khÃ´ng pháº£i sáº£n pháº©m)
* Online learning: Data Ä‘Æ°á»£c update liÃªn tá»¥c, model train liÃªn tá»¥c trÃªn nÃ³, quÃ¡ trÃ¬nh training nhanh, ráº».
* Transfer learning: Láº¥y kiáº¿n thá»©c tá»« má»™t pretrain model vÃ  sá»­ dá»¥ng nÃ³, káº¿t há»£p vá»›i má»™t pháº§n máº¡ng cá»§a mÃ¬nh. CÃ¡c báº¡n cÃ³ thá»ƒ tham kháº£o thÃªm vá» Tranfer learning qua má»™t bÃ i viáº¿t cá»§a mÃ¬nh á»Ÿ [Ä‘Ã¢y](https://viblo.asia/p/transfer-learning-va-bai-toan-face-recognition-3Q75w7xD5Wb ) 
* Active learning: hay cÃ²n Ä‘Æ°á»£c gá»i lÃ  â€œHuman in the loopâ€ learning. Con ngÆ°á»i tÆ°Æ¡ng tÃ¡c vá»›i mÃ´ hÃ¬nh, cung cáº¥p cÃ¡c cáº­p nháº­t vá»›i labels, vá»›i nhá»¯ng samples mÃ  model khÃ´ng cháº¯c cháº¯n vá» nÃ³
* Ensembling: KhÃ´ng háº³n lÃ  má»™t dáº¡ng learning, nÃ³ gá»™p nhiá»u thuáº­t toÃ¡n Ä‘Ã£ Ä‘Æ°á»£c há»c Ä‘á»ƒ cho ra má»™t káº¿t quáº£ tá»‘t
### 3. Váº¥n Ä‘á» Underfitting
* Xáº£y ra khi model khÃ´ng biá»ƒu diá»…n Ä‘Æ°á»£c Ä‘áº·c trÆ°ng cá»§a dá»¯ liá»‡u, báº¡n cÃ³ thá»ƒ hiá»ƒu vÃ­ dá»¥ nhÆ° mÃ´ hÃ¬nh quÃ¡ phá»©c táº¡p, nhÆ°ng dá»¯ liá»‡u quÃ¡ nhá», táº¥t nhiÃªn cÃ¡c parameters sáº½ khÃ´ng Ä‘Æ°á»£c Ä‘iá»u chá»‰nh tá»‘t Ä‘á»ƒ phá»¥c vá»¥ bÃ i toÃ¡n
* CÃ³ thá»ƒ thá»­ má»™t sá»‘ cÃ¡ch sau Ä‘á»ƒ giáº£i quyáº¿t váº¥n Ä‘á» Underfitting:
    * Training lÃ¢u hÆ¡n
    * ThÃªm dá»¯ liá»‡u
    * Chá»‰nh sá»­a láº¡i mÃ´ hÃ¬nh
### 4. Váº¥n Ä‘á» Overfitting
* CÃ¡ch dá»… nháº¥t Ä‘á»ƒ nháº­n biáº¿t váº¥n Ä‘á» Overfitting Ä‘Ã³ lÃ  khi báº¡n training vá»›i táº­p training set vÃ  validation set, Ä‘áº¿n má»™t epoch nÃ o Ä‘Ã³, train-loss váº«n giáº£m nhÆ°ng validation-loss báº¯t Ä‘áº§u tÄƒng thÃ¬ Ä‘Ã³ chÃ­nh lÃ  má»™t biá»ƒu hiá»‡n cá»§a overfitting. Hay khi báº¡n huáº¥n luyá»‡n mÃ´ hÃ¬nh tá»‘t trÃªn training set (vÃ­ dá»¥ accuracy Ä‘áº¡t 99%) tuy nhiÃªn vÃ´ cÃ¹ng tá»‡ trÃªn táº­p test.
* CÃ³ thá»ƒ giáº£i quyáº¿t váº¥n Ä‘á» Overfitting nÃ y qua má»™t sá»‘ cÃ¡ch sau:
    * Sá»­ dá»¥ng L1, L2 regularization 
    * Sá»­ dá»¥ng Dropout
    * KÄ© thuáº­t Early stopping
    * Sá»­ dá»¥ng Data Augmentation (sinh thÃªm dá»¯ liá»‡u tá»« dá»¯ liá»‡u gá»‘c báº±n má»™t vÃ i phÃ©p biáº¿n Ä‘á»•i)
    * Sá»­ dá»¥ng Batch normalization 
### 5. Hyperparameter tuning
* TrÆ°á»›c háº¿t mÃ¬nh nghÄ© cÃ¡c báº¡n cáº§n phÃ¢n biá»‡t rÃµ parameter â€“ hyperparameter. CÃ¡c báº¡n cÃ³ thá»ƒ tham kháº£o á»Ÿ [Ä‘Ã¢y](https://viblo.asia/p/mot-vai-hieu-nham-khi-moi-hoc-machine-learning-4dbZNoDnlYM)
* Má»™t sá»‘ váº¥n Ä‘á» liÃªn quan tá»›i hyperparameter tuning:
    * CÃ i Ä‘áº·t learning rate: ThÃ´ng thÆ°á»ng, learning rate cao thÃ¬ thuáº­t toÃ¡n thÃ­ch nghi nhanh chÃ³ng vá»›i dá»¯ liá»‡u má»›i, tuy nhiÃªn cÃ³ thá»ƒ xáº£y ra váº¥n Ä‘á» khÃ´ng há»™i tá»¥ vÃ¬ dao Ä‘á»™ng quanh Ä‘iá»ƒm Global minimum. CÃ²n Learning rate tháº¥p thÃ¬ thuáº­t toÃ¡n thÃ­ch nghi cháº­m vá»›i dá»¯ liá»‡u má»›i vÃ  máº¥t khÃ¡ nhiá»u thá»i gian Ä‘á»ƒ há»™i tá»¥. TÃ¹y tá»«ng bÃ i toÃ¡n vÃ  káº¿t quáº£ sau training mÃ  cÃ¡c báº¡n sáº½ Ä‘iá»u chá»‰nh cho phÃ¹ há»£p. ThÃ´ng thÆ°á»ng mÃ¬nh hay sá»­ dá»¥ng learning rate lÃ  1e-2 hoáº·c 1e-3
    * Báº¡n Ä‘á»c cÃ³ thá»ƒ tham kháº£o má»™t paper khÃ¡ ná»•i tiáº¿ng vá» váº¥n Ä‘á» nÃ y: [A disciplined approach to neural network hyperparameter](https://arxiv.org/pdf/1803.09820.pdf ) 
    * BÃªn cáº¡nh Ä‘Ã³, cÃ³ thá»ƒ tuning má»™t sá»‘ hyperparameter khÃ¡c:
        * Sá»‘ layers (trong Neural network)
        * Batch size
        * Sá»‘ cÃ¢y (trong Decision tree)
        * Sá»‘ bÆ°á»›c láº·p
        * â€¦
## IV. Analysis/Evaluation 
### 1. Evaluation metrics
* MÃ¬nh Ä‘Ã£ cÃ³ má»™t bÃ i viáº¿t cá»¥ thá»ƒ vá» cÃ¡c Evaluation metrics á»Ÿ [Ä‘Ã¢y](https://viblo.asia/p/danh-gia-cac-mo-hinh-hoc-may-RnB5pp4D5PG), cÃ¡c báº¡n cÃ³ thá»ƒ tham kháº£o thÃªm Ä‘á»ƒ hiá»ƒu rÃµ hÆ¡n. 
### 2. Feature importance
* Features nÃ o mang thuá»™c tÃ­nh quan trá»ng nháº¥t trong model?
* CÃ³ nÃªn loáº¡i bá» má»™t sá»‘ features Ä‘á»ƒ giÃºp mÃ´ hÃ¬nh â€œnháº¹â€ hÆ¡n, tá»‘t hÆ¡n? Táº¡i sao loáº¡i bá» feature Ä‘Ã³? So sÃ¡nh káº¿t quáº£ khi sá»­ dá»¥ng vÃ  loáº¡i bá» cÃ¡c features Ä‘Ã³?
### 3. Training/Inference time, cost
* Model train máº¥t bao lÃ¢u? Model Predict máº¥t bao lÃ¢u? CÃ³ kháº£ thi vá»›i bÃ i toÃ¡n Ã¡p dá»¥ng?
* Cáº¥u hÃ¬nh pháº§n cá»©ng Ã¡p dá»¥ng?
### 4. Comparing to other models
* So sÃ¡nh vá»›i cÃ¡c model khÃ¡c cÃ¹ng doimain, káº¿t quáº£ tá»‘t hÆ¡n hay kÃ©m hÆ¡n? NguyÃªn nhÃ¢n? Do data hay do mÃ´ hÃ¬nh?
### 5. Server mode (deploying a model)
* Sá»­ dá»¥ng model trong sáº£n pháº©m: NghiÃªn cá»©u khÃ¡c thá»±c táº¿ ğŸ˜Š)). CÃ³ thá»ƒ trÃªn nghiÃªn cá»© thÃ¬ mÃ´ hÃ¬nh nÃ y Ä‘áº¡t káº¿t quáº£ cao, tuy nhiÃªn Ä‘Æ°a ra thá»±c táº¿ sáº£n pháº©m thÃ¬ chÆ°a cháº¯c. NÃªn nhá»› con Ä‘Æ°á»ng Ä‘i tá»« nghiÃªn cá»©u ra thá»±c táº¿ khÃ´ng pháº£i Ä‘Æ¡n giáº£n.
* Má»™t vÃ i tools cÃ³ thá»ƒ sá»­ dá»¥ng:
    * Tensorflow serving 
    * Pytorch serving 
    * Google AI Platform 
    * Sagemaker 
* MLOps: NÆ¡i mÃ  software engineering gáº·p Machine learning.
### 6. Retrain model
* Hiá»‡u suáº¥t model sau khi serving?
* Model sáº½ dáº§n â€œngá»‘â€ khi mÃ  dá»¯ liá»‡u liÃªn tá»¥c Ä‘Æ°á»£c update. VÃ¬ váº­y cáº§n retrain model.
# Summary
BÃ i viáº¿t cá»§a mÃ¬nh Ä‘áº¿n Ä‘Ã¢y lÃ  háº¿t rá»“i. BÃ i viáº¿t chá»§ yáº¿u Ä‘Æ°a ra má»™t Roadmap vá» Machine Learning, giÃºp cÃ¡c báº¡n má»›i Ä‘á»¡ bá»¡ ngá»¡, dá»… tiáº¿p cáº­n hÆ¡n. BÃ i viáº¿t chá»§ yáº¿u Ä‘Æ°a ra cÃ¡c tá»« khÃ³a giÃºp cÃ¡c báº¡n tra cá»©u nhanh hÆ¡n. Trong cÃ¡c bÃ i viáº¿t sau mÃ¬nh sáº½ cá»‘ gáº¯ng Ä‘i sÃ¢u vÃ o cÃ¡c váº¥n Ä‘á» hÆ¡n.
Cáº£m Æ¡n cÃ¡c báº¡n Ä‘Ã£ Ä‘á»c Ä‘áº¿n cuá»‘i bÃ i viáº¿t. ChÃºc má»i ngÆ°á»i cuá»‘i tuáº§n vui váº»! ^^

# References 
[Machine Learning Roadmap (2020)](https://whimsical.com/machine-learning-roadmap-2020-CA7f3ykvXpnJ9Az32vYXva?fbclid=IwAR1nGRPFISiS1d6nEj4EngcZPa811YgJtydaF9rh5EBsZj9SNF_jrJsXRqc)