# Gi·ªõi thi·ªáu b√†i to√°n
Toxic Span Detection l√† b√†i to√°n ph√°t hi·ªán c√°c t·ª´/c·ª•m t·ª´ ƒë·ªôc h·∫°i trong vƒÉn b·∫£n, c·ª• th·ªÉ l√† c√°c b√¨nh lu·∫≠n, b√†i ƒëƒÉng tr√™n m·∫°ng x√£ h·ªôi. M·∫∑c d√π m·ªôt s·ªë b·ªô d·ªØ li·ªáu v√† m√¥ h√¨nh ph√°t hi·ªán toxic ƒë√£ ƒë∆∞·ª£c nghi√™n c·ª©u nh∆∞ng h·∫ßu h·∫øt ch√∫ng ƒë·ªÅu ph√¢n lo·∫°i to√†n b·ªô vƒÉn b·∫£n v√† kh√¥ng x√°c ƒë·ªãnh ƒë∆∞·ª£c c√°c d·∫•u hi·ªáu khi·∫øn m·ªôt vƒÉn b·∫£n tr·ªü n√™n ƒë·ªôc h·∫°i. Trong khi ƒë√≥, vi·ªác ph√°t hi·ªán ra nh·ªØng span mang l·∫°i hi·ªáu qu·∫£ cao h∆°n khi n√≥ h·ªó tr·ª£ ng∆∞·ªùi ki·ªÉm duy·ªát n·ªôi dung highlight nh·ªØng t·ª´ ng·ªØ th√¥ t·ª•c thay v√¨ ch·ªâ ƒë∆∞a ra m·ªôt ƒëi·ªÉm s·ªë m·ª©c ƒë·ªô ƒë·ªôc h·∫°i nh∆∞ ph∆∞∆°ng ph√°p tr√™n. Do ƒë√≥, toxic span detection l√† m·ªôt b∆∞·ªõc quan tr·ªçng trong ki·ªÉm duy·ªát b√°n t·ª± ƒë·ªông. 

SemEval 2021 Task 5 cung c·∫•p b·ªô d·ªØ li·ªáu c≈©ng nh∆∞ h·ªá th·ªëng ƒë√°nh gi√° cho b√†i to√°n Toxic Span Detection. B√†i to√°n n√†y c√≥ th·ªÉ ƒë∆∞·ª£c gi·∫£i quy·∫øt b·∫±ng sequence tagging, t·ª©c l√† ta s·∫Ω d·ª± ƒëo√°n nh√£n (toxic/non-toxic) cho t·ª´ng t·ª´ trong c√¢u. Trong b√†i n√†y m√¨nh s·∫Ω ƒë∆∞a v·ªÅ d·∫°ng Named Entity Recognition trong ƒë√≥ m·ªói toxic span ƒë∆∞·ª£c coi l√† m·ªôt th·ª±c th·ªÉ. Th∆∞ vi·ªán ƒë∆∞·ª£c s·ª≠ d·ª•ng l√† [Flair](https://github.com/flairNLP/flair) - m·ªôt th∆∞ vi·ªán c·ª±c ti·ªán l·ª£i cho c√°c b√†i to√°n NLP, ƒë·∫∑c bi·ªát l√† NER.
# Chu·∫©n b·ªã d·ªØ li·ªáu
D·ªØ li·ªáu c√≥ th·ªÉ t·∫£i t·∫°i [official page](https://competitions.codalab.org/competitions/25623#learn_the_details-data) c·ªßa c·ªßa cu·ªôc thi. C√°c toxic span ƒë∆∞·ª£c annotate b·∫±ng c√°c offset c·ªßa k√Ω t·ª± trong span ƒë√≥. V√≠ d·ª•:
![](https://images.viblo.asia/d32a6874-dba8-402a-b548-d074128a902f.png)

ƒê·∫ßu ti√™n m√¨nh s·∫Ω load data, b√≥c t√°ch toxic spans t·ª´ offset groundtruth v√† l∆∞u d∆∞·ªõi d·∫°ng dictionary:
```python
def normalize(text):
    text = re.sub('`|‚Äò|‚Äô', "'", text)
    text = re.sub('‚Äù|‚Äú', '"', text)
    text = re.sub('\n|‚Ä¶|[^\x00-\x7F]|\[|\]', ' ', text)
    return text
    
# group continuous offset sequence
def group_sequence(l):
    temp_list = cycle(l)
    next(temp_list)
    groups = groupby(l, key=lambda j: j + 1 == next(temp_list))
    for k, v in groups:
        if k:
            yield tuple(v) + (next((next(groups)[1])),)


def load_data(path):
    tsd = pd.read_csv(path)
    tsd.spans = tsd.spans.apply(literal_eval)
    data = []
    for row in tsd.iterrows():
        span = row[1]['spans']
        text = normalize(row[1]['text'])
        temp = []
        text_spans = []
        if span:
            segments = group_sequence(span)
            for seg in segments:
                temp.append([seg[0], seg[-1]])
                text_spans.append(text[seg[0]: seg[-1] + 1])
        if len(temp) == 1 and temp[0][1] - temp[0][0] >= 60:
            continue
        data.append({'text': text, 'spans': temp, 'text_spans': text_spans})
    return data
```
```
[{'text': 'Another violent and aggressive immigrant killing a innocent and intelligent US Citizen.... Sarcasm', 'spans': [[8, 39]], 'text_spans': ['violent and aggressive immigrant']},
 {'text': 'Damn, a whole family. Sad indeed.', 'spans': [[0, 3]], 'text_spans': ['Damn']}]
```
V√¨ d·ªØ li·ªáu ƒë∆∞·ª£c annotate theo offset c·ªßa t·ª´ng k√Ω t·ª± n√™n c√°c b∆∞·ªõc ti·ªÅn x·ª≠ l√Ω ti·∫øp theo ph·∫£i ƒë·∫∑c bi·ªát th·∫≠n tr·ªçng v·ªõi offset n·∫øu kh√¥ng s·∫Ω l√†m l·ªách label c·ªßa d·ªØ li·ªáu. M√¨nh implement h√†m `replace()` l√† extension c·ªßa `re.sub()` ƒë·ªÉ replace pattern m√† v·∫´n gi·ªØ l·∫°i offset c·ªßa c√°c k√Ω t·ª± kh√°c. 
```python
from ast import literal_eval
import pandas as pd
from itertools import groupby, cycle
import numpy as np
import re
from nltk.tokenize.api import TokenizerI
```
```python
def replace(text, pattern, replacement, pos):
    matches = [0]

    def capture_and_replace(match, ret):
        matches.extend([match.start() + 1, match.end()])
        return ret

    l = len(text)
    text = re.sub(pattern, lambda match: capture_and_replace(match, replacement), text, flags=re.IGNORECASE)
    matches.append(l)
    slices = np.array_split(matches, int(len(matches) / 2))
    res = []
    for s in slices:
        res += pos[s[0]:s[1]]
    assert len(text) == len(res)
    return text, res
```

Ti·∫øp ƒë√≥ l√† preprocess d·ªØ li·ªáu bao g·ªìm lo·∫°i b·ªè URLs, merge c√°c chu·ªói k√Ω t·ª± l·∫∑p l·∫°i th√†nh 1 k√Ω t·ª± duy nh·∫•t (v√≠ d·ª• ?????? -> ?) v√† strip d·∫•u c√¢u ·ªü ƒë·∫ßu, cu·ªëi text.
```python
def preprocess(text, pos):
    # remove urls
    text, pos = replace(text, r'http\S+', ' ', pos)
    text, pos = replace(text, r'www.\S+', ' ', pos)
    
    # collapse duplicated punctuations 
    punc = ',. !?\"\''
    for c in punc:
        pat = '([' + c + ']{2,})'
        text, pos = replace(text, pat, c, pos)
    
    # strip text
    text = text.lstrip(' |.|,|!|?')
    pos = pos[len(pos)-len(text):]
    text = text.rstrip(' |.|,|!|?')
    pos = pos[:len(text)]
    assert len(text) == len(pos)
    return text, pos
```

B∆∞·ªõc ti·∫øp theo l√† tokenize text. ·ªû ƒë√¢y m√¨nh customize l·∫°i tokenizer c·ªßa NLTK ƒë·ªÉ ph√π h·ª£p h∆°n v·ªõi m·ªôt s·ªë ƒë·∫∑c ƒëi·ªÉm c·ªßa comment/post (v√≠ d·ª• nh·ªØng t·ª´ nh∆∞ a$$, f\*\*k th√¨ n√™n gi·ªØ nguy√™n thay v√¨ v√¨ tokenize th√†nh \['a', '\$', '\$'\]  hay \['f', '\*', '\*', 'k'\])
```python
class MacIntyreContractions:
    """
    List of contractions adapted from Robert MacIntyre's tokenizer.
    """

    CONTRACTIONS2 = [
        r"(?i)\b(can)(?#X)(not)\b",
        r"(?i)\b(d)(?#X)('ye)\b",
        r"(?i)\b(gim)(?#X)(me)\b",
        r"(?i)\b(gon)(?#X)(na)\b",
        r"(?i)\b(got)(?#X)(ta)\b",
        r"(?i)\b(lem)(?#X)(me)\b",
        r"(?i)\b(mor)(?#X)('n)\b",
        r"(?i)\b(wan)(?#X)(na)\s",
    ]
    CONTRACTIONS3 = [r"(?i) ('t)(?#X)(is)\b", r"(?i) ('t)(?#X)(was)\b"]
    CONTRACTIONS4 = [r"(?i)\b(whad)(dd)(ya)\b", r"(?i)\b(wha)(t)(cha)\b"]


class NLTKWordTokenizer(TokenizerI):

    # Starting quotes.
    STARTING_QUOTES = [
        (re.compile(u"([¬´‚Äú‚Äò‚Äû]|[`]+)", re.U), r" \1 "),
        (re.compile(r"(\")"), r" \1 "),
        (re.compile(r"([ \(\[{<])(\"|\'{2})"), r'\1 " '),
        (re.compile(r"(?i)(\')(?!re|ve|ll|m|t|s|d)(\w)\b", re.U), r"\1 \2"),
    ]

    # Ending quotes.
    ENDING_QUOTES = [
        (re.compile(u"( ')", re.U), r" \1 "),
        (re.compile(r'"'), ' " '),
        (re.compile(r"(\S)(\")"), r"\1 \2 "),
        (re.compile(r"([^' ]['ll|'LL|'re|'RE|'ve|'VE|n't|N'T]) "), r"\1 "),
    ]

    PUNCTUATION = [
        (re.compile(r'([^\.])(\.)([\]\)}>"\'' u"¬ª‚Äù‚Äô " r"]*)\s*$", re.U), r"\1 \2 \3 "),
        (re.compile(r"([:])([^\d])"), r" \1 \2"),
        (re.compile(r"([:,])$"), r" \1 "),
        (re.compile(r"\.{2,}", re.U), r" \g<0> "), # See https://github.com/nltk/nltk/pull/2322
        (re.compile(r"([\w;@*#$%?!&\^\'\"])+"), r" \g<0> "),
        (
            re.compile(r'([^\.])(\.)([\]\)}>"\']*)\s*$'),
            r"\1 \2\3 ",
        ),  # Handles the final period.
        (re.compile(r"([,])+"), r" \g<0> "),
        (re.compile(r"([^'])' "), r"\1 ' "),
    ]

    # Pads parentheses
    PARENS_BRACKETS = (re.compile(r"[\]\[\(\)\{\}\<\>]"), r" \g<0> ")

    # Optionally: Convert parentheses, brackets and converts them to PTB symbols.
    CONVERT_PARENTHESES = [
        (re.compile(r"\("), "-LRB-"),
        (re.compile(r"\)"), "-RRB-"),
        (re.compile(r"\["), "-LSB-"),
        (re.compile(r"\]"), "-RSB-"),
        (re.compile(r"\{"), "-LCB-"),
        (re.compile(r"\}"), "-RCB-"),
    ]

    DOUBLE_DASHES = (re.compile(r"--"), r" -- ")

    # List of contractions adapted from Robert MacIntyre's tokenizer.
    _contractions = MacIntyreContractions()
    CONTRACTIONS2 = list(map(re.compile, _contractions.CONTRACTIONS2))
    CONTRACTIONS3 = list(map(re.compile, _contractions.CONTRACTIONS3))

    def tokenize(self, text, convert_parentheses=False, return_str=False):
        for regexp, substitution in self.STARTING_QUOTES:
            text = regexp.sub(substitution, text)

        for regexp, substitution in self.PUNCTUATION:
            text = regexp.sub(substitution, text)

        # Handles parentheses.
        regexp, substitution = self.PARENS_BRACKETS
        text = regexp.sub(substitution, text)
        # Optionally convert parentheses
        if convert_parentheses:
            for regexp, substitution in self.CONVERT_PARENTHESES:
                text = regexp.sub(substitution, text)

        # Handles double dash.
        regexp, substitution = self.DOUBLE_DASHES
        text = regexp.sub(substitution, text)

        # add extra space to make things easier
        text = " " + text + " "

        for regexp, substitution in self.ENDING_QUOTES:
            text = regexp.sub(substitution, text)


        for regexp in self.CONTRACTIONS2:
            text = regexp.sub(r" \1 \2 ", text)
        for regexp in self.CONTRACTIONS3:
            text = regexp.sub(r" \1 \2 ", text)

        return text if return_str else text.split()
```
Th·ª≠ xem tokenizer c√≥ ho·∫°t ƒë·ªông kh√¥ng....
```python
tokenizer = NLTKWordTokenizer()
print(tokenizer.tokenize("Trump is a pu*ssy"))
```
```
['Trump', 'is', 'a', 'pu*ssy']
```
Tokenize text c√πng th√¥ng tin offset:
```python
def tokenize(text, pos):
    tokens = tokenizer.tokenize(text)

    alignment = []
    start = 0
    for t in tokens:
        res = text.find(t, start)
        alignment.append(pos[res:res + len(t)])
        start = res + len(t)
    assert len(tokens) == len(alignment)
    return tokens, alignment
```
Cu·ªëi c√πng, ch√∫ng ta bi·ªÉu di·ªÖn span d∆∞·ªõi d·∫°ng IOB (Inside-Outside-Beginning). Tr√™n th·ª±c t·∫ø, v√¨ b√†i to√°n ch·ªâ c√≥ m·ªôt lo·∫°i th·ª±c th·ªÉ n√™n ch·ªâ c·∫ßn bi·ªÉu di·ªÖn nh·ªã ph√¢n l√† ƒë·ªß. Tuy nhi√™n trong qu√° tr√¨nh th·ª±c nghi·ªám m√¨nh nh·∫≠n th·∫•y bi·ªÉu di·ªÖn IOB cho k·∫øt qu·∫£ t·ªët h∆°n.
```python
def annotate(spans, alignment, tokens):
    i = 0
    annotations = []
    for span in spans:
        while i < len(alignment):
            if alignment[i][-1] <= span[0]:
                annotations.append('O')
                i += 1
            elif alignment[i][0] <= span[0] < alignment[i][-1]:
                annotations.append('B-T')
                i += 1
            elif span[0] < alignment[i][0] < span[-1]:
                annotations.append('I-T')
                i += 1
            elif alignment[i][0] >= span[-1]:
                annotations.append('O')
                i += 1
                break
    annotations.extend(['O'] * (len(tokens) - len(annotations)))
    return annotations
```
T·ªïng h·ª£p l·∫°i, ta load d·ªØ li·ªáu, ti·ªÅn x·ª≠ l√Ω, tokenize v√† annotate span d∆∞·ªõi d·∫°ng IOB. L∆∞u √Ω xuy√™n su·ªët qu√° tr√¨nh tr√™n offset ban ƒë·∫ßu c·ªßa k√Ω t·ª± ph·∫£i ƒë∆∞·ª£c gi·ªØ nguy√™n.
```python
def prepare_data(data):
    idx = 0
    formated_data = []
    for d in data:
        text = d['text']
        pos = [i for i in range(len(text))]
        text, pos = preprocess(text, pos)
        tokens, alignment = tokenize(text, pos)
        annotations = annotate(d['spans'], alignment, tokens)
        ls = [[tokens[i], annotations[i]] for i in range(len(tokens))]
        formated_data.extend(ls)
        formated_data.append([None])
        idx += 1
    return formated_data


train = load_data("toxic_spans/data/tsd_train.csv")

train = prepare_data(train)
train = pd.DataFrame(train)
train.to_csv('train_flair.txt', index=False, columns=None, header=False, sep=' ')

test = load_data("toxic_spans/data/tsd_trial.csv")
test = prepare_data(test)
test = pd.DataFrame(test)
test.to_csv('dev_flair.txt', index=False, columns=None, header=False, sep=' ')
```
D·ªØ li·ªáu ƒë∆∞·ª£c l∆∞u th√†nh hai files `train_flair.txt` ph·ª•c v·ª• training v√† `dev_flair.txt` ph·ª•c v·ª• qu√° tr√¨nh t·ªëi ∆∞u model.

# M√¥ h√¨nh v√† hu·∫•n luy·ªán 
M√¥ h√¨nh h·ªçc s√¢u m√¨nh s·ª≠ d·ª•ng c√≥ ki·∫øn tr√∫c kh√° ƒë∆°n gi·∫£n. Input features l√† s·ª± k·∫øt h·ª£p c·ªßa c√°c embedding kh√°c nhau, sau ƒë√≥ ƒëi qua m·ªôt l·ªõp Reproject Embeddings (th·ª±c ch·∫•t l√† m·ªôt l·ªõp Dense) r·ªìi cu·ªëi c√πng l√† ph·∫ßn m·∫°ng n∆°-ron h·ªìi quy BiLSTM-CRF. 
![](https://images.viblo.asia/67c19dc9-ccd8-4ea1-9bbf-bbb8990302c1.png)

Trong h·∫ßu h·∫øt c√°c b√†i to√°n NLP, word embedding l√† ph·∫ßn quan tr·ªçng nh·∫•t. Vi·ªác k·∫øt h·ª£p c√°c embedding kh√°c nhau gi√∫p m√¥ h√¨nh tr√≠ch xu·∫•t ƒë∆∞·ª£c c·∫£ th√¥ng tin v·ªÅ c√∫ ph√°p l·∫´n ng·ªØ nghƒ©a c·ªßa m·ªôt t·ª´. Nh·ªØng embedding n√†y tƒÉng c∆∞·ªùng ƒëi·ªÉm m·∫°nh l·∫´n nhau trong l√†m gi·∫£m nh·ªØng ƒëi·ªÉm y·∫øu c·ªßa nhau. C·ª• th·ªÉ nh·ªØng embedding m√¨nh s·ª≠ d·ª•ng l√†: 
* **Flair**: l√† m√¥ h√¨nh ng√¥n ng·ªØ ng·ªØ c·∫£nh ho·∫°t ƒë·ªông ·ªü m·ª©c k√Ω t·ª±, mang th√¥ng tin v·ªÅ h√¨nh th√°i t·ª´. Flair c√≥ hai lo·∫°i m√¥ h√¨nh ng√¥n ng·ªØ l√† `forward` v√† `backward`. M√¨nh fine-tune hai pre-trained models c·ªßa Flair (`news-forward` v√† `news-backward`) tr√™n t·∫≠p d·ªØ li·ªáu toxic n√†y ƒë·ªÉ m√¥ h√¨nh adapt t·ªët h∆°n v√†o mi·ªÅn d·ªØ li·ªáu. Link download: [forward](https://drive.google.com/file/d/1-Aww3LIPh7GmPlbeQjw5N80HPJlMx7W8/view?usp=sharing) v√† [backward](https://drive.google.com/file/d/1-IF4d9UnG25sadw_cq42J4jN8br5OowF/view?usp=sharing) (link ƒë√£ kh√¥ng c√≤n kh·∫£ d·ª•ng do m√¨nh l·ª° tay xo√° m·∫•t models tr√™n Drive ü•≤).
* **[Toxic RoBERTa](https://huggingface.co/unitary/unbiased-toxic-roberta)**: m√¥ h√¨nh ng√¥n ng·ªØ RoBERTa ƒë∆∞·ª£c c·ªông ƒë·ªìng huggingface fine-tune tr√™n t·∫≠p d·ªØ li·ªáu g·ªëc c·ªßa t·∫≠p d·ªØ li·ªáu n√†y - Civil Comment Dataset. M√¥ h√¨nh RoBERTa th√¨ kh·ªèi ph·∫£i n√≥i r·ªìi, state-of-the-art, l·∫°i ƒë∆∞·ª£c fine-tune tr√™n mi·ªÅn d·ªØ li·ªáu toxic n√™n embedding c·ªßa n√≥ th·ª±c s·ª± hi·ªáu qu·∫£ trong b√†i to√°n n√†y. V√¨ c√°c l·ªõp transformer trong RoBERTa bi·ªÉu di·ªÖn m·ª©c ƒë·ªô ng·ªØ c·∫£nh kh√°c nhau n√™n m√¨nh tr√≠ch xu·∫•t embedding t·ª´ 3 l·ªõp trong RoBERTa: l·ªõp ƒë·∫ßu, l·ªõp gi·ªØa (#6) v√† l·ªõp cu·ªëi. 
* **Word2Vec**: Tuy h∆°i c≈© v√† ƒë∆∞·ª£c cho l√† k√©m hi·ªáu qu·∫£ so v·ªõi c√°c m√¥ h√¨nh ng√¥n ng·ªØ ng·ªØ c·∫£nh hi·ªán nay nh∆∞ng m√¨nh v·∫´n cho th√™m embedding n√†y v√† n√≥ c≈©ng c·∫£i thi·ªán hi·ªáu nƒÉng m√¥ h√¨nh th√™m m·ªôt ch√∫t. ·ªû ƒë√¢y m√¨nh s·ª≠ d·ª•ng vect∆° t·ª´ c·ªßa m√¥ h√¨nh fastText ƒë·ªÉ tr√≠ch xu·∫•t embedding k·∫øt h·ª£p v·ªõi Byte Pair Embedding ƒë·ªÉ bi·ªÉu di·ªÖn cho c√°c t·ª´ out-of-vocabulary. S·ª± k·∫øt h·ª£p n√†y ho·∫°t ƒë·ªông t·ªët nh∆∞ fastText g·ªëc trong khi gi·∫£m m·ª©c s·ª≠ d·ª•ng b·ªô nh·ªõ m·ªôt c√°ch hi·ªáu qu·∫£. 

Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt:

```python
from flair.data import Corpus, Token, Sentence
from flair.datasets import ColumnCorpus
from utils import *
from torch.optim.adamw import AdamW
import pandas as pd
from flair.embeddings import *
from flair.models import SequenceTagger
from flair.trainers import ModelTrainer
from flair.training_utils import AnnealOnPlateau
```

Load corpus b·∫±ng Flair: 
```python
# define columns
columns = {0: 'text', 1: 'ner'}

# this is the folder in which train, test and dev files reside
data_folder = ''

# init a corpus using column format, data folder and the names of the train, dev and test files
corpus: Corpus = ColumnCorpus(data_folder, columns,
                              train_file='train_flair.txt',
                              dev_file='dev_flair.txt')
```

Sau ƒë√≥, ta kh·ªüi t·∫°o c√°c embeddings v√† concat ch√∫ng l·∫°i th√†nh m·ªôt vect∆° d√†i bi·ªÉu di·ªÖn cho t·ª´. Flair h·ªó tr·ª£ r·∫•t nhi·ªÅu embeddings kh√°c nhau, ta ch·ªâ c·∫ßn kh·ªüi t·∫°o c√°c embedding mong mu·ªën v√†o m·ªôt list v√† truy·ªÅn list ƒë√≥ v√†o `StackedEmbeddings` l√† xong. V·ªõi `TransformerWordEmbeddings` m√¨nh s·∫Ω s·ª≠ d·ª•ng layer 0 (ƒë·∫ßu), layer -6 (gi·ªØa) v√† layer -1 (cu·ªëi) nh∆∞ ƒë√£ n√≥i ·ªü tr√™n, ƒë·ªìng th·ªùi set `fine_tune=False` ƒë·ªÉ freeze toxic RoBERTa. M√¨nh nh·∫≠n th·∫•y ƒëi·ªÅu n√†y kh√¥ng nh·ªØng gi√∫p qu√° tr√¨nh training nhanh h∆°n m√† c√≤n ·ªïn ƒë·ªãnh h∆°n.
```python 
embedding_types = [
    TransformerWordEmbeddings('unitary/unbiased-toxic-roberta', layers="-1, -6, 0", fine_tune=False, allow_long_sentences=True, pooling_operation='mean'),
    WordEmbeddings('crawl'),
    BytePairEmbeddings('en'),
    FlairEmbeddings('forward.pt'),
    FlairEmbeddings('backward.pt'),
]

embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)
```

Ti·∫øp theo ta build model (tagger) v√† trainer. Vi·ªác n√†y ƒë·ªëi v·ªõi Flair c·ª±c k·ª≥ ƒë∆°n gi·∫£n nh∆∞ sau:
```python
tag_type = 'ner'
tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)
tagger: SequenceTagger = SequenceTagger(hidden_size=256,
                                        rnn_layers=2,
                                        dropout=0.39951586265227385,
                                        locked_dropout=0.4413066564103566,
                                        word_dropout=0.06774606445713507,
                                        embeddings=embeddings,
                                        tag_dictionary=tag_dictionary,
                                        tag_type=tag_type,
                                        train_initial_hidden_state=True,
                                        use_crf=True)
trainer: ModelTrainer = ModelTrainer(tagger, corpus, optimizer=AdamW)
```

V√† cu·ªëi c√πng l√† train model:
```python
trainer.train('output',
              learning_rate=0.0005,
              embeddings_storage_mode='cpu',
              min_learning_rate=0.0000001,
              mini_batch_size=32,
              max_epochs=50,
              scheduler = AnnealOnPlateau,
              )
```
```
2021-02-22 08:40:08,739 ----------------------------------------------------------------------------------------------------
2021-02-22 08:40:43,913 epoch 1 - iter 21/219 - loss 14.28931536 - samples/sec: 19.11 - lr: 0.000500
2021-02-22 08:41:17,078 epoch 1 - iter 42/219 - loss 11.51475904 - samples/sec: 20.26 - lr: 0.000500
2021-02-22 08:41:51,011 epoch 1 - iter 63/219 - loss 10.56218508 - samples/sec: 19.80 - lr: 0.000500
2021-02-22 08:42:23,469 epoch 1 - iter 84/219 - loss 10.08471138 - samples/sec: 20.70 - lr: 0.000500
2021-02-22 08:42:57,697 epoch 1 - iter 105/219 - loss 9.58165464 - samples/sec: 19.63 - lr: 0.000500
2021-02-22 08:43:31,524 epoch 1 - iter 126/219 - loss 9.26009862 - samples/sec: 19.87 - lr: 0.000500
2021-02-22 08:44:06,541 epoch 1 - iter 147/219 - loss 9.18564796 - samples/sec: 19.19 - lr: 0.000500
2021-02-22 08:44:41,786 epoch 1 - iter 168/219 - loss 8.81589066 - samples/sec: 19.07 - lr: 0.000500
2021-02-22 08:45:15,839 epoch 1 - iter 189/219 - loss 8.55171940 - samples/sec: 19.73 - lr: 0.000500
2021-02-22 08:45:49,743 epoch 1 - iter 210/219 - loss 8.43762823 - samples/sec: 19.82 - lr: 0.000500
2021-02-22 08:46:03,925 ----------------------------------------------------------------------------------------------------
2021-02-22 08:46:03,925 EPOCH 1 done: loss 8.4037 - lr 0.0005000
2021-02-22 08:46:33,552 DEV : loss 8.37681770324707 - score 0.5564
2021-02-22 08:46:33,683 BAD EPOCHS (no improvement): 0
2021-02-22 08:46:41,367 ----------------------------------------------------------------------------------------------------
2021-02-22 08:46:50,622 epoch 2 - iter 21/219 - loss 7.97614507 - samples/sec: 72.63 - lr: 0.000500
2021-02-22 08:46:59,075 epoch 2 - iter 42/219 - loss 7.76867146 - samples/sec: 79.51 - lr: 0.000500
2021-02-22 08:47:07,467 epoch 2 - iter 63/219 - loss 7.33009972 - samples/sec: 80.09 - lr: 0.000500
2021-02-22 08:47:15,520 epoch 2 - iter 84/219 - loss 7.38141656 - samples/sec: 83.46 - lr: 0.000500
2021-02-22 08:47:24,430 epoch 2 - iter 105/219 - loss 7.21088008 - samples/sec: 75.44 - lr: 0.000500
2021-02-22 08:47:32,693 epoch 2 - iter 126/219 - loss 7.61673441 - samples/sec: 81.34 - lr: 0.000500
2021-02-22 08:47:40,978 epoch 2 - iter 147/219 - loss 7.45866577 - samples/sec: 81.12 - lr: 0.000500
2021-02-22 08:47:49,173 epoch 2 - iter 168/219 - loss 7.25671026 - samples/sec: 82.02 - lr: 0.000500
2021-02-22 08:47:58,407 epoch 2 - iter 189/219 - loss 7.12561822 - samples/sec: 72.79 - lr: 0.000500
2021-02-22 08:48:07,609 epoch 2 - iter 210/219 - loss 6.97803578 - samples/sec: 73.04 - lr: 0.000500
2021-02-22 08:48:10,952 ----------------------------------------------------------------------------------------------------
2021-02-22 08:48:10,952 EPOCH 2 done: loss 6.9765 - lr 0.0005000
2021-02-22 08:48:15,613 DEV : loss 7.445516109466553 - score 0.5716
2021-02-22 08:48:15,737 BAD EPOCHS (no improvement): 0
2021-02-22 08:48:23,522 ----------------------------------------------------------------------------------------------------
2021-02-22 08:48:32,302 epoch 3 - iter 21/219 - loss 8.60981764 - samples/sec: 76.55 - lr: 0.000500
2021-02-22 08:48:40,087 epoch 3 - iter 42/219 - loss 7.09732554 - samples/sec: 86.33 - lr: 0.000500
2021-02-22 08:48:48,280 epoch 3 - iter 63/219 - loss 6.86162996 - samples/sec: 82.04 - lr: 0.000500
2021-02-22 08:48:57,734 epoch 3 - iter 84/219 - loss 6.89683542 - samples/sec: 71.10 - lr: 0.000500
2021-02-22 08:49:06,968 epoch 3 - iter 105/219 - loss 6.81617200 - samples/sec: 72.78 - lr: 0.000500
2021-02-22 08:49:16,236 epoch 3 - iter 126/219 - loss 6.80960591 - samples/sec: 72.52 - lr: 0.000500
2021-02-22 08:49:23,976 epoch 3 - iter 147/219 - loss 6.77141035 - samples/sec: 86.84 - lr: 0.000500
2021-02-22 08:49:32,838 epoch 3 - iter 168/219 - loss 6.67913532 - samples/sec: 75.84 - lr: 0.000500
2021-02-22 08:49:40,967 epoch 3 - iter 189/219 - loss 6.55790764 - samples/sec: 82.67 - lr: 0.000500
2021-02-22 08:49:50,167 epoch 3 - iter 210/219 - loss 6.54964189 - samples/sec: 73.06 - lr: 0.000500
2021-02-22 08:49:54,542 ----------------------------------------------------------------------------------------------------
2021-02-22 08:49:54,542 EPOCH 3 done: loss 6.4866 - lr 0.0005000
2021-02-22 08:49:59,195 DEV : loss 7.138552665710449 - score 0.5883
2021-02-22 08:49:59,318 BAD EPOCHS (no improvement): 0
2021-02-22 08:50:07,225 ----------------------------------------------------------------------------------------------------
2021-02-22 08:50:15,322 epoch 4 - iter 21/219 - loss 5.59429038 - samples/sec: 83.02 - lr: 0.000500
2021-02-22 08:50:24,630 epoch 4 - iter 42/219 - loss 5.68623964 - samples/sec: 72.21 - lr: 0.000500
2021-02-22 08:50:33,023 epoch 4 - iter 63/219 - loss 5.78819111 - samples/sec: 80.08 - lr: 0.000500
2021-02-22 08:50:41,336 epoch 4 - iter 84/219 - loss 5.96195083 - samples/sec: 80.85 - lr: 0.000500
2021-02-22 08:50:50,367 epoch 4 - iter 105/219 - loss 5.96660735 - samples/sec: 74.42 - lr: 0.000500
2021-02-22 08:50:59,163 epoch 4 - iter 126/219 - loss 6.26441736 - samples/sec: 76.42 - lr: 0.000500
2021-02-22 08:51:07,049 epoch 4 - iter 147/219 - loss 6.15406115 - samples/sec: 85.23 - lr: 0.000500
2021-02-22 08:51:15,161 epoch 4 - iter 168/219 - loss 6.22897461 - samples/sec: 82.85 - lr: 0.000500
2021-02-22 08:51:24,139 epoch 4 - iter 189/219 - loss 6.16187050 - samples/sec: 74.86 - lr: 0.000500
2021-02-22 08:51:33,240 epoch 4 - iter 210/219 - loss 6.11923879 - samples/sec: 73.85 - lr: 0.000500
2021-02-22 08:51:37,001 ----------------------------------------------------------------------------------------------------
2021-02-22 08:51:37,002 EPOCH 4 done: loss 6.1315 - lr 0.0005000
2021-02-22 08:51:41,718 DEV : loss 6.522727966308594 - score 0.6082
2021-02-22 08:51:41,845 BAD EPOCHS (no improvement): 0
2021-02-22 08:51:49,728 ----------------------------------------------------------------------------------------------------
2021-02-22 08:51:58,504 epoch 5 - iter 21/219 - loss 5.58368506 - samples/sec: 76.62 - lr: 0.000500
2021-02-22 08:52:07,493 epoch 5 - iter 42/219 - loss 5.46633082 - samples/sec: 74.77 - lr: 0.000500
.
.
.
2021-02-22 10:06:04,808 EPOCH 50 done: loss 3.2447 - lr 0.0000020
2021-02-22 10:06:09,542 DEV : loss 3.4154860973358154 - score 0.6238
2021-02-22 10:06:09,668 BAD EPOCHS (no improvement): 3
2021-02-22 10:06:17,481 ----------------------------------------------------------------------------------------------------
2021-02-22 10:06:17,482 Testing using best model ...
2021-02-22 10:06:17,482 loading file output/best-model.pt
2021-02-22 10:06:54,445 0.6881	0.5904	0.6355
2021-02-22 10:06:54,446 
Results:
- F1-score (micro) 0.6355
- F1-score (macro) 0.6355

By class:
T          tp: 578 - fp: 262 - fn: 401 - precision: 0.6881 - recall: 0.5904 - f1-score: 0.6355
2021-02-22 10:06:54,446 ----------------------------------------------------------------------------------------------------
```

Model c·ªßa ch√∫ng ta ƒë·∫°t **0.6355** F1-score tr√™n t·∫≠p dev. Tuy nhi√™n ƒë√¢y ch·ªâ l√† F1-score ·ªü m·ª©c t·ª´ trong khi benchmark c·ªßa task n√†y s·ª≠ d·ª•ng F1-score ·ªü m·ª©c k√Ω t·ª±. Ph·∫ßn ti·∫øp theo ta s·∫Ω t√≠nh F1-score m·ª©c k√Ω t·ª± tr√™n t·∫≠p private test.
# Evaluate tr√™n t·∫≠p test
Metric F1 m√¨nh s·ª≠ d·ª•ng tr·ª±c ti·∫øp t·ª´ code c·ªßa BTC cu·ªôc thi nh∆∞ sau:
```python
def f1(predictions, gold):
    """
    F1 (a.k.a. DICE) operating on two lists of offsets (e.g., character).
    assert f1([0, 1, 4, 5], [0, 1, 6]) == 0.5714285714285714
    :param predictions: a list of predicted offsets
    :param gold: a list of offsets serving as the ground truth
    :return: a score between 0 and 1
    """
    if len(gold) == 0:
        return 1. if len(predictions) == 0 else 0.
    if len(predictions) == 0:
        return 0.
    predictions_set = set(predictions)
    gold_set = set(gold)
    nom = 2 * len(predictions_set.intersection(gold_set))
    denom = len(predictions_set) + len(gold_set)
    return float(nom)/float(denom)
```
H√†m evaluate model cho m·ªôt list vƒÉn b·∫£n:
```python
from tqdm import tqdm
def eval(model, sents, alignments):
    preds = []
    for idx, sent in tqdm(enumerate(sents)):
        model.predict(sent)
        pred = []
        for span in sent.get_spans():
            for tok in span.tokens:
                pred.extend(alignments[idx][tok.idx-1])
        preds.append(pred)
    return preds
```
Ti·∫øp theo ta load d·ªØ li·ªáu trong t·∫≠p private test, th·ª±c hi·ªán c√°c b∆∞·ªõc x·ª≠ l√Ω d·ªØ li·ªáu nh∆∞ v·ªõi t·∫≠p train, v√† predict t·ª´ng vƒÉn b·∫£n b·∫±ng model ƒë√£ train:
```python
df = pd.read_csv('tsd_test.csv', index_col=False)[['text']]
sents, alignments = [], []
for text in df['text']:
    pos = [i for i in range(len(text))]
    text, pos = preprocess(text, pos)
    tokens, alignment = tokenize(text, pos)
    sents.append(Sentence(tokens))
    alignments.append(alignment)

model = SequenceTagger.load('output/best-model.pt')
preds = eval(model, sents, alignments)
```
Cu·ªëi c√πng, ta t√≠nh F1 m·ª©c k√Ω t·ª± cho c√°c predictions c·ªßa model:
```python
from ast import literal_eval
df = pd.read_csv('data/tsd_test.csv', index_col=False)['spans']
df = df.tolist()
gt = [literal_eval(x) for x in df]
score = 0
for i in range(len(preds)):
    score += f1(preds[i], gt[i])
score /= len(preds)
print(score)
```
```
0.702659023493706
```
F1-score **0.7026!** ƒê√¢y l√† m·ªôt ƒëi·ªÉm s·ªë kh√° cao khi top 1 ƒë·∫°t 0.7083 v√† top 2 ƒë·∫°t 0.7077.

# K·∫øt
Tr√™n ƒë√¢y m√¨nh ƒë√£ gi·ªõi thi·ªáu b√†i to√°n Toxic Span Detection v√† NER implementation cho task 5 SemEval 2021 - m·ªôt trong nh·ªØng Workshop uy t√≠n nh·∫•t trong NLP. Implementation n√†y cho k·∫øt qu·∫£ r·∫•t ƒë√°ng h√†i l√≤ng, ƒëi·ªÅu n√†y c≈©ng nh·ªù m·ªôt ph·∫ßn l·ªõn t·ª´ th∆∞ vi·ªán Flair v·ªõi h√†ng lo·∫°t c√°c tools, modules cho b√†i to√°n NER. N·∫øu th·∫•y  b√†i vi·∫øt h·ªØu √≠ch h√£y cho m√¨nh m·ªôt upvote nh√© :D