# Introduction
Image to Image translation lÃ  quÃ¡ trÃ¬nh táº¡o ra  phiÃªn báº£n má»›i cá»§a má»™t bá»©c áº£nh vá»›i  má»™t Ä‘áº·c trÆ°ng cá»¥ thá»ƒ. VÃ­ dá»¥ nhÆ° chuyá»ƒn tá»« áº£nh grayscale sang áº£nh mÃ u, áº£nh máº·t ngÆ°á»i tháº­t sang áº£nh anime, tÄƒng Ä‘á»™ phÃ¢n giáº£i cá»§a áº£nh

![](https://images.viblo.asia/c20bfcb6-8d64-45d7-9e55-dee12563aacb.jpg)

![](https://images.viblo.asia/34695d49-1637-4bb9-a431-1c9c5018beb2.png)

![](https://images.viblo.asia/c2e62ef4-c65f-4311-b114-8efbb228f5bc.jpg)

Äá»ƒ huáº¥n luyá»‡n mÃ´ hÃ¬nh cho bÃ i toÃ¡n image to image translation theo hÆ°á»›ng supervised learning, ta sáº½ cáº§n má»™t lÆ°á»£ng lá»›n cÃ¡c cáº·p áº£nh input vÃ  label. VÃ­ dá»¥ nhÆ°: áº£nh mÃ u vÃ  áº£nh grayscale tÆ°Æ¡ng á»©ng vá»›i nÃ³, áº£nh má» vÃ  áº£nh Ä‘Ã£ Ä‘Æ°á»£c lÃ m nÃ©t. CÃ¡c vÃ­ dá»¥ dá»¯ liá»‡u á»Ÿ trÃªn cÃ³ thá»ƒ Ä‘Æ°á»£c táº¡o ra khÃ¡ dá»… dÃ ng báº±ng cÃ¡c phÆ°Æ¡ng phÃ¡p xá»­ lÃ½ áº£nh. Tuy nhiÃªn,  cÅ©ng cÃ³ vÃ´ sá»‘ trÆ°á»ng há»£p mÃ  viá»‡c táº¡o ra cáº·p dataset nhÆ° váº­t lÃ  gáº§n nhÆ° khÃ´ng thá»ƒ:
* Style transfer áº£nh tá»« mÃ¹a hÃ¨ sang mÃ¹a Ä‘Ã´ng (kiáº¿m Ä‘Æ°á»£c áº£nh phong cáº£nh trong cÃ¡c Ä‘iá»u kiá»‡n khÃ¡c nhau)
*   Chuyá»ƒn áº£nh chá»¥p sang phong cÃ¡ch cá»§a Van Gogh (táº¡ch rá»“i sao gá»i Ã´ng áº¥y vá» váº½ Ä‘Æ°á»£c ná»¯a :v)
*   Face filter máº·t ngÆ°á»i sang anime
*    Biáº¿n ngá»±a thÆ°á»ng thÃ nh ngá»±a váº±n (khÃ³ mÃ  kiáº¿m Ä‘Æ°á»£c áº£nh cá»§a 1 con ngá»±a thÆ°á»ng vÃ  áº£nh cá»§a nÃ³ nhÆ°ng lÃ  ngá»±a váº±n ðŸ˜„).

Do cÃ¡c bá»™ dataset theo cáº·p gáº§n nhÆ° lÃ  khÃ´ng tá»“n táº¡i hoáº·c khÃ³ Ä‘á»ƒ Ä‘Ã¡nh nhÃ£n nÃªn cÃ¡c nhÃ  nghiÃªn cá»©u má»›i hÆ°á»›ng tá»›i giáº£i quyáº¿t bÃ i toÃ¡n image to image translation theo hÆ°á»›ng unsupervised vá»›i dá»¯ liá»‡u unpaired. Cá»¥ thá»ƒ hÆ¡n lÃ  ta cÃ³ thá»ƒ sá»­ dá»¥ng báº¥t ká»³ hai táº­p áº£nh khÃ´ng liÃªn quan vÃ  cÃ¡c Ä‘áº·c trÆ°ng chung Ä‘Æ°á»£c trÃ­ch xuáº¥t tá»« má»—i bá»™ sÆ°u táº­p vÃ  sá»­ dá»¥ng trong quÃ¡ trÃ¬nh image translation. ÄÃ¢y Ä‘Æ°á»£c gá»i lÃ  bÃ i toÃ¡n unpaired image-to-image translation.

Hiá»‡n nay, cÃ¡c cÃ¡ch tiáº¿p cáº­n tá»‘t nháº¥t cho bÃ i toÃ¡n image to image translation Ä‘á»u Generative Adversarial Network (GAN). TiÃªn phong cho bÃ i toÃ¡n unpaired image to image translation cÃ³ thá»ƒ ká»ƒ Ä‘áº¿n mÃ´ hÃ¬nh: CycleGAN vÃ  DualGAN hay gáº§n Ä‘Ã¢y hÆ¡n lÃ  má»™t cáº£i tiáº¿n cá»§a CycleGAN lÃ  **GANILLA - Generative Adversarial Networks for Image to
Illustration Translation**

# Generative Advesarial Network
Generative Adversarial Networks, lÃ  má»™t há» cÃ¡c mÃ´ hÃ¬nh ná»•i tiáº¿ng vá»›i viá»‡c sá»­ dá»¥ng cÃ¡c mÃ´ hÃ¬nh con Ä‘á»‘i nghá»‹ch nhau (Adversarial) Ä‘á»ƒ sinh ra (Generative) dá»¯ liá»‡u. GAN cáº¥u táº¡o gá»“m 2 máº¡ng nÆ¡ron lÃ  Generator vÃ  Discriminator. Trong khi Generator sinh ra cÃ¡c dá»¯ liá»‡u giá»‘ng nhÆ° tháº­t thÃ¬ Discriminator cá»‘ gáº¯ng phÃ¢n biá»‡t Ä‘Ã¢u lÃ  dá»¯ liá»‡u Ä‘Æ°á»£c sinh ra tá»« Generator vÃ  Ä‘Ã¢u lÃ  dá»¯ liá»‡u tháº­t cÃ³.

Má»™t vÃ­ dá»¥ ná»•i tiáº¿ng minh há»a cho mÃ´ hÃ¬nh GAN lÃ  cuá»™c chiáº¿n giá»¯a cáº£nh sÃ¡tvÃ  tá»™i pháº¡m lÃ m tiá»n giáº£. Vá»›i dá»¯ liá»‡u cÃ³ Ä‘Æ°á»£c lÃ  tiá»n tháº­t, Generator giá»‘ngnhÆ° tÃªn tá»™i pháº¡m cÃ²n Discriminator giá»‘ng nhÆ° cáº£nh sÃ¡t. TÃªn tá»™i pháº¡m sáº½ cá»‘ gáº¯ng lÃ m ra tiá»n giáº£ mÃ  cáº£nh sÃ¡t cÅ©ng khÃ´ng phÃ¢n biá»‡t Ä‘Æ°á»£c. CÃ²n cáº£nh sÃ¡t sáº½ pháº£i phÃ¢n biá»‡t Ä‘Ã¢u lÃ  tiá»n tháº­t vÃ  Ä‘Ã¢u lÃ  tiá»n giáº£. Má»¥c tiÃªu cuá»‘i cÃ¹ng cá»§a tÃªn tá»™i pháº¡m lÃ  lÃ m ra tiá»n mÃ  cáº£nh sÃ¡t cÅ©ng khÃ´ng phÃ¢n biá»‡t Ä‘Æ°á»£c Ä‘Ã¢u lÃ  tháº­t vÃ  Ä‘Ã¢u lÃ  giáº£ vÃ  mang tiá»n Ä‘i tiÃªu Ä‘Æ°á»£c. Cáº£nh sÃ¡t cÅ©ng qua nhiá»u láº§n tháº¥y tiá»n giáº£ mÃ  kháº£ nÄƒng phÃ¢n biá»‡t cÅ©ng tÄƒng lÃªn. Tá»« Ä‘Ã³, dáº«n Ä‘áº¿n tá»™i pháº¡m sáº½ pháº£i nÃ¢ng cáº¥p kháº£ nÄƒng lÃ m tiá»n giáº£ cá»§a mÃ¬nh.

# Kiáº¿n trÃºc mÃ´ hÃ¬nh GANILLA
BÃ i bÃ¡o GANILLA Ä‘Ã£ giá»›i thiá»‡u má»™t domain má»›i trong bÃ i toÃ¡n style transfer: tranh minh há»a cho sÃ¡ch tráº» em. So vá»›i cÃ¡c domain truyá»n thá»‘ng trong cÃ¡c paper vá» image to image translation, tÃ¡c giáº£ bÃ i bÃ¡o cho ráº±ng domain má»›i nÃ y cÃ³ tÃ­nh trá»«u tÆ°á»£ng cao hÆ¡n

![](https://images.viblo.asia/46116240-6656-4989-860f-fea59f1c6565.png)

CÃ¡c mÃ´ hÃ¬nh truyá»n thá»‘ng nhÆ° CycleGAN hay DualGAN gáº·p khÃ³ khÄƒn trong viá»‡c cÃ¢n báº±ng  phong cÃ¡ch (style) trá»«u tÆ°á»£ng trong tranh minh há»a vÃ  ná»™i dung (content) gá»‘c cá»§a áº£nh Ä‘Æ°á»£c transfer sang. 

![](https://images.viblo.asia/749a56d1-8255-4b2b-8b27-5d9f1b1b8abf.png)

Äá»ƒ giáº£i quyáº¿t váº¥n Ä‘á» cÃ¢n báº±ng giá»¯a style vÃ  content, bÃ i bÃ¡o Ä‘á» xuáº¥t má»™t sá»‘ thay Ä‘á»•i vá»›i kiáº¿n trÃºc generator cá»§a CycleGAN. CÅ©ng giá»‘ng nhÆ° CycleGAN, GANILLA bao gá»“m 2 Generator vÃ  2 Discriminator. Generator Ä‘áº§u tiÃªn gá»i lÃ  G, nháº­n Ä‘áº§u vÃ o lÃ  áº£nh tá»« domain X (tranh minh há»a) vÃ  convert nÃ³ sang domain Y (áº£nh chá»¥p phong cáº£nh). Generator cÃ²n láº¡i gá»i lÃ  Y, cÃ³ nhiá»‡m vá»¥ convert áº£nh tá»« domain Y sang X. Má»—i máº¡ng Generator cÃ³ 1 Discriminator tÆ°Æ¡ng á»©ng vá»›i nÃ³
* $D_Y$: phÃ¢n biá»‡t áº£nh láº¥y tá»« domain Y vÃ  áº£nh Ä‘Æ°á»£c translate G(x).
* $D_X$: phÃ¢n biá»‡t áº£nh láº¥y tá»« domain X vÃ  áº£nh Ä‘Æ°á»£c translate F(y).

![](https://images.viblo.asia/bbaa6c06-d707-4bd8-b988-81b44086136f.png)

## Generator
Kiáº¿n trÃºc generator bao gá»“m 2 giai Ä‘oáº¡n upsampling vÃ  downsampling

Pháº§n downsampling lÃ  má»™t máº¡ng Resnet18 vá»›i má»™t sá»‘ sá»­a Ä‘á»•i:
* Thay tháº¿ lá»›p batch normalization báº±ng [instance normalization](https://arxiv.org/abs/1607.08022)
* Táº¡i cuá»‘i má»—i khá»‘i residual, thay vÃ¬ cá»™ng feature map cá»§a input vÃ  output cá»§a khá»‘i, ta tiáº¿n hÃ nh concat. 

Cá»¥ thá»ƒ hÆ¡n, pháº§n sample sáº½ báº¯t Ä‘áº§u báº±ng lá»›p convolution vá»›i filter $7 \times 7$ ná»‘i tiáº¿p bá»Ÿi lá»›p instance normalization, hÃ m kÃ­ch hoáº¡t ReLU vÃ  1 lá»›p max pooling giáº£m kÃ­ch thÆ°á»›c feature map xuá»‘ng 1/2. Tiáº¿p theo, tÆ°Æ¡ng tá»± nhÆ° kiáº¿n trÃºc resnet18, ta cÃ³ 4 lá»›p con I, II, III, IV, má»—i lá»›p bao gá»“m 2 khá»‘i residual. Má»—i khá»‘i residual bao gá»“m 1 lá»›p convolution vá»›i filter $3 \times 3$, instance normalization, hÃ m kÃ­ch hoáº¡t ReLU theo sau bá»Ÿi 1 lá»›p convolution vÃ  1 lá»›p instance normalization ná»¯a. Input cá»§a khá»‘i residual sáº½ Ä‘Æ°á»£c concat vá»›i output cá»§a nÃ³. Output cá»§a quÃ¡ trÃ¬nh nÃ y sau Ä‘Ã³ Ä‘Æ°á»£c cho qua 1 lá»›p convolution vÃ  ReLU cuá»‘i cÃ¹ng. KÃ­ch thÆ°á»›c feature map sáº½ giáº£m Ä‘i má»™t ná»­a báº±ng convolution stride 2 sau má»—i lá»›p con trá»« lá»›p I

![image.png](https://images.viblo.asia/896635dd-3fa2-49b4-9062-e14f34cc1637.png)

Trong pháº§n upsampling, tÃ¡c giáº£ sá»­ dá»¥ng skip connection giá»‘ng mÃ´ hÃ¬nh [Unet](https://arxiv.org/abs/1505.04597), Ä‘á»ƒ káº¿t há»£p cÃ¡c feature map tá»« lá»›p con cá»§a pháº§n downsample vá»›i cÃ¡c output tá»« cÃ¡c lá»›p tÃ­ch cháº­p cá»§a pháº§n upsampling. á»ž Ä‘Ã¢y, thay vÃ¬ sá»­ dá»¥ng concat, tÃ¡c giáº£ sá»­ dá»¥ng phÃ©p cá»™ng cÃ¡c feature map. CÃ¡c káº¿t ná»‘i táº¯t nÃ y giÃºp báº£o toÃ n content cá»§a áº£nh. Äáº§u tiÃªn, output cá»§a lá»›p con IV tá»« downsample Ä‘Æ°á»£c cho qua 1 lá»›p convolution filter $1 \times 1$ vÃ  nearest neighbor upsampling Ä‘á»ƒ cÃ³ kÃ­ch thÆ°á»›c báº±ng feature map cá»§a lá»›p con III. QuÃ¡ trÃ¬nh nÃ y Ä‘Æ°á»£c tiáº¿p tá»¥c á»Ÿ cÃ¡c lá»›p con tiáº¿p theo. Cuá»‘i cÃ¹ng, áº£nh output Ä‘Æ°á»£c táº¡o ra tá»« má»™t lá»›p convolution $7 \times 7$ vá»›i 3 filter..

## Discriminator
GANILLA váº«n giá»¯ nguyÃªn kiáº¿n trÃºc CNN PatchGAN tá»« CycleGAN. MÃ´ hÃ¬nh Ä‘Æ°á»£c táº¡o thÃ nh tá»« 3 khá»‘i, má»—i khá»‘i bao gá»“m 2 lá»›p convolution vá»›i kernel $4 \times 4$ theo sau bá»Ÿi lá»›p instance normalization vÃ  hÃ m kÃ­ch hoáº¡t LeakyReLU. Khá»‘i Ä‘áº§u tiÃªn sáº½ cÃ³ 64 filter vÃ  gáº¥p Ä‘Ã´i vá»›i má»—i khá»‘i á»Ÿ sau. Output cá»§a Discriminator sáº½ lÃ  má»™t lÆ°á»›i $70 \times 70$. Má»—i Ã´ sáº½ dá»± Ä‘oÃ¡n xÃ¡c suáº¥t lÃ  tháº­t giáº£ cá»§a má»™t vÃ¹ng tÆ°Æ¡ng á»©ng trÃªn áº£nh.

## Loss function
Pháº§n nÃ y cÅ©ng tÆ°Æ¡ng tá»± CycleGAN, bao gá»“m 2 loss thÃ nh pháº§n

### Adversarial loss
Gá»“m 2 GAN loss cho 2 cáº·p Generator vÃ  Discriminator

   $L_{GAN}(G, D_Y, X, Y) = E_{y \ \sim ~ p_{data}(y)}[ logD_{Y}(y)] + E_{x \ \sim ~ p_{data}(x)}[log(1- D_Y(G(x))]$
   
$L_{adv}(F, D_X, Y, X ) = E_{x \ \sim ~ p_{data}(x)}[ logD_{X}(x)] + E_{y \ \sim ~ p_{data}(y)}[log(1- D_X(F(y))]$

### Cycle consistency loss
Äá»ƒ Ä‘áº£m báº£o khi ta translate áº£nh X sang domain Y, sau Ä‘Ã³ translate ngÆ°á»£c láº¡i vá» X sáº½ Ä‘Æ°á»£c áº£nh ban Ä‘áº§u
   $L_{cycle}(G, F) = \frac{1}{n}\sum||F(G(x_i)) - x_i||_1+||G(F(y_i)) - y_i||_1$ 

### Full Loss
  $L =L_{GAN}(G, D_Y, X, Y) +L_{adv}(F, D_X, Y, X ) + \lambda L_{cycle}(G, F)$
  
  # CÃ i Ä‘áº·t mÃ´ hÃ¬nh
  Äá»ƒ cÃ i Ä‘áº·t vÃ  huáº¥n luyá»‡n mÃ´ hÃ¬nh, mÃ¬nh sáº½ sá»­ dá»¥ng [pytorch](https://github.com/pytorch/pytorch), [fastai](https://github.com/fastai/fastai) vÃ  [UPIT](https://github.com/tmabraham/UPIT).
  Install cÃ¡c thÆ° viá»‡n cáº§n thiáº¿t
  ```python
  pip install fastai==2.3.0
pip install git+https://github.com/tmabraham/UPIT.git
  ```
### Generator
  ```python
  from fastai.vision.all import *
from fastai.basics import *
from typing import List

# Cell
class BasicBlock_Ganilla(nn.Module):
    expansion = 1
    def __init__(self, in_planes, planes, use_dropout, stride=1):
        super(BasicBlock_Ganilla, self).__init__()
        self.rp1 = nn.ReflectionPad2d(1)
        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=0, bias=False)
        self.bn1 = nn.InstanceNorm2d(planes)
        self.use_dropout = use_dropout
        if use_dropout:
            self.dropout = nn.Dropout(use_dropout)
        self.rp2 = nn.ReflectionPad2d(1)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=0, bias=False)
        self.bn2 = nn.InstanceNorm2d(planes)
        self.out_planes = planes

        self.shortcut = nn.Sequential()
        if stride != 1 or in_planes != self.expansion*planes:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),
                nn.InstanceNorm2d(self.expansion*planes)
            )

            self.final_conv = nn.Sequential(
                nn.ReflectionPad2d(1),
                nn.Conv2d(self.expansion * planes * 2, self.expansion * planes, kernel_size=3, stride=1,
                                        padding=0, bias=False),
                nn.InstanceNorm2d(self.expansion * planes)
            )
        else:
            self.final_conv = nn.Sequential(
                nn.ReflectionPad2d(1),
                nn.Conv2d(planes*2, planes, kernel_size=3, stride=1, padding=0, bias=False),
                nn.InstanceNorm2d(planes)
            )

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(self.rp1(x))))
        if self.use_dropout:
            out = self.dropout(out)
        out = self.bn2(self.conv2(self.rp2(out)))
        inputt = self.shortcut(x)
        catted = torch.cat((out, inputt), 1)
        out = self.final_conv(catted)
        out = F.relu(out)
        return out

# Cell
class PyramidFeatures(nn.Module):
    def __init__(self, C2_size, C3_size, C4_size, C5_size, fpn_weights, feature_size=128):
        super(PyramidFeatures, self).__init__()

        self.sum_weights = fpn_weights #[1.0, 0.5, 0.5, 0.5]

        # upsample C5 to get P5 from the FPN paper
        self.P5_1 = nn.Conv2d(C5_size, feature_size, kernel_size=1, stride=1, padding=0)
        self.P5_upsampled = nn.Upsample(scale_factor=2, mode='nearest')
        #self.rp1 = nn.ReflectionPad2d(1)
        #self.P5_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=0)

        # add P5 elementwise to C4
        self.P4_1 = nn.Conv2d(C4_size, feature_size, kernel_size=1, stride=1, padding=0)
        self.P4_upsampled = nn.Upsample(scale_factor=2, mode='nearest')
        #self.rp2 = nn.ReflectionPad2d(1)
        #self.P4_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=0)

        # add P4 elementwise to C3
        self.P3_1 = nn.Conv2d(C3_size, feature_size, kernel_size=1, stride=1, padding=0)
        self.P3_upsampled = nn.Upsample(scale_factor=2, mode='nearest')
        #self.rp3 = nn.ReflectionPad2d(1)
        #self.P3_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=0)

        self.P2_1 = nn.Conv2d(C2_size, feature_size, kernel_size=1, stride=1, padding=0)
        self.P2_upsampled = nn.Upsample(scale_factor=2, mode='nearest')
        self.rp4 = nn.ReflectionPad2d(1)
        self.P2_2 = nn.Conv2d(int(feature_size), int(feature_size/2), kernel_size=3, stride=1, padding=0)

        #self.P1_1 = nn.Conv2d(feature_size, feature_size, kernel_size=1, stride=1, padding=0)
        #self.P1_upsampled = nn.Upsample(scale_factor=2, mode='nearest')
        #self.rp5 = nn.ReflectionPad2d(1)
        #self.P1_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=0)

    def forward(self, inputs):

        C2, C3, C4, C5 = inputs

        i = 0
        P5_x = self.P5_1(C5) * self.sum_weights[i]
        P5_upsampled_x = self.P5_upsampled(P5_x)
        #P5_x = self.rp1(P5_x)
        # #P5_x = self.P5_2(P5_x)
        i += 1
        P4_x = self.P4_1(C4) * self.sum_weights[i]
        P4_x = P5_upsampled_x + P4_x
        P4_upsampled_x = self.P4_upsampled(P4_x)
        #P4_x = self.rp2(P4_x)
        # #P4_x = self.P4_2(P4_x)
        i += 1
        P3_x = self.P3_1(C3) * self.sum_weights[i]
        P3_x = P3_x + P4_upsampled_x
        P3_upsampled_x = self.P3_upsampled(P3_x)
        #P3_x = self.rp3(P3_x)
        #P3_x = self.P3_2(P3_x)
        i += 1
        P2_x = self.P2_1(C2) * self.sum_weights[i]
        P2_x = P2_x * self.sum_weights[2] + P3_upsampled_x
        P2_upsampled_x = self.P2_upsampled(P2_x)
        P2_x = self.rp4(P2_upsampled_x)
        P2_x = self.P2_2(P2_x)

        return P2_x

# Cell
class ResNet(nn.Module):

    def __init__(self, input_nc, output_nc, ngf, use_dropout, fpn_weights, block, layers):
        self.inplanes = ngf
        super(ResNet, self).__init__()

        # first conv
        self.pad1 = nn.ReflectionPad2d(input_nc)
        self.conv1 = nn.Conv2d(input_nc, ngf, kernel_size=7, stride=1, padding=0, bias=True)
        self.in1 = nn.InstanceNorm2d(ngf)
        self.relu = nn.ReLU(inplace=True)
        self.pad2 = nn.ReflectionPad2d(1)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=0)

        # Output layer
        self.pad3 = nn.ReflectionPad2d(output_nc)
        self.conv2 = nn.Conv2d(64, output_nc, 7)
        self.tanh = nn.Tanh()


        if block == BasicBlock_Ganilla:
            # residuals
            self.layer1 = self._make_layer_ganilla(block, 64, layers[0], use_dropout, stride=1)
            self.layer2 = self._make_layer_ganilla(block, 128, layers[1], use_dropout, stride=2)
            self.layer3 = self._make_layer_ganilla(block, 128, layers[2], use_dropout, stride=2)
            self.layer4 = self._make_layer_ganilla(block, 256, layers[3], use_dropout, stride=2)

            fpn_sizes = [self.layer1[layers[0] - 1].conv2.out_channels,
                         self.layer2[layers[1] - 1].conv2.out_channels,
                         self.layer3[layers[2] - 1].conv2.out_channels,
                         self.layer4[layers[3] - 1].conv2.out_channels]

        else:
            print("This block type is not supported")
            sys.exit()

        self.fpn = PyramidFeatures(fpn_sizes[0], fpn_sizes[1], fpn_sizes[2], fpn_sizes[3], fpn_weights)


    def _make_layer_ganilla(self, block, planes, blocks, use_dropout, stride=1):
        strides = [stride] + [1] * (blocks - 1)
        layers = []
        for stride in strides:
            layers.append(block(self.inplanes, planes, use_dropout, stride))
            self.inplanes = planes * block.expansion
        return nn.Sequential(*layers)

    def freeze_bn(self):
        '''Freeze BatchNorm layers.'''
        for layer in self.modules():
            if isinstance(layer, nn.BatchNorm2d):
                layer.eval()

    def forward(self, inputs):

        img_batch = inputs

        x = self.pad1(img_batch)
        x = self.conv1(x)
        x = self.in1(x)
        x = self.relu(x)
        x = self.pad2(x)
        x = self.maxpool(x)

        x1 = self.layer1(x)
        x2 = self.layer2(x1)
        x3 = self.layer3(x2)
        x4 = self.layer4(x3)

        out = self.fpn([x1, x2, x3, x4]) # use all resnet layers

        out = self.pad3(out)
        out = self.conv2(out)
        out = self.tanh(out)

        return out

def ganilla_generator(input_nc, output_nc, ngf, drop, fpn_weights=[1.0, 1.0, 1.0, 1.0], init_type='normal', gain=0.02, **kwargs):
    """Constructs a ResNet-18 GANILLA generator."""
    model = ResNet(input_nc, output_nc, ngf, drop, fpn_weights, BasicBlock_Ganilla, [2, 2, 2, 2],  **kwargs)
    return model
  ```
  
### PatchGAN Discrminator
  ```python
  def conv_norm_lr(ch_in:int, ch_out:int, norm_layer:nn.Module=None, ks:int=3, bias:bool=True, pad:int=1, stride:int=1,
                 activ:bool=True, slope:float=0.2, init=nn.init.normal_, init_gain:int=0.02)->List[nn.Module]:
    conv = nn.Conv2d(ch_in, ch_out, kernel_size=ks, padding=pad, stride=stride, bias=bias)
    if init:
        if init == nn.init.normal_:
            init(conv.weight, 0.0, init_gain)
        else:
            init(conv.weight)
        if hasattr(conv, 'bias') and hasattr(conv.bias, 'data'): conv.bias.data.fill_(0.)
    layers = [conv]
    if norm_layer is not None: layers.append(norm_layer(ch_out))
    if activ: layers.append(nn.LeakyReLU(slope, inplace=True))
    return layers
    
  def discriminator(ch_in:int, n_ftrs:int=64, n_layers:int=3, norm_layer:nn.Module=None, sigmoid:bool=False)->nn.Module:
    norm_layer = ifnone(norm_layer, nn.InstanceNorm2d)
    bias = (norm_layer == nn.InstanceNorm2d)
    layers = conv_norm_lr(ch_in, n_ftrs, ks=4, stride=2, pad=1)
    for i in range(n_layers-1):
        new_ftrs = 2*n_ftrs if i <= 3 else n_ftrs
        layers += conv_norm_lr(n_ftrs, new_ftrs, norm_layer, ks=4, stride=2, pad=1, bias=bias)
        n_ftrs = new_ftrs
    new_ftrs = 2*n_ftrs if n_layers <=3 else n_ftrs
    layers += conv_norm_lr(n_ftrs, new_ftrs, norm_layer, ks=4, stride=1, pad=1, bias=bias)
    layers.append(nn.Conv2d(new_ftrs, 1, kernel_size=4, stride=1, padding=1))
    if sigmoid: layers.append(nn.Sigmoid())
    return nn.Sequential(*layers)
  ```
  
 ### Training
 Viá»‡c huáº¥n luyá»‡n mÃ´ hÃ¬nh khÃ¡ Ä‘Æ¡n giáº£n vá»›i sá»± há»— trá»£ cá»§a UPIT vÃ  fastai. á»ž Ä‘Ã¢y mÃ¬nh sáº½ sá»­ dá»¥ng bá»™ dataset [monet2photo](https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/). Cáº¥u trÃºc thÆ° má»¥c cá»§a dataset sáº» nhÆ° tháº¿ nÃ y
 
![image.png](https://images.viblo.asia/a4218075-7e33-474b-8fa9-2c432ef7d040.png)

![image.png](https://images.viblo.asia/457b279f-520e-46a4-9b81-b2943d93eb81.png)
 ```python
 from fastai.vision.all import *
from upit.data.unpaired import get_dls
from upit.models.ganilla import *
from upit.train.cyclegan import cycle_learner
from upit.tracking.wandb import SaveModelAtEndCallback
image_path = Path('monet2photo') 

trainA_path = image_path / 'trainA'
trainB_path = image_path / 'trainB'

ganilla = GANILLA(3, 3, 64)
dls = get_dls(trainA_path, trainB_path, load_size=256, crop_size=224, num_workers=4)

learn = cycle_learner(dls,
                      ganilla,
                      opt_func=partial(Adam,mom=0.5,sqr_mom=0.999),
                      cbs=[SaveModelAtEndCallback()],
                      show_imgs=False)
learn.fit(50,50,1e-4)
 ```
 Káº¿t quáº£ 
 ![](https://images.viblo.asia/0b591d2d-2326-4080-996f-b2cb61d0538f.png)

![](https://images.viblo.asia/e2715678-6ab2-44f4-81db-2330ced3dc61.png)

![](https://images.viblo.asia/b4daba67-ca35-45bb-b7bf-ed5d3fcf35ae.png)

## Extra: Ghibli dataset
![image.png](https://images.viblo.asia/8fed9442-9257-4237-b7da-da754e24b942.png)
Káº¿t quáº£ khi train trÃªn bá»™ áº£nh crop ra tá»« phim Ghibli [(link)](https://github.com/TachibanaYoshino/AnimeGAN/releases/tag/dataset-1)

![](https://images.viblo.asia/9229e6a3-7b16-4505-b6b9-9e8734758d16.png)

![](https://images.viblo.asia/9e98575b-d142-433d-aa9b-b379fa8af922.png)

![](https://images.viblo.asia/d14554dd-db79-408a-8c48-2203fdef79d6.png)

# References
* GANILLA [https://arxiv.org/pdf/2002.05638.pdf](https://arxiv.org/pdf/2002.05638.pdf)
* CycleGAN [https://arxiv.org/abs/1703.10593](https://arxiv.org/abs/1703.10593)
* DualGAN [https://arxiv.org/abs/1704.02510](https://arxiv.org/abs/1704.02510)
* https://github.com/tmabraham/UPIT
* https://github.com/TachibanaYoshino/AnimeGAN/