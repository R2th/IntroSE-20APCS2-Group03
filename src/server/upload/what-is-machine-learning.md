Machine learning has caused worldwide technological fever in the past few years. In the academic world, there are thousands of scientific articles on the subject each year. In the industry, from large companies like Google, Facebook, Microsoft to startup companies are investing in machine learning. The wide range of applications that use machine learning comes in all walks of life, from computer science to less relevant fields such as physics, chemistry, medicine, politics. AlphaGo , the go-to-play machine with the ability to calculate in a space with more than the number of particles in the universe, is better than any great player, is one of many eloquent examples. For the superiority of machine learning compared to classical methods.
In essence, what is machine learning?
To introduce the machine learning, I rely on its relationship with the following three concepts:
* Machine learning and artificial intelligence (AI)
* Machine learning and Big Data .
* Machine learning and predicting the future .

## Artificial Intelligence
Artificial Intelligence , AI, a phrase that is both close and alien to us. Close because the world is feverish with technologies labeled AI. It's strange because a real AI is still out of our reach. Speaking to AI, everyone would associate a different image. Recent decades have seen a change in the face of AI in international films. In the past, filmmakers regularly included robotic images in films (such as the Terminator ), which aimed to sow the viewer's mind that artificial intelligence was a human way of cloning humans mechanically. However, in more recent films on the subject, such as TranscendenceBecause Johnny Depp plays the lead, we do not see the image of a robot at all. Instead, it was a gigantic computer brain that commanded tens of thousands of nanobots, called Singularity. Of course both images are fictional and fictional, but such changes also partly reflect the change of human notions about AI. AI is now considered invisible, or in other words, able to carry any shape. Talking about AI is about a brain , not a body, a software, not a hardware.

In academia, as a whole, AI is a science that was born with the purpose of making computers intelligible. This goal is still vague because not everyone agrees with a unified definition of intelligence. Scientists must define some more specific goals, one of which is to make the computer trick the Turing Test . The Turing Test was created by Alan Turing (1912-1954), who is considered the father of modern computer science, to distinguish whether the opposite is human or not. first
AI represents a human goal . Machine learning is a tool that is expected to help people achieve that goal. And the reality is that machine learning has brought people far away on the road to conquer AI. But there is still a long way to go. Machine learning and AI are closely related, but not necessarily coincidental, on the one hand, and on the other hand on machine learning. Conquering AI is still the ultimate goal of machine learning, but the current machine learning focuses on short-term goals such as:
* Make the computer have basic human cognitive abilities such as hearing, vision, language understanding, mathematics, programming, etc.
* Helping people deal with the huge amount of information we face every day, or Big Data.

## Big Data
Big Data is not a legitimate science. It is a folk phrase and is acclaimed by the media to indicate the explosion of data today. It is no different from the terms "industrial revolution", "software era". Big Data is an inevitable consequence of the increasing number of Internet connections. With the advent of social networking, Facebook, Instagram, Twitter, the need to share human growth through a dizzying. Youtube can also be considered a social network where people share videos and comment on the content of the video.
Explosion of information is not the only reason for the introduction of Big Data. Remember that Big Data has been around for a few years, but the amount of data that has accumulated since the Internet came out at the end of last century is not small. But then people sit around a pile of data and do not know what to do with them apart from archiving and copying. Until one day, scientists realized that the data contained a huge amount of knowledge. That knowledge can help us understand more about people and society. From a person's favorite movie list, we can draw out the person's preferences and present those films he has never seen before, but in line with his interests. From the search list of the internet community we will know the hot topic is being interested and will focus more information on the issue. Big Data only really begins when we understand the value of information contained in the data, and have the resources and technology to exploit them on a massive scale. And it's no surprise that machine learning is a key component of that technology. Here we have a mutual relation between machine learning and Big Data: machine learning is further developed thanks to the increase in data volume of Big Data; Conversely, the value of Big Data depends on the ability to exploit knowledge from machine learning data. They have the resources and technology to exploit them on a huge scale. And it's no surprise that machine learning is a key component of that technology. Here we have a mutual relation between machine learning and Big Data: machine learning is further developed thanks to the increase in data volume of Big Data; Conversely, the value of Big Data depends on the ability to exploit knowledge from machine learning data. They have the resources and technology to exploit them on a huge scale. And it's no surprise that machine learning is a key component of that technology. Here we have a mutual relation between machine learning and Big Data: machine learning is further developed thanks to the increase in data volume of Big Data; Conversely, the value of Big Data depends on the ability to exploit knowledge from machine learning data.

Back in history, machine learning appeared long before the Internet was born. One of the first machine learning algorithms was the perceptron algorithm invented by Frank Rosenblatt in 1957. This is a classic algorithm used to classify two concepts. An example is the spam classification and the normal letter (square). You will hardly figure out how to do that. For perceptrons, this is no different from drawing a straight line on a plane to divide two sets of points:

![](https://images.viblo.asia/857b6683-3dd6-46c8-93c9-13235f3ba60c.png)

The triangular and square points represent the emails we already know the labels. They are used to train the perceptron. After drawing a line dividing the two points, we get the unmarked points representing the emails that need to be sorted (rounded points). We labeled a point by the label of the points along the half plane with that point.

A brief description of the mail classification process is described later. First of all, we need an algorithm to translate emails into data points. This stage is very important because if we choose to perform properly, the work of the perceptron will be much lighter. Next, the perceptron reads the coordinates of each point and uses this information to update the parameters of the straight line. You can see through the demo of the perceptron (the green point is the perceptron point of processing):

![](https://images.viblo.asia/672760f1-d490-48fc-b0b1-278c0d728da2.jpg)

Since it's a fairly simple algorithm, there are many problems that can arise with perceptrons, such as the sorting point located just above the dividing line. Or worse, with a more complex data set, the dividing line does not exist:

![](https://images.viblo.asia/3131ed92-f704-4fc6-9905-04024b97c801.png)

At this point, we need the type of line "not straight". But that's another story.

Perceptron is a supervised learning algorithm : we give the computer a series of samples with the same sample answer in the hope that the computer will find the traits needed to make predictions for other unanswered examples. future words. In addition, there are also machine learning algorithms that do not need sample answers, called unsupervised learning . In this case, the computer tries to exploit the hidden structure of a data set without the sample answer. Another type of machine learning is called reinforcement learning. In this form, there is no sample answer, but instead the computer receives a response for each action. Rely on positive or negative feedback that the computer will adjust accordingly. The following is an illustrative example:

![](https://images.viblo.asia/99d18ef0-f2dd-4086-b819-47c5f0648f36.jpg)

The goal of the car is to climb the top of the hill and get the star. The car has two moves to and fro. By testing the motions and getting feedback as the height reached and the time it takes to get the star, the car is getting better at climbing the hill.

Machine learning has a very close relationship with statistics . Machine learning uses statistical models to "remember" the distribution of data. However, not just memory, machine learning must be generalizableWhat has been seen and given predictions for cases not yet seen. You can imagine a machine learning model that is not as general as a rote learning child: only answering the answers that it has learned by heart. General ability is a natural and wonderful ability of man: You can not see all human faces in the world, but you can see whether one is a human face or not. Absolutely perfect. The culmination of machine learning will be the simulation of this generalization and inference of the human person.
## Predicting the future
As we have seen, talking about machine learning is about "anticipation": from predicting the classification label to predicting the action to take in the next step. So machine learning can predict the future or not? It may or may not: machine learning can predict the future, but only if the future is closely related to the present.
To finish, I want to take a look at a simple example. Suppose you were given a coin, then asked to toss the coin a number of times. The problem is: based on the coin toss, you predict the next hit. Just rely on the odds of the previous rolls, you can make a good prediction. But if you throw a different coin at a time, things will be different. Different coins have different probabilities. At this time the forecast is almost impossible because the probability of the next hit back has nothing to do with the launch. The same thing happens with predicting the future by machine learning, if we consider that every day there is a "coin" released to see if an event takes place or not. If the "coin" Tomorrow is chosen in a discretionary manner without any distribution of machine learning will fail. Fortunately, in many cases this is not entirely true, the world operates under certain rules and machine learning can recognize those laws. But after all, machine learning is not a prophetess, but just like us: judging by generalizing experiences that have been learned from the data.