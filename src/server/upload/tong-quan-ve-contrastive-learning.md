# Lá»i má»Ÿ Ä‘áº§u
Xin chÃ o táº¥t cáº£ má»i ngÆ°á»i, thá»i gian gáº§n Ä‘Ã¢y mÃ¬nh cÃ³ tÃ¬m hiá»ƒu vá» **Contrastive Learning** vÃ  mÃ¬nh cÃ³ dáº¡o má»™t vÃ²ng google thÃ¬ sá»‘ lÆ°á»£ng bÃ i viáº¿t chia sáº» vá» chá»§ Ä‘á» nÃ y cÃ²n khÃ¡ háº¡n cháº¿ nÃªn hÃ´m nay mÃ¬nh máº¡n phÃ©p Ä‘Ã³ng gÃ³p má»™t bÃ i viáº¿t vá» chá»§ Ä‘á» nÃ y, mong nháº­n Ä‘Æ°á»£c sá»± á»§ng há»™ vÃ  gÃ³p Ã½ tá»« táº¥t cáº£ cÃ¡c báº¡n. BÃ i viáº¿t vá»›i má»¥c Ä‘Ã­ch chia sáº» tá»›i cÃ¡c báº¡n má»™t cÃ¡i nhÃ¬n tá»•ng quan vá» bÃ i toÃ¡n **Contrastive Learning** cho cÃ¡c báº¡n muá»‘n tÃ¬m hiá»ƒu vá» chá»§ Ä‘á» nÃ y cÃ³ thá»ƒ tiáº¿p cáº­n má»™t cÃ¡ch dá»… dÃ ng hÆ¡n

# Contrastive learning lÃ  gÃ¬ ?
Ã tÆ°á»Ÿng chÃ­nh cá»§a Contrastive learning lÃ  tÃ¬m ra cÃ¡c cáº·p Ä‘áº·c trÆ°ng cá»§a dá»¯ liá»‡u cÃ³ tÃ­nh tÆ°Æ¡ng Ä‘á»“ng - tÆ°Æ¡ng pháº£n nhau trong bá»™ dataset. Tá»« Ä‘Ã³, vá»›i nhá»¯ng cáº·p dá»¯ liá»‡u mang tÃ­nh tÆ°Æ¡ng Ä‘á»“ng ta cÃ³ thá»ƒ "kÃ©o" chÃºng láº¡i gáº§n Ä‘á»ƒ há»c Ä‘Æ°á»£c nhá»¯ng Ä‘áº·c trÆ°ng cáº¥p cao hÆ¡n cá»§a nhau, vÃ  ngÆ°á»£c láº¡i vá»›i nhá»¯ng cáº·p nhá»¯ng liá»‡u tÆ°Æ¡ng pháº£n sáº½ bá»‹ "Ä‘áº©y" ra xa. Äá»ƒ lÃ m Ä‘Æ°á»£c Ä‘iá»u nÃ y, ta sáº½ cáº§n sá»­ dá»¥ng cÃ¡c similarity metric Ä‘á»ƒ tÃ­nh toÃ¡n khoáº£ng cÃ¡ch giá»¯a cÃ¡c embedding vector biá»ƒu diá»…n cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u vá»›i nhau. VÃ­ dá»¥, ta Ä‘Ã£ cÃ³ 1 Ä‘iá»ƒm dá»¯ liá»‡u gá»‘c gá»i lÃ  anchor, sau Ä‘Ã³ cÃ³ thá»ƒ dÃ¹ng thÃªm cÃ¡c ká»¹ thuáº­t augmentation khÃ¡c nhau Ä‘á»ƒ cÃ³ thÃªm 1 biáº¿n thá»ƒ tá»« anchor gá»‘c gá»i lÃ  positive sample, vÃ  pháº§n cÃ²n láº¡i cá»§a batch / dataset sáº½ Ä‘Æ°á»£c coi lÃ  negative sample. Sau Ä‘Ã³ model sáº½ Ä‘Æ°á»£c train Ä‘á»ƒ cÃ³ thá»ƒ phÃ¢n biá»‡t Ä‘Æ°á»£c positive sample vá»›i negative sample tá»« 1 cá»¥m dá»¯ liá»‡u.
![](https://images.viblo.asia/65e33ebd-d8d6-4a05-a102-7bfd6b49b440.png)

<div align="center">(Nguá»“n: https://towardsdatascience.com/understanding-contrastive-learning-d5b19fd96607)</div>
Nghe cÃ³ váº» hÆ¡i trá»«u tÆ°á»£ng nhá»‰ ? Láº¥y má»™t vÃ­ dá»¥ Ä‘Æ¡n giáº£n nhÆ° tháº¿ nÃ y nhÃ©. Con ngÆ°á»i chÃºng ta cÃ³ thá»ƒ dá»… dÃ ng phÃ¢n biá»‡t Ä‘Æ°á»£c cÃ¡c váº­t thá»ƒ khÃ¡c nhau tá»« khi cÃ²n bÃ© ( tháº­m chÃ­ lÃ  sÆ¡ sinh), vÃ  nhá»¯ng váº­t thá»ƒ Ä‘áº¥y sau nÃ y sáº½ Ä‘Æ°á»£c ngÆ°á»i lá»›n "gÃ¡n nhÃ£n". Váº­y táº¡i sao chÃºng ta láº¡i cÃ³ thá»ƒ phÃ¢n biá»‡t Ä‘Æ°á»£c cÃ¡c váº­t thá»ƒ Ä‘áº¥y tá»« láº§n nhÃ¬n Ä‘áº§u tiÃªn ? Bá»Ÿi vÃ¬ nÃ£o bá»™ cá»§a chÃºng ta cÃ³ thá»ƒ nháº­n biáº¿t Ä‘Æ°á»£c nhá»¯ng Ä‘áº·c Ä‘iá»ƒm, Ä‘áº·c trÆ°ng khÃ¡c biá»‡t nháº¥t cá»§a má»™t váº­t thá»ƒ hay cÃ²n gá»i lÃ  Ä‘áº·c trÆ°ng báº­c cao, vÃ  Ä‘em so sÃ¡nh nhá»¯ng Ä‘áº·c trÆ°ng Ä‘áº¥y vá»›i "dá»¯ liá»‡u" cÃ³ sáºµn trong nÃ£o bá»™ Ä‘á»ƒ cÃ³ thá»ƒ phÃ¢n biá»‡t Ä‘Æ°á»£c nhá»¯ng váº­t thá»ƒ giá»‘ng - khÃ¡c loáº¡i ( loÃ i ). Chá»‰ cáº§n nháº­n ra nhá»¯ng Ä‘iá»ƒm tÆ°Æ¡ng Ä‘á»“ng vÃ  khÃ¡c biá»‡t giá»¯a váº­t khÃ¡c nhau, bá»™ nÃ£o cÃ³ thá»ƒ há»c Ä‘Æ°á»£c cÃ¡c Ä‘áº·c Ä‘iá»ƒm cáº¥p cao cá»§a cÃ¡c Ä‘á»‘i tÆ°á»£ng trong tháº¿ giá»›i cá»§a chÃºng ta. VÃ­ dá»¥, chÃºng ta cÃ³ thá»ƒ nháº­n ra má»™t cÃ¡ch vÃ´ thá»©c ráº±ng hai con mÃ¨o á»Ÿ áº£nh trÃªn cÃ³ Ä‘Ã´i tai nhá»n, trong khi con chÃ³ cÃ³ Ä‘Ã´i tai cá»¥p xuá»‘ng. Hoáº·c chÃºng ta cÃ³ thá»ƒ Ä‘á»‘i chiáº¿u chiáº¿c mÅ©i nhÃ´ ra cá»§a chÃ³ vá»›i máº·t pháº³ng cá»§a mÃ¨o. Vá» cÆ¡ báº£n thÃ¬ contrastive learning cÅ©ng cho phÃ©p model cá»§a chÃºng ta lÃ m Ä‘iá»u tÆ°Æ¡ng tá»±, má»¥c tiÃªu cá»§a phÆ°Æ¡ng phÃ¡p lÃ  Ä‘á»‘i chiáº¿u sá»± tÆ°Æ¡ng pháº£n giá»¯a embedding cá»§a cÃ¡c phiÃªn báº£n biáº¿n Ä‘á»•i cá»§a cÃ¹ng má»™t sample Ä‘á»ƒ há»c cÃ¡c Ä‘áº·c trÆ°ng báº¥t biáº¿n cá»§a nÃ³ trong khi váº«n phÃ¢n biá»‡t Ä‘Æ°á»£c embedding cá»§a cÃ¡c sample khÃ¡c.



## Táº¡i sao Contrastive learning láº¡i Ä‘Æ°á»£c coi lÃ  má»™t phÆ°Æ¡ng phÃ¡p cá»±c ká»³ máº¡nh máº½ vÃ  cÃ ng ngÃ y cÃ ng phá»• biáº¿n ?
CÃ¡c phÆ°Æ¡ng phÃ¡p supervised learning truyá»n thá»‘ng phá»¥ thuá»™c ráº¥t nhiá»u vÃ o lÆ°á»£ng dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c gÃ¡n nhÃ£n sáºµn trong khi lÆ°á»£ng data chÆ°a Ä‘Æ°á»£c gÃ¡n nhÃ£n lÃ  vÃ´ cÃ¹ng khá»•ng lá»“. Máº·c dÃ¹ sá»‘ dataset Ä‘Ã£ cÃ³ nhÃ£n lÃ  ráº¥t lá»›n nhÆ°ng vá»›i sá»± gia tÄƒng máº¡nh máº½ cá»§a nhu cáº§u vá» lÆ°á»£ng dá»¯ liá»‡u cÅ©ng nhÆ° cÃ¡c bÃ i toÃ¡n má»›i thÃ¬ sá»‘ dataset Ä‘áº¥y lÃ  hoÃ n toÃ n khÃ´ng Ä‘á»§. Nháº¥t lÃ  vá»›i nhá»¯ng bÃ i toÃ¡n cáº§n gÃ¡n nhÃ£n chÃ­nh xÃ¡c Ä‘áº¿n tá»«ng pixel nhÆ° semantic segmentation thÃ¬ Ä‘Ã¢y lÃ  viá»‡c vÃ´ cÃ¹ng tá»‘n cÃ´ng sá»©c vÃ  thá»i gian. VÃ  tá»« Ä‘áº¥y cÃ¡c phÆ°Æ¡ng phÃ¡p há»c self/semi - supervised learning lÃªn ngÃ´i. Vá»›i cÃ¡c ká»¹ thuáº­t self-supervised learning thÃ¬ chÃºng ta cÃ³ thá»ƒ train Ä‘á»ƒ model há»c ráº¥t tá»‘t tá»« data chÆ°a Ä‘Æ°á»£c gÃ¡n nhÃ£n vÃ  self - supervised learning Ä‘Æ°á»£c Ã¡p dá»¥ng khÃ¡ phá»• biáº¿n vá»›i 2 hÆ°á»›ng Ä‘i : GANs vÃ  **Contrastive learning**. Trong vÃ i nÄƒm qua, cÃ¡c phÆ°Æ¡ng phÃ¡p self - supervised learning tá»‘t nháº¥t Ä‘Ã£ dáº§n dáº§n chuyá»ƒn tá»« pretext task learning nhÆ°  rotation, colorization vÃ  jigsaw puzzling sang contrastive learning vÃ  xu hÆ°á»›ng nÃ y Ä‘ang ngÃ y cÃ ng Ä‘Æ°á»£c cá»§ng cá»‘ nhá» viá»‡c cÃ³ nhiá»u hÆ¡n nhá»¯ng nghiÃªn cá»©u mang tÃ­nh Ä‘á»™t phÃ¡ trong computer vision vá»›i contrastive learning.

# CÃ¡ch thá»±c hiá»‡n cá»§a Contrastive Representation Learning
![](https://images.viblo.asia/423233af-7d3f-4297-a47a-623e00530607.gif)
> <div align="center"><b>Pipeline thá»±c hiá»‡n cá»§a contrastive learning (nguá»“n: Advancing Self-Supervised and Semi-Supervised Learning with SimCLR)</b></div>

Vá» pipeline thá»±c hiá»‡n cá»§a contrastive learning thÃ¬ mÃ¬nh sáº½ diá»…n Ä‘áº¡t theo hÆ°á»›ng cá»§a bÃ i [A Simple Framework for Contrastive Learning of Visual Representations](https://arxiv.org/pdf/2002.05709v3.pdf), vÃ¬ hÆ°á»›ng nÃ y lÃ  hÆ°á»›ng chÃ­nh cá»§a contrastive learning vÃ  khÃ¡ dá»… hiá»ƒu Ä‘á»“ng thá»i mÃ¬nh cÅ©ng tÃ¬m Ä‘Æ°á»£c 1 implementation khÃ¡ Ä‘Æ¡n giáº£n cá»§a bÃ i bÃ¡o, cÃ¡c báº¡n cÃ³ thá»ƒ xem á»Ÿ cuá»‘i bÃ i viáº¿t ğŸ˜‰. CÃ¡c bÆ°á»›c thá»±c hiá»‡n nhÆ° sau :
1. Data Augmentation
2. Encoder
3. Projection head
4. Loss function


## 1. Data augmentation
Data augmentation lÃ  1 pháº§n khÃ¡ quan trá»ng trong cÃ¡c mÃ´ hÃ¬nh contrastive learning vÃ  gáº§n nhÆ° lÃ  khÃ´ng thá»ƒ thiáº¿u Ä‘Æ°á»£c. Trong bÃ i SoTA SImCLR cá»§a nhÃ³m tÃ¡c giáº£ Ä‘áº¿n tá»« Google Research, ngÆ°á»i ta Ä‘Ã£ dÃ nh háº³n 1 chÆ°Æ¡ng Ä‘á»ƒ nÃ³i vá» data augmentation vÃ  táº§m quan trá»ng cá»§a nÃ³ Ä‘á»‘i vá»›i contrastive learning :`Composition of data augmentation operations is crucial for learning good representations` vÃ  `Contrastive learning needs stronger data augmentation than supervised learning`. Náº¿u data augmentation khÃ´ng Ä‘á»§ phá»©c táº¡p thÃ¬ gradient sáº½ khÃ´ng Ä‘á»§ tá»‘t Ä‘á»ƒ model cÃ³ thá»ƒ há»c cÃ¡c Ä‘áº·c trÆ°ng trong dá»¯ liá»‡u vÃ  ngÆ°á»£c láº¡i. CÃ¡c tÃ¡c giáº£ chá»§ yáº¿u sá»­ dá»¥ng tuáº§n tá»± 3 loáº¡i augment :  random crop, random color distortions, vÃ  random Gaussian blur. VÃ  nháº­n ra ráº±ng random crop vÃ  color distortion lÃ  khÃ¡ quan trá»ng Ä‘á»ƒ model Ä‘áº¡t Ä‘Æ°á»£c káº¿t quáº£ tá»‘t. NhÆ°ng náº¿u chá»‰ sá»­ dá»¥ng riÃªng ráº½ 1 loáº¡i biáº¿n Ä‘á»•i thÃ¬ sáº½ khÃ´ng cho ra káº¿t quáº£ vÆ°á»£t trá»™i, chá»‰ khi káº¿t há»£p 2 loáº¡i biáº¿n Ä‘á»•i vá»›i nhau thÃ¬ káº¿t quáº£ má»›i Ä‘áº¡t tá»›i " state-of-the-art ".

![](https://images.viblo.asia/6c2b69bb-5f01-4250-9c97-4c73d35450c9.png)
<div align="center"><b>Minh há»a cÃ¡c phÃ©p data augmentation (nguá»“n: SimCLR)  </b></div>

## 2. Encoder
ThÃ´ng thÆ°á»ng, encoder lÃ  1 máº¡ng CNN cÃ³ nhiá»‡m vá»¥ map cÃ¡c Ä‘áº§u vÃ o lÃ  áº£nh thÃ nh cÃ¡c embedding vector lÃ m Ä‘áº§u vÃ o cho contrastive loss. Náº¿u máº¡ng encoder trÃ­ch xuáº¥t Ä‘áº·c trÆ°ng khÃ´ng Ä‘á»§ tá»‘t thÃ¬ model sáº½ ráº¥t khÃ³ Ä‘á»ƒ há»c Ä‘Æ°Æ¡c cÃ¡ch phÃ¢n biá»‡t cÃ¡c Ä‘áº·c Ä‘iá»ƒm cÃ³ trong 1 Ä‘iá»ƒm dá»¯ liá»‡u. Äa sá»‘ máº¡ng encoder Ä‘Æ°á»£c sá»­ dá»¥ng hiá»‡n nay lÃ  Resnet vÃ  biáº¿n thá»ƒ cá»§a nÃ³, trong sá»‘ Ä‘Ã³ thÃ¬ cÃ³ láº½ Resnet-50 lÃ  biáº¿n thá»ƒ Ä‘Æ°á»£c cÃ¡c nhÃ  nghiÃªn cá»©u sá»­ dá»¥ng nhiá»u nháº¥t vÃ¬ tÃ­nh cÃ¢n báº±ng giá»¯a kÃ­ch thÆ°á»›c vÃ  hiá»‡u suáº¥t cá»§a mÃ´ hÃ¬nh. 
![](https://images.viblo.asia/9fe83b43-2858-4e06-af71-1dd2e8b77f50.png)
Äá»ƒ model encoder há»c Ä‘Æ°á»£c cÃ¡c Ä‘áº·c Ä‘iá»ƒm cá»§a váº­t thá»ƒ vá»›i hÆ°á»›ng Ä‘i cá»§a self-supervised thÃ¬ pháº£i tráº£i qua pretext task ( sáº½ Ä‘Æ°á»£c nÃ³i á»Ÿ pháº§n sau ) sau khi Ä‘Æ°á»£c huáº¥n luyá»‡n qua vá»›i pretext task thÃ¬ model sáº½ cÃ³ kháº£ nÄƒng trÃ­ch xuáº¥t cÃ¡c Ä‘áº·c trÆ°ng tá»‘t hÆ¡n, Ä‘á»“ng thá»i cÃ³ thá»ƒ kiÃªm thÃªm downstream task. Má»™t sá»‘ model thuá»™c lÄ©nh vá»±c unsupervised learning tháº­m chÃ­ cÃ²n cÃ³ thá»ƒ "outperform" cÃ¡c model SoTA cá»§a supervised learning, tiÃªu biá»ƒu nhÆ° MoCo cá»§a Facebook hay SimCLR cá»§a Google. Äiá»u nÃ y cÃ³ nghÄ©a lÃ  cÃ¡c nhÃ  nghiÃªn cá»©u Ä‘Ã£ dáº§n thu háº¹p Ä‘Æ°á»£c khoáº£ng cÃ¡ch giá»¯a unsupervised vÃ  supervised representation learning trong má»™t sá»‘ task computer vision.
## 3. Projection Head
Sau khi data Ä‘i qua khá»‘i encoder thÃ¬ sáº½ Ä‘Æ°á»£c tÃ­nh toÃ¡n vá»›i projection head Ä‘Æ°á»£c cáº¥u thÃ nh tá»« cÃ¡c fully-connected layer Ä‘á»ƒ cho ra output lÃ  1 embedding vector biá»ƒu diá»…n cho áº£nh á»Ÿ Ä‘áº§u vÃ o. Máº¡ng nÃ y cÃ³ tÃ¡c dá»¥ng khuáº¿ch Ä‘áº¡i cÃ¡c Ä‘áº·c trÆ°ng báº¥t biáº¿n cá»§a dá»¯ liá»‡u tá»« Ä‘Ã³ tá»‘i Ä‘a hÃ³a kháº£ nÄƒng phÃ¢n biá»‡t cÃ¡c phÃ©p biáº¿n Ä‘á»•i tá»« cÃ¹ng 1 bá»©c áº£nh cá»§a máº¡ng.
## 4. Loss function
Má»™t Ä‘iá»u cuá»‘i cÃ¹ng khÃ´ng thá»ƒ thiáº¿u Ä‘Ã³ chÃ­nh lÃ  hÃ m loss, hÃ m loss Ä‘Æ°á»£c sá»­ dá»¥ng á»Ÿ trong bÃ i bÃ¡o SimCLR Ä‘Æ°á»£c gá»i lÃ  NT-Xent (normalized temperature-scaled cross entropy loss). Äáº§u tiÃªn, ngÆ°á»i ta láº¥y máº«u ngáº«u nhiÃªn má»™t minibatch gá»“m $N$ Ä‘iá»ƒm dá»¯ liá»‡u vÃ  Ã¡p dá»¥ng 2 phÃ©p data augmentation khÃ¡c nhau táº¡o thÃ nh $2N$ Ä‘iá»ƒm :
$$\tilde{\mathbf{x}}_i = t(\mathbf{x}),\quad\tilde{\mathbf{x}}_j = t'(\mathbf{x}),\quad t, t' \sim \mathcal{T}$$
Giáº£ sá»­ cÃ³ 1 cáº·p dá»¯ liá»‡u positive, $2(N-1)$ Ä‘iá»ƒm cÃ²n láº¡i Ä‘Æ°á»£c coi lÃ  cÃ¡c Ä‘iá»ƒm negative. XÃ¡c Ä‘á»‹nh vector biá»ƒu diá»…n Ä‘Æ°á»£c táº¡o bá»Ÿi encoder $f(.)$ :
$$\mathbf{h}_i = f(\tilde{\mathbf{x}}_i),\quad \mathbf{h}_j = f(\tilde{\mathbf{x}}_j)$$
HÃ m loss sáº½ Ä‘Æ°á»£c tÃ­nh toÃ¡n dá»±a trÃªn vector biá»ƒu diá»…n Ä‘Æ°á»£c táº¡o bá»Ÿi projection head $g(.)$. Gá»i $\operatorname { sim } ( u , v ) = u ^ { T } v / \| u \| \| v \|$ lÃ  hÃ m cosine similarity cá»§a 2 vector Ä‘Ã£ Ä‘Æ°á»£c chuáº©n hÃ³a $\ell _ { 2 }$ norm $u,v$, khi Ä‘Ã³ hÃ m loss cho 1 cáº·p dá»¯ liá»‡u positive $(i, j)$ Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a nhÆ° sau :
$$\begin{aligned}
\mathbf{z}_i &= g(\mathbf{h}_i),\quad
\mathbf{z}_j = g(\mathbf{h}_j) \\
\mathcal{L}_\text{SimCLR}^{(i,j)} &= - \log\frac{\exp(\text{sim}(\mathbf{z}_i, \mathbf{z}_j) / \tau)}{\sum_{k=1}^{2N} \mathbb{1}_{[k \neq i]} \exp(\text{sim}(\mathbf{z}_i, \mathbf{z}_k) / \tau)}
\end{aligned}$$

Trong Ä‘Ã³ $\mathbb{1}_{[k \neq i]}=
\begin{cases}
1 &k\neq i \\
0  &\text{cÃ²n láº¡i}
\end{cases}$


## 5. Pretext task
Pretext task chÃ­nh lÃ  self-supervised task vá»›i nhiá»‡m vá»¥ huáº¥n luyá»‡n Ä‘á»ƒ model há»c Ä‘Æ°á»£c cÃ¡c Ä‘áº·c trÆ°ng vá»›i dá»¯ liá»‡u chÆ°a Ä‘Æ°á»£c Ä‘Ã¡nh nhÃ£n, vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p thÃ´ng thÆ°á»ng thÃ¬ cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u sáº½ Ä‘Æ°á»£c gÃ¡n nhÃ£n giáº£ báº±ng cÃ¡ch: biáº¿n Ä‘á»•i áº£nh gá»‘c dá»±a trÃªn cÃ¡c phÃ©p data augmentation (augmented data) lÃ m input cho model vÃ  output sáº½ lÃ  áº£nh gá»‘c Ä‘áº¥y. Tá»©c lÃ  chÃºng ta sáº½ Ä‘á»ƒ model há»c Ä‘Æ°á»£c cÃ¡ch khÃ´i phá»¥c áº£nh gá»‘c dá»±a trÃªn cÃ¡c bá»©c áº£nh Ä‘Ã£ bá»‹ biáº¿n Ä‘á»•i. Má»¥c tiÃªu cá»§a pretext task thÃ´ng thÆ°á»ng khÃ¡c pretext task cá»§a contrastive learning - contrastive prediction task á»Ÿ chá»— pretext task sáº½ cá»‘ gáº¯ng khÃ´i phá»¥c láº¡i áº£nh cÅ© tá»« áº£nh Ä‘Ã£ biáº¿n Ä‘á»•i, cÃ²n contrastive prediction task sáº½ cá»‘ gáº¯ng há»c nhá»¯ng Ä‘áº·c trÆ°ng báº¥t biáº¿n cá»§a áº£nh gá»‘c tá»« áº£nh Ä‘Ã£ biáº¿n Ä‘á»•i trong khi váº«n phÃ¢n biá»‡t Ä‘Æ°á»£c vá»›i cÃ¡c áº£nh negative khÃ¡c trong batch. Model Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n qua pretext task cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng á»Ÿ cÃ¡c nhiá»‡m vá»¥ khÃ¡c nhÆ° fine-tune vá»›i cÃ¡c bá»™ dataset Ä‘Ã£ gÃ¡n nhÃ¡n Ä‘á»ƒ thá»±c hiá»‡n cÃ¡c downstream task. CÃ³ 4 loáº¡i pretext task thÆ°á»ng Ä‘Æ°á»£c dÃ¹ng Ä‘Ã³ lÃ  : color transformation, geometric transformation, context-based tasks, vÃ  cross-modal-based tasks.
## 6. Downstream task
Downstream task nÃ³i nÃ´m na chÃ­nh lÃ  viá»‡c chÃºng ta táº­n dá»¥ng cÃ¡c kiáº¿n thá»©c model Ä‘Ã£ Ä‘Æ°á»£c há»c qua pretext task Ä‘á»ƒ sá»­ dá»¥ng chÃºng vÃ o má»™t má»¥c Ä‘Ã­ch cá»¥ thá»ƒ nÃ o Ä‘Ã³. Model Ä‘Ã£ Ä‘Æ°á»£c pretrain á»Ÿ pretext task chá»©a nhá»¯ng trá»ng sá»‘ giÃ u thÃ´ng tin vÃ  hoÃ n toÃ n cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng vá»›i nhá»¯ng bÃ i toÃ¡n thÃ­ch há»£p khÃ¡c thÃ´ng qua ká»¹ thuáº­t fine-tune model hay tá»•ng thá»ƒ Ä‘Æ°á»£c gá»i lÃ  transfer learning. Káº¿t quáº£ cá»§a transfer learning vá»›i nhá»¯ng task cÃ³ level cao hÆ¡n kia sáº½ miÃªu táº£ kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a cá»§a cÃ¡c Ä‘áº·c trÆ°ng Ä‘Ã£ Ä‘Æ°á»£c há»c. Má»™t sá»‘ downstream task thÆ°á»ng Ä‘Æ°á»£c biáº¿t Ä‘áº¿n lÃ  clasification, detection, segmentation, future prediction. Viá»‡c sá»­ dá»¥ng má»™t hay nhiá»u trong cÃ¡c task Ä‘Ã£ Ä‘Æ°á»£c liá»‡t kÃª cÅ©ng cÃ³ thá»ƒ coi lÃ  Ä‘ang kiá»ƒm thá»­ vÃ  Ä‘Ã¡nh giÃ¡ hiá»‡u quáº£ cá»§a model.
Äá»ƒ Ä‘Ã¡nh giÃ¡ cÃ¡c Ä‘áº·c trÆ°ng Ä‘Ã£ Ä‘Æ°á»£c há»c cá»§a hÆ°á»›ng self-supervised cho cÃ¡c downstream task, ngÆ°á»i ta thÆ°á»ng sá»­ dá»¥ng cÃ¡c phÆ°Æ¡ng phÃ¡p nhÆ°  kernel visualization, feature map visualization, vÃ  nearest-neighbor-based. Nhá»¯ng phÆ°Æ¡ng phÃ¡p nÃ y cÅ©ng há»— trá»£ viá»‡c phÃ¢n tÃ­ch hiá»‡u quáº£ cá»§a pretext task trong training model

## 7. CÃ¡c dáº¡ng kiáº¿n trÃºc khÃ¡c
![](https://images.viblo.asia/8f918fe7-53ac-4c2d-b868-65d884dc5bd0.png)
Coi 1 áº£nh anchor Ä‘Ã£ Ä‘Æ°á»£c encode lÃ  query $q$ vÃ  1 táº­p cÃ¡c sample Ä‘Ã£ Ä‘Æ°á»£c encode lÃ  {${k_0, k_1, k_2,...}$}, táº­p nÃ y ta coi lÃ  key cá»§a 1 bá»™ dictionary. Giáº£ sá»­ trong dictionary cÃ³ khÃ³a $k_+$ trÃ¹ng vá»›i query $q$. Váº­y thÃ¬ hÃ m contrastive loss sáº½ lÃ  hÃ m tÃ­nh toÃ¡n ra giÃ¡ trá»‹ tháº¥p náº¿u $q$ giá»‘ng vá»›i positive key $k_+$ vÃ  khÃ¡c vá»›i táº¥t cáº£ key cÃ²n láº¡i (coi nhá»¯ng key nÃ y lÃ  negative vá»›i $q$). NhÆ° váº­y lÃ  ngÆ°á»i ta Ä‘Ã£ mÃ´ hÃ¬nh hÃ³a contrastive learning thÃ nh bÃ i toÃ¡n Ä‘i tÃ¬m key trong 1 bá»™ dictionary gá»“m ráº¥t nhiá»u key khÃ¡c nhau Ä‘Æ°á»£c lÆ°u trá»¯ dÆ°á»›i dáº¡ng hÃ ng Ä‘á»£i vÃ  kÃ­ch thÆ°á»›c cá»§a dictionary pháº£i lá»›n hÆ¡n mini-batch size ráº¥t nhiá»u.
### 7.1 Memory bank
Vá»›i SimCLR, ngÆ°á»i ta Ä‘Ã£ chá»©ng minh Ä‘Æ°á»£c training vá»›i batch size cÃ ng lá»›n thÃ¬ káº¿t quáº£ Ä‘áº¡t Ä‘Æ°á»£c cÃ ng tá»‘t vÃ  cÃ¡c nhÃ  nghiÃªn cá»©u cá»§a google Ä‘Ã£ cho ra káº¿t quáº£ SoTA vá»›i batch size 4096 trong 100 epoch. CÆ¡ sá»Ÿ cho káº¿t luáº­n nÃ y náº±m á»Ÿ sá»‘ lÆ°á»£ng negative sample cÃ³ trong 1 batch vÃ  hiá»ƒn nhiÃªn lÃ  batch size cÃ ng lá»›n thÃ¬ sá»‘ negative sample cÃ ng nhiá»u. NhÆ°ng Ä‘iá»u nÃ y láº¡i cÃ³ 1 vÃ i báº¥t lá»£i Ä‘Ã³ lÃ  quÃ¡ trÃ¬nh training khÃ¡ tá»‘n tÃ i nguyÃªn vÃ  batch size lá»›n cÅ©ng cÃ³ thá»ƒ áº£nh hÆ°á»Ÿng Ä‘áº¿n hÃ m tá»‘i Æ°u hÃ³a, tá»« Ä‘Ã³ ngÆ°á»i ta Ä‘Æ°a ra 1 giáº£i phÃ¡p kháº¯c phá»¥c Ä‘Ã³ lÃ  duy trÃ¬ má»™t bá»™ dictionary gá»i lÃ  Memory Bank. 
    Má»¥c tiÃªu chÃ­nh cá»§a memory bank lÃ  tÃ­ch lÅ©y sá»‘ lÆ°á»£ng lá»›n cÃ¡c Ä‘áº·c trÆ°ng biá»ƒu diá»…n cá»§a negative sample trong suá»‘t quÃ¡ trÃ¬nh training thÃ´ng qua viá»‡c sá»­ dá»¥ng má»™t bá»™ dictionary Ä‘á»ƒ lÆ°u trá»¯ vÃ  Ä‘á»“ng thá»i update liÃªn tá»¥c cÃ¡c embedding vector má»›i nháº¥t. Tuy nhiÃªn, viá»‡c duy trÃ¬ memory bank cÅ©ng gáº·p 1 sá»‘ báº¥t lá»£i, má»™t trong sá»‘ Ä‘Ã³ chÃ­nh lÃ  khá»‘i lÆ°á»£ng tÃ­nh toÃ¡n khÃ¡ lá»›n khi pháº£i update cÃ¡c vector biá»ƒu diá»…n trong memory bank liÃªn tá»¥c bá»Ÿi vÃ¬ cÃ¡c vector biá»ƒu diá»…n nÃ y lá»—i thá»i khÃ¡ nhanh. 

### 7.2 Momentum encoder
Sá»­ dá»¥ng hÃ ng Ä‘á»£i khÃ´ng chá»‰ khiáº¿n dictionary lá»›n mÃ  cÃ²n khiáº¿n key encoder khÃ³ cáº­p nháº­t thÃ´ng qua back-propagation (gradient sáº½ pháº£i Ä‘Æ°á»£c lan truyá»n Ä‘áº¿n táº¥t cáº£ cÃ¡c sample trong hÃ ng Ä‘á»£i). Äá»ƒ giáº£i quyáº¿t váº¥n Ä‘á» Ä‘Ã£ nÃªu, ngÆ°á»i ta Ä‘á» xuáº¥t copy luÃ´n gradient tá»« query encoder $f_q$ cho key encoder $f_k$, nhÆ°ng hÆ°á»›ng Ä‘i nÃ y láº¡i cho káº¿t quáº£ khÃ¡ tá»‡, nguyÃªn nhÃ¢n cÃ³ thá»ƒ lÃ  do viá»‡c thay Ä‘á»•i gradient quÃ¡ nhanh Ä‘Ã£ lÃ m giáº£m tÃ­nh nháº¥t quÃ¡n cá»§a key encoder. VÃ¬ tháº¿ ngÆ°á»i ta Ä‘Ã£ Ä‘á» xuáº¥t phÆ°Æ¡ng phÃ¡p momentum update Ä‘á»ƒ giáº£i quyáº¿t váº¥n Ä‘á» nÃ y: 
Coi cÃ¡c tham sá»‘ cá»§a $f_k$ lÃ  $\theta_k$ vÃ  cá»§a $f_q$ lÃ  $\theta_q$. $\theta_k$ sáº½ Ä‘Æ°á»£c update vá»›i cÃ´ng thá»©c sau: 
$$\theta_{k}\leftarrow m\theta_{k}+(1-m)\theta_{q}.$$ 
 Vá»›i $m \in [ 0,1 )$ lÃ  há»‡ sá»‘ momentum, vÃ  chá»‰ cÃ³ $\theta_q$ lÃ  Ä‘Æ°á»£c cáº­p nháº­t thÃ´ng qua back-propagation. Momentum update sáº½ giÃºp $\theta_k$ Ä‘Æ°á»£c cáº­p nháº­t trÆ¡n tru hÆ¡n $\theta_q$. Lá»£i tháº¿ cá»§a viá»‡c sá»­ dá»¥ng phÆ°Æ¡ng phÃ¡p nÃ y lÃ  khÃ´ng cáº§n pháº£i traing 2 model riÃªng biá»‡t, Ä‘á»“ng thá»i cÅ©ng khÃ´ng cáº§n giá»¯ láº¡i memory bank trÃ¡nh khá»‘i lÆ°á»£ng tÃ­nh toÃ¡n lá»›n vÃ  tá»‘n nhiá»u tÃ i nguyÃªn.

### 7.3 Clustering Feature Representations
Hai kiáº¿n trÃºc ká»ƒ trÃªn Ä‘á»u táº­p trung vÃ o hÆ°á»›ng so sÃ¡nh cÃ¡c sample sá»­ dá»¥ng similarity metric vÃ  cá»‘ gáº¯ng kÃ©o cÃ¡c sample giá»‘ng nhau láº¡i gáº§n, Ä‘áº©y cÃ¡c sample khÃ¡c nhau ra xa. NhÆ°ng kiáº¿n trÃºc á»Ÿ pháº§n nÃ y láº¡i Ä‘i ngÆ°á»£c vá»›i 2 hÆ°á»›ng ká»ƒ trÃªn, ngÆ°á»i ta Ä‘i theo hÆ°á»›ng end-to-end vá»›i 2 encoders chia sáº» trá»ng sá»‘ vá»›i nhau, nhÆ°ng thay vÃ¬ Ä‘i theo hÆ°á»›ng contrastive thÃ¬ há» sá»­ dá»¥ng thuáº­t toÃ¡n phÃ¢n cá»¥m Ä‘á»ƒ nhÃ³m cÃ¡c Ä‘áº·c trÆ°ng giá»‘ng nhau láº¡i. Má»¥c Ä‘Ã­ch cá»§a viá»‡c nÃ y khÃ´ng chá»‰ lÃ  Ä‘á»ƒ kÃ©o 1 cáº·p sample láº¡i vá»›i nhau mÃ  cÃ²n Ä‘áº£m báº£o Ä‘Æ°á»£c viá»‡c táº¥t cáº£ cÃ¡c sample tÆ°Æ¡ng tá»± vá»›i nhau Ä‘Æ°á»£c phÃ¢n vÃ o 1 cá»¥m vá»›i nhau. VÃ­ dá»¥ trong khÃ´ng gian embedding cá»§a 1 táº­p Ä‘a dáº¡ng cÃ¡c loáº¡i áº£nh, cÃ¡c Ä‘áº·c trÆ°ng cá»§a mÃ¨o sáº½ Ä‘uá»£c kÃ©o láº¡i gáº§n vá»›i cá»§a chÃ³ (cÃ¹ng lÃ  con váº­t) vÃ  Ä‘áº©y ra xa vá»›i cá»§a nhÃ . Má»™t trong nhá»¯ng nghiÃªn cá»©u Ä‘Æ°á»£c Ä‘á» xuáº¥t gáº§n Ä‘Ã¢y á»©ng dá»¥ng phÆ°Æ¡ng phÃ¡p phÃ¢n cá»¥m nÃ³i trÃªn chÃ­nh lÃ  [SwAV](https://proceedings.neurips.cc//paper/2020/file/70feb62b69f16e0238f741fab228fec2-Paper.pdf):
![](https://images.viblo.asia/a4b8709c-0516-4601-814a-9651ad1beb6c.png)
<div align="center"><b>Contrastive learning (trÃ¡i) vs SwAV (pháº£i) </b></div>

Vá»›i contrastive learning thÃ´ng thÆ°á»ng, má»—i sample sáº½ Ä‘Æ°á»£c coi lÃ  1 class riÃªng trong dataset, Ä‘iá»u nÃ y trá»Ÿ nÃªn mÃ¢u thuáº«n khi cÃ³ má»™t áº£nh thuá»™c vá» negative sample nhÆ°ng láº¡i cÃ¹ng lá»›p vá»›i áº£nh gá»‘c, trÆ°á»ng há»£p nÃ y Ä‘Æ°á»£c coi lÃ  **false negative**. VÃ­ dá»¥ nhÆ° chÃºng ta cÃ³ áº£nh 1 con mÃ¨o lÃ m gá»‘c vÃ  cÃ¡c áº£nh khÃ¡c trong batch sáº½ Ä‘Æ°á»£c gá»i lÃ  negative, váº¥n Ä‘á» sáº½ xáº£y ra khi trong sá»‘ Ä‘áº¥y cÃ³ áº£nh cá»§a 1 hay vÃ i con mÃ¨o ná»¯a. Trong trÆ°á»ng há»£p nÃ y, model báº¯t buá»™c pháº£i há»c ráº±ng 2 bá»©c áº£nh cá»§a 2 con mÃ¨o lÃ  khÃ¡c nhau trong khi chÃºng láº¡i thuá»™c vá» cÃ¹ng 1 class vÃ  cÃ³ thá»ƒ gÃ¢y ra "degradation" - suy thoÃ¡i Ä‘á»‘i vá»›i cháº¥t lÆ°á»£ng vector biá»ƒu diá»…n. Váº¥n Ä‘á» nÃ y Ä‘Æ°á»£c giáº£i quyáº¿t ngáº§m báº±ng cÃ¡ch tiáº¿p cáº­n dá»±a trÃªn phÃ¢n cá»¥m.

# Supervised Contrastive Learning
Trong paper â€œ [Supervised Contrastive Learning](https://arxiv.org/abs/2004.11362)â€ Ä‘Æ°á»£c trÃ¬nh bÃ y táº¡i NeurIPS 2020, cÃ¡c nhÃ  nghiÃªn cá»©u Ä‘Ã£ Ä‘á» xuáº¥t má»™t hÃ m loss gá»i lÃ  **SupCon** -  Supervised Contrastive Loss, nháº±m thu háº¹p khoáº£ng cÃ¡ch vÃ´ hÃ¬nh giá»¯a self-supervised learning vÃ  fully-supervised learning vÃ  cho phÃ©p contrastive learning cÃ³ thá»ƒ Ä‘Æ°á»£c á»©ng dá»¥ng vá»›i cÃ¡c bÃ i toÃ¡n supervised learning. Váº«n giá»¯ nguyÃªn Ã½ tÆ°á»Ÿng cá»§a contrastive learning, hÃ m SupCon sáº½ cá»‘ gáº¯ng táº­n dá»¥ng dá»¯ liá»‡u Ä‘Æ°á»£c gÃ¡n nhÃ£n Ä‘á»ƒ kÃ©o cÃ¡c embedding vector Ä‘Ã£ Ä‘Æ°á»£c chuáº©n hÃ³a cá»§a cÃ¹ng 1 class láº¡i gáº§n nhau vÃ  ngÆ°á»£c láº¡i Ä‘á»‘i vá»›i nhá»¯ng embedding vector khÃ¡c class. Viá»‡c cÃ³ thÃªm dá»¯ liá»‡u Ä‘Æ°á»£c gÃ¡n nhÃ£n sáº½ lÃ m Ä‘Æ¡n giáº£n hÃ³a quÃ¡ trÃ¬nh chá»n positive sample vÃ  trÃ¡nh Ä‘Æ°á»£c cÃ¡c trÆ°á»ng há»£p false negative 
![](https://images.viblo.asia/17f64278-09bc-48c5-b11b-77d2bf0d4ab7.png)

> Cross-entropy, self-supervised contrastive loss vÃ  supervised contrastive loss. **TrÃ¡i**: Cross-entropy loss sá»­ udngj nhÃ£n lÃ  hÃ m softmax loss Ä‘á»ƒ huáº¥n luyá»‡n 1 bá»™ classifier. **Giá»¯a**: Self-supervised contrastive loss sá»­ dá»¥ng hÃ m contrastive loss vÃ  data augmentation Ä‘á»ƒ há»c cÃ¡c Ä‘áº·c trÆ°ng biá»ƒu diá»…n. **Pháº£i**: HÃ m supervised contrastive loss cÅ©ng há»c cÃ¡c Ä‘áº·c trÆ°ng biá»ƒu diá»…n thÃ´ng qua contrastive loss nhÆ°ng cÃ³ sá»­ dá»¥ng thÃªm thÃ´ng tin tá»« nhÃ£n cá»§a dá»¯ liá»‡u Ä‘á»ƒ láº¥y máº«u positive (cÃ¡c bá»©c áº£nh cÃ¹ng lá»›p vá»›i anchor) ngoÃ i dá»¯ liá»‡u augmentation cá»§a 1 bá»©c áº£nh.
## Supervised contrastive loss
Giáº£ sá»­ cÃ³ 1 táº­p $n$ cáº·p dá»¯ liá»‡u (áº£nh, nhÃ£n) training $\{\mathbf{x}_i, y_i\}_{i=1}^n$, tá»« Ä‘Ã³ ta cÃ³ thÃªm $n$ cáº·p dá»¯ liá»‡u thÃ´ng qua 2 phÃ©p augmentation vá»›i má»—i máº«u Ä‘Ã£ cho $\{\tilde{\mathbf{x}}_i, \tilde{y}_i\}_{i=1}^{2n}$. HÃ m supervised contrastive loss $\mathcal{L}_\text{supcon}$ sáº½ táº­n dá»¥ng nhiá»u positive vÃ  negative sample: 
$$\mathcal{L}_\text{supcon} = - \sum_{i=1}^{2n} \frac{1}{2 \vert N_i \vert - 1} \sum_{j \in N(y_i), j \neq i} \log \frac{\exp(\mathbf{z}_i \cdot \mathbf{z}_j / \tau)}{\sum_{k \in I, k \neq i}\exp({\mathbf{z}_i \cdot \mathbf{z}_k / \tau})}$$
Trong Ä‘Ã³ $\mathbf{z}_k=P(E(\tilde{\mathbf{x}_k}))$, $E(.)$ lÃ  1 máº¡ng encoder, $P(.)$ lÃ  projection head. $N_i= \{j \in I: \tilde{y}_j = \tilde{y}_i \}$ chá»©a 1 táº­p cÃ¡c chá»‰ sá»‘ cá»§a cÃ¡c máº«u vá»›i nhÃ£n $y_i$, cÃ ng cÃ³ nhiá»u positive sample trong táº­p $N_i$ thÃ¬ káº¿t quáº£ cÃ ng Ä‘Æ°á»£c cáº£i thiá»‡n



## Supervised contrastive learning framework
Vá» cÆ¡ báº£n thÃ¬ phÆ°Æ¡ng phÃ¡p nÃ y cÃ³ cáº¥u trÃºc tÆ°Æ¡ng tá»± vá»›i phÆ°Æ¡ng phÃ¡p Ä‘Æ°á»£c sá»­ dá»¥ng trong self-supervised contrastive learning nhÆ°ng cÃ³ thÃªm Ä‘iá»u chá»‰nh cho tÃ¡c vá»¥ supervised classification. Vá»›i má»™t batch dá»¯ liá»‡u, chÃºng ta sáº½ tiáº¿n hÃ nh Ã¡p dá»¥ng data augmentation 2 láº§n Ä‘á»ƒ cÃ³ 2 báº£n copy cá»§a má»—i sample trong batch. Cáº£ 2 báº£n sáº½ Ä‘Æ°á»£c forward qua encoder Ä‘á»ƒ láº¥y embedding vector vÃ  sau Ä‘Ã³ Ä‘Æ°á»£c chuáº©n hÃ³a $L2$ rá»“i Ä‘Æ°a qua projection head. Supervised contrastive loss sáº½ Ä‘Æ°á»£c tÃ­nh toÃ¡n vá»›i vector Ä‘Ã£ Ä‘Æ°á»£c chuáº©n hÃ³a á»Ÿ Ä‘áº§u ra cá»§a projection head. CÃ¡c positive sample sáº½ lÃ  phiÃªn báº£n augmentation cá»§a anchor cÃ¹ng vá»›i sample cÃ¹ng class vá»›i anchor trong cáº£ 2 batch vÃ  negative sample chÃ­nh lÃ  pháº§n cÃ²n láº¡i. 
# Lá»i káº¿t
Máº·c dÃ¹ thoáº¡t nhÃ¬n, contrastive learning cÃ³ váº» dá»… dÃ ng nhÆ°ng viá»‡c khiáº¿n chÃºng há»c cÃ¡ch biá»ƒu diá»…n Ä‘áº§u vÃ o cÃ³ Ã½ nghÄ©a lÃ  Ä‘iá»u khÃ¡ khÃ³ khÄƒn. HÆ°á»›ng Ä‘i nÃ y cÅ©ng cÃ²n ráº¥t nhiá»u tiá»m nÄƒng Ä‘á»ƒ khai thÃ¡c, hi vá»ng chÃºng ta sáº½ tháº¥y nhiá»u bÃ i nghiÃªn cá»©u mang káº¿t quáº£ Ä‘á»™t phÃ¡ hÆ¡n ná»¯a trong tÆ°Æ¡ng lai. BÃ i viáº¿t Ä‘áº¿n Ä‘Ã¢y cÅ©ng Ä‘Ã£ dÃ i, trong quÃ¡ trÃ¬nh Ä‘á»c cÃ³ tháº¥y sai sÃ³t mong má»i ngÆ°á»i tÃ­ch cá»±c gÃ³p Ã½ Ä‘á»ƒ mÃ¬nh cáº£i thiá»‡n hÆ¡n á»Ÿ cÃ¡c bÃ i viáº¿t sau. Náº¿u tháº¥y bÃ i há»¯u Ã­ch cÃ³ thá»ƒ táº·ng mÃ¬nh má»™t Upvote cho mÃ¬nh cÃ³ thÃªm Ä‘á»™ng lá»±c Ä‘á»ƒ chuáº©n bá»‹ cho má»™t sá»‘ bÃ i viáº¿t khÃ¡c sáº¯p tá»›i. 
Cáº£m Æ¡n má»i ngÆ°á»i Ä‘Ã£ á»§ng há»™ !

# Code
Pháº§n code chÃ­nh thá»‘ng cá»§a bÃ i bÃ¡o SimCLR cÃ¡c báº¡n cÃ³ thá»ƒ tham kháº£o á»Ÿ Ä‘Ã¢y nhÃ© : [Official work](https://github.com/google-research/simclr)

CÃ²n Ä‘Ã¢y lÃ  má»™t phiÃªn báº£n implementation Ä‘Æ¡n giáº£n hÆ¡n cá»§a nÃ³: [simple implementation](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial17/SimCLR.html)



# Reference
1. [A Simple Framework for Contrastive Learning of Visual Representations](https://arxiv.org/pdf/2002.05709v3.pdf)
2. [Supervised Contrastive Learning](https://arxiv.org/pdf/2004.11362.pdf)
3. [Contrastive Representation Learning
](https://lilianweng.github.io/posts/2021-05-31-contrastive/#supervised-contrastive-learning)
4. [Extending Contrastive Learning to the Supervised Setting](https://ai.googleblog.com/2021/06/extending-contrastive-learning-to.html)
5. [Unsupervised Learning of Visual Features
by Contrasting Cluster Assignments](https://proceedings.neurips.cc//paper/2020/file/70feb62b69f16e0238f741fab228fec2-Paper.pdf)
6. [Momentum Contrast for Unsupervised Visual Representation Learning](https://openaccess.thecvf.com/content_CVPR_2020/papers/He_Momentum_Contrast_for_Unsupervised_Visual_Representation_Learning_CVPR_2020_paper.pdf)
7. [A Survey on Contrastive Self-supervised Learning
](https://arxiv.org/abs/2011.00362)
8. [Understanding Contrastive Learning
](https://towardsdatascience.com/understanding-contrastive-learning-d5b19fd96607)