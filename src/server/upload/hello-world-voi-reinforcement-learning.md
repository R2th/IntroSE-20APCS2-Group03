Xin ch√†o c√°c b·∫°n. Ti·∫øp n·ªëi series v·ªÅ **[Reinforcement Learning (RL)](https://viblo.asia/s/reinforcement-learning-yEZkg8kyZQ0)**, h√¥m nay m√¨nh xin gi·ªõi thi·ªáu m·ªôt v√≠ d·ª• ƒë∆°n gi·∫£n c√≥ th·ªÉ coi nh∆∞ l√† *"Hello world"* c·ªßa RL. 

# 1. Gi·ªõi thi·ªáu
Trong b√†i tr∆∞·ªõc [ƒê√¥i ƒëi·ªÅu c∆° b·∫£n v·ªÅ h·ªçc tƒÉng c∆∞·ªùng ](https://viblo.asia/p/doi-dieu-co-ban-ve-hoc-tang-cuong-ORNZqAYrZ0n) m√¨nh ƒë√£ gi·ªõi thi·ªáu m·ªôt s·ªë kh√°i ni·ªám c·ªßa RL. Trong ƒë√≥ m√¥i tr∆∞·ªùng v√† c√°c tr·∫°ng th√°i, ph·∫ßn th∆∞·ªüng l√† nh·ªØng y·∫øu t·ªë quan tr·ªçng. ƒê·ªÉ cho c√≥ th·ªÉ h·ªçc ƒë∆∞·ª£c nh·ªØng chi·∫øn l∆∞·ª£c t·ªëi ∆∞u ho·∫∑c t√¨m ƒë∆∞·ª£c chu·ªói h√†nh ƒë·ªông t·ªëi ∆∞u, t√°c nh√¢n ph·∫£i th·ª≠ nhi·ªÅu l·∫ßn v√† h·ªçc t·ª´ c√°c l·∫ßn th·ª≠ ƒë√≥, v√† r·∫•t kh√≥ ƒë·ªÉ cho t√°c nh√¢n c√≥ th·ªÉ h·ªçc ƒë∆∞·ª£c trong th·∫ø gi·ªõi th·ª±c. Do ƒë√≥ vi·ªác m√¥ ph·ªèng l·∫°i m√¥i tr∆∞·ªùng l√† r·∫•t quan tr·ªçng. R·∫•t may [OpenAI](https://openai.com/) ƒë√£ c√≥ m·ªôt opensource mang t√™n [Gym](https://www.gymlibrary.ml/content/api/) ƒë·ªÉ m√¥ ph·ªèng l·∫°i m·ªôt s·ªë m√¥i tr∆∞·ªùng ƒë∆°n gi·∫£n. Trong b√†i n√†y m√¨nh s·∫Ω s·ª≠ d·ª•ng **Q-Learning** ƒë·ªÉ gi·∫£i quy·∫øt game *mountain-car* trong th∆∞ vi·ªán *gym*.

V·ªõi mountain-car, m·ª•c ti√™u c·ªßa ch√∫ng ta l√† ƒëi·ªÅu khi·ªÉn xe √¥ t√¥ ƒë·∫øn l√° c·ªù:
![image.png](https://images.viblo.asia/220fc837-636e-4f20-a467-2e3b027c67e2.png)
Ta x√©t c√°c thu·ªôc t√≠nh c·ªßa game n√†y:
1. T·∫≠p tr·∫°ng th√°i $S = R^{2}$ bao g·ªìm v·ªã tr√≠ c·ªßa xe tr√™n tr·ª•c ngang v√† v·∫≠n t·ªëc c·ªßa xe, chi·ªÅu d∆∞∆°ng t·ª´ tr√°i sang ph·∫£i. C·ª• th·ªÉ h∆°n:
$$
S = \left \{ (x ,v ) | x,v \in R; -1.2 \leq x \leq 0.6; -0.07 \leq v \leq 0.07 \right \}
$$
2. T·∫≠p h√†nh ƒë·ªông c·ªßa xe:
$$
\mathcal{A} = \left \{0; 1; 2 \right \}
$$
V·ªõi $a \in \mathcal{A}$:
* a = 0: tƒÉng t·ªëc sang tr√°i 
* a = 1: Kh√¥ng tƒÉng t·ªëc
* a = 2: TƒÉng t·ªëc sang ph·∫£i
3. Ph·∫ßn th∆∞·ªüng: V·ªõi m·ªói h√†nh ƒë·ªông, n·∫øu ch∆∞a t·ªõi ƒë√≠ch $(x < 0.5)$ nh·∫≠n ƒë∆∞·ª£c ph·∫ßn th∆∞·ªüng -1, n·∫øu kh√¥ng th√¨ nh·∫≠n ƒë∆∞·ª£c 0 v√† k·∫øt th√∫c game
4. Tr·∫°ng th√°i kh·ªüi t·∫°o $x_0 \in [-0.6, -0.4]$ v√† $v_0=0$
5. Tr·∫°ng th√°i k·∫øt th√∫c: ƒê·∫øn ƒë∆∞·ª£c c·ªù t·∫°i $x=0.5$ ho·∫∑c t·ªïng ph·∫ßn th∆∞·ªüng ƒë·∫°t ƒë·∫øn -200


Code import th∆∞ vi·ªán v√† kh·ªüi t·∫°o m√¥i tr∆∞·ªùng:
```python
# import
import gym
import time
import numpy as np
import IPython.display
from tqdm.notebook import tqdm

# create environment
env = gym.make('MountainCar-v0')
```

H√£y c√πng ch·∫°y th·ª≠ 1 v√≠ d·ª•:
```python
s = env.reset()    # reset to starting State
action_list = list(range(env.action_space.n))
while True:
    a = np.random.choice(action_list)   # random an action
    s, r, done, _ = env.step(a)         # perform action, get state, reward, is terminated
    if done:
        break
    env.render()
    time.sleep(0.05)
```

# 2. Code th√¥i
### 2.1 X√¢y d·ª±ng Q-function
H√£y c√πng x√©t c√¥ng th·ª©c cu·ªëi c√πng c·ªßa b√†i vi·∫øt [tr∆∞·ªõc](https://viblo.asia/p/doi-dieu-co-ban-ve-hoc-tang-cuong-ORNZqAYrZ0n):

$$
Q(s,a) = (1-\alpha) Q(s,a) + \alpha E_{s \sim P(s' | s , a)} \left (R(s, a) + \gamma\max _ {a \in \mathcal{A}}Q^*(s,a) \right )
$$

Trong tr∆∞·ªùng h·ª£p ch√∫ng ta ƒëang x√©t, do th∆∞ vi·ªán *gym* m√¥ ph·ªèng l·∫°i hi·ªán t∆∞·ª£ng v·∫≠t l√Ω t·ª´ th·ª±c t·∫ø n√™n t·∫°i m·ªói tr·∫°ng th√°i $s$, ta th·ª±c hi·ªán h√†nh ƒë·ªông $a$ s·∫Ω ƒë∆∞a ta ƒë·∫øn tr·∫°ng th√°i $s'$ x√°c ƒë·ªãnh. Do ƒë√≥ h√†m $Q$ trong tr∆∞·ªùng h·ª£p n√†y ƒë∆∞·ª£c vi·∫øt l·∫°i:

$$
\tag{1}
Q(s,a) = (1-\alpha) Q(s,a) + \alpha \left (R(s, a) + \gamma\max _ {a \in \mathcal{A}}Q^*(s,a) \right )
$$

Trong game n√†y,kh√¥ng gian tr·∫°ng th√°i c√≥ 2 chi·ªÅu ($x$ v√† $v$), t·∫≠p h√†nh ƒë·ªông ch·ªâ c√≥ 3 h√†nh ƒë·ªông, do ƒë√≥ ch√∫ng ta c√≥ th·ªÉ m√¥ h√¨nh h√†m $Q$ b·∫±ng m·ªôt b·∫£ng  $Q-table$, v·ªõi c√°c c·ªôt t∆∞∆°ng ·ª©ng v·ªõi  c√°c gi√° tr·ªã kh√°c nhau c·ªßa v·∫≠n t·ªëc $v$, c√°c c·ªôt t∆∞∆°ng ·ª©ng v·ªõi  c√°c gi√° tr·ªã kh√°c nhau c·ªßa t·ªça ƒë·ªô $x$. Tuy nhi√™n $x,v \in R$ m√† s·ªë c·ªôt v√† h√†ng ph·∫£i h·ªØu h·∫°n n√™n ta c·∫ßn *l∆∞·ª£ng t·ª≠ h√≥a tr·∫°ng th√°i* c·ªßa xe th√†nh $N$ tr·∫°ng th√°i r·ªùi r·∫°c kh√°c nhau, nghƒ©a l√† chia chia c√°c gi√° tr·ªã c·ªßa t·ªça ƒë·ªô v√† v·∫≠n t·ªëc th√†nh $N$ ƒëo·∫°n b·∫±ng nhau, tr·∫°ng th√°i c·ªßa xe s·∫Ω l√† ch·ªâ s·ªë c·ªßa c√°c ƒëo·∫°n t∆∞∆°ng ·ª©ng. 

Code l∆∞·ª£ng t·ª≠ h√≥a cho 1 chi·ªÅu c·ªßa tr·∫°ng th√°i
```python
class NBinDiscretizer:
    def __init__(self, min_val, max_val, nbins):
        self.min_val = min_val
        self.max_val = max_val
        self.step = (max_val - min_val) / nbins
        self.nbins = int(nbins)
    def __call__(self, val):
        return int(round((val - self.min_val) / self.step)) % self.nbins
```

Code l∆∞·ª£ng t·ª≠ h√≥a cho kh√¥ng gian tr·∫°ng th√°i:
```python
class Dicretezation:
    def __init__(self, discretezers):
        self.discretezers = discretezers
    def __getitem__(self, index):
        assert len(index) == len(self.discretezers)
        return tuple([self.discretezers[i](index[i]) for i in range(len(index))])
```

T·∫°o c√°c *quantizer*:
```python
n_quantization = 50
x_quantizer = NBinDiscretizer(env.observation_space.low[0], env.observation_space.high[0], n_quantization)
v_quantizer = NBinDiscretizer(env.observation_space.low[0], env.observation_space.high[0], n_quantization)
state_quantizer = Dicretezation([x_quantizer, v_quantizer])
```

### 2.2 X√¢y d·ª±ng qu√° tr√¨nh h·ªçc
C√°c b∆∞·ªõc c·ªßa qu√° tr√¨nh h·ªçc nh∆∞ sau:
* Kh·ªüi t·∫°o m·ªôt gi√° x√°c xu·∫•t $\epsilon$ l√† x√°c su·∫•t th·ª±c hi·ªán m·ªôt b∆∞·ªõc ƒëi ng·∫´u nhi√™n, c√≤n l·∫°i $1 - \epsilon$ l√† x√°c su·∫•t th·ª±c hi·ªán theo chi·∫øn l∆∞·ª£c $\pi ^ * = \argmax _{a \in \mathcal{A}} Q^{*} (s,a)$. $\epsilon$ s·∫Ω gi·∫£m d·∫ßn trong qu√° tr√¨nh h·ªçc b·ªüi ban ƒë·∫ßu khi m√¥ h√¨nh ch∆∞a h·ªçc ƒë∆∞·ª£c nhi·ªÅu th√¨ ta c·∫ßn l·∫•y c√°c b∆∞·ªõc ng·∫´u nhi√™n ƒë·ªÉ t·∫°o th√™m d·ªØ li·ªáu cho m√¥ h√¨nh h·ªçc, c√≤n khi h·ªçc ƒë∆∞·ª£c r·ªìi th√¨ s·∫Ω h·ªçc t·ª´ ch√≠nh m√¥ h√¨nh.
*  T·∫°i m·ªói epoch:
    1. Reset m√¥i tr∆∞·ªùng v·ªÅ tr·∫°ng th√°i kh·ªüi t·∫°o $s=s_0$ 
    2. Ch·ªçn m·ªôt b∆∞·ªõc $a$ ƒëi v·ªõi x√°c su·∫•t $\epsilon$ ng·∫´u nhi√™n, v√† $1-\epsilion$ tu√¢n theo chi·∫øn l∆∞·ª£c $\pi^*$.
    3. Th·ª±c hi·ªán b∆∞·ªõc ƒëi $a$, nh·∫≠n v·ªÅ tr·∫°ng th√°i ti·∫øp theo $s'$, l·ª£i t·ª©c t·ª©c th·ªùi $r$ v√† tr·∫°ng th√°i k·∫øt th√∫c.
    4. C·∫≠p nh·∫≠t $Q$ theo (1)

Code:
```python
# inititalize some variables
lr = 0.1
gamma = 0.9
epochs = 10000
epsilon = 0.9
epsilon_scale = epsilon / (epochs / 4)

# some metrics
max_reward = -1000
max_pos = -1000

# logging
# inititalize some variables
epochs = 10000
epsilon = 0.9
epsilon_scale = epsilon / (epochs / 4)

# some metrics
max_reward = -1000
max_pos = -1000

# logging
log = display('', display_id=True)
reach_log = display('', display_id=True)

for epoch in tqdm(range(epochs), desc="Epoch"):
    ep_max_pos = -1000
    ep_reward = 0
    
    # reset environment
    obs = env.reset()
    done = False

    while not done:
        
        # take an action
        if np.random.random_sample() > epsilon:
            a = np.argmax(Q[state_quantizer[obs]])
        else:
            a = np.random.randint(0, env.action_space.n)
        
        # perform action
        new_obs, r, done, info = env.step(a)
        ep_reward += r
        
        if new_obs[0] >= env.goal_position:
            reach_log.update(f"Reach goal at epoch {epoch} with reward: {ep_reward}")
        # update Q
        cur_q_value = Q[state_quantizer[obs]][a]        
        new_q_value = (1-lr) * cur_q_value + lr * (r + gamma * max(Q[state_quantizer[new_obs]]))
        Q[state_quantizer[obs]][a] = new_q_value
        obs = new_obs
        ep_max_pos = max(obs[0], ep_max_pos)
        
    max_reward = max(ep_reward, max_reward)
    max_pos = max(ep_max_pos, max_pos)
    epsilon = max(0, epsilon - epsilon_scale)
    
    log.update("epoch {}: ep_reward: {:9.6f}, max_reward: {:9.6f}, ep_max_pos: {:.6f}, max_pos: {:.6f}, epsilon: {:.6f}".format(epoch, ep_reward, max_reward, ep_max_pos, max_pos, epsilon))
```

# 3. T·ªïng k·∫øt
Hy v·ªçng v·ªõi m·ªôt v√≠ d·ª• nh·ªè trong b√†i vi·∫øt n√†y s·∫Ω gi√∫p c√°c b·∫°n hi·ªÉu h∆°n v·ªÅ **Q-function** v√† **Q-learning**. Code ƒë·∫ßy ƒë·ªß trong b√†i vi·∫øt n√†y m√¨nh s·∫Ω ƒë·ªÉ ·ªü [ƒë√¢y](https://github.com/nguyentuxuancong/reinforcementlearning). N·∫øu c√≥ g√¨ g√≥p √Ω th√¨ ƒë·ª´ng ng·∫ßn ng·∫°i cho m√¨nh bi·∫øt ƒë·ªÉ ho√†n thi·ªán h∆°n. C√≤n n·∫øu th·∫•y hay th√¨ cho m√¨nh xin 1 upvote üòÑ.